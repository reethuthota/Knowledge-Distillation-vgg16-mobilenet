{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y2U2PlMvPyV",
        "outputId": "c561bcdc-1328-4c11-afca-d6a78fd1708d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "WtISuKAwvsVu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH9So8A6HFvK",
        "outputId": "08428451-7595-4c89-fb7f-b9f852bf437b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Epoch: 189 [40832/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 189 [40960/50000]\tLoss: 0.0394\tLR: 0.000800\n",
            "Training Epoch: 189 [41088/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 189 [41216/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 189 [41344/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 189 [41472/50000]\tLoss: 0.0288\tLR: 0.000800\n",
            "Training Epoch: 189 [41600/50000]\tLoss: 0.0358\tLR: 0.000800\n",
            "Training Epoch: 189 [41728/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 189 [41856/50000]\tLoss: 0.0289\tLR: 0.000800\n",
            "Training Epoch: 189 [41984/50000]\tLoss: 0.0715\tLR: 0.000800\n",
            "Training Epoch: 189 [42112/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 189 [42240/50000]\tLoss: 0.0272\tLR: 0.000800\n",
            "Training Epoch: 189 [42368/50000]\tLoss: 0.0462\tLR: 0.000800\n",
            "Training Epoch: 189 [42496/50000]\tLoss: 0.0239\tLR: 0.000800\n",
            "Training Epoch: 189 [42624/50000]\tLoss: 0.0157\tLR: 0.000800\n",
            "Training Epoch: 189 [42752/50000]\tLoss: 0.0361\tLR: 0.000800\n",
            "Training Epoch: 189 [42880/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 189 [43008/50000]\tLoss: 0.0168\tLR: 0.000800\n",
            "Training Epoch: 189 [43136/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 189 [43264/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 189 [43392/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 189 [43520/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 189 [43648/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 189 [43776/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 189 [43904/50000]\tLoss: 0.0224\tLR: 0.000800\n",
            "Training Epoch: 189 [44032/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 189 [44160/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 189 [44288/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 189 [44416/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 189 [44544/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 189 [44672/50000]\tLoss: 0.0458\tLR: 0.000800\n",
            "Training Epoch: 189 [44800/50000]\tLoss: 0.0283\tLR: 0.000800\n",
            "Training Epoch: 189 [44928/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 189 [45056/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 189 [45184/50000]\tLoss: 0.0246\tLR: 0.000800\n",
            "Training Epoch: 189 [45312/50000]\tLoss: 0.0265\tLR: 0.000800\n",
            "Training Epoch: 189 [45440/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 189 [45568/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 189 [45696/50000]\tLoss: 0.0486\tLR: 0.000800\n",
            "Training Epoch: 189 [45824/50000]\tLoss: 0.0428\tLR: 0.000800\n",
            "Training Epoch: 189 [45952/50000]\tLoss: 0.0363\tLR: 0.000800\n",
            "Training Epoch: 189 [46080/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 189 [46208/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 189 [46336/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 189 [46464/50000]\tLoss: 0.0466\tLR: 0.000800\n",
            "Training Epoch: 189 [46592/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 189 [46720/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 189 [46848/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 189 [46976/50000]\tLoss: 0.0332\tLR: 0.000800\n",
            "Training Epoch: 189 [47104/50000]\tLoss: 0.0330\tLR: 0.000800\n",
            "Training Epoch: 189 [47232/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 189 [47360/50000]\tLoss: 0.0493\tLR: 0.000800\n",
            "Training Epoch: 189 [47488/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 189 [47616/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 189 [47744/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 189 [47872/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 189 [48000/50000]\tLoss: 0.0743\tLR: 0.000800\n",
            "Training Epoch: 189 [48128/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 189 [48256/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 189 [48384/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 189 [48512/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 189 [48640/50000]\tLoss: 0.0389\tLR: 0.000800\n",
            "Training Epoch: 189 [48768/50000]\tLoss: 0.0160\tLR: 0.000800\n",
            "Training Epoch: 189 [48896/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 189 [49024/50000]\tLoss: 0.0241\tLR: 0.000800\n",
            "Training Epoch: 189 [49152/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 189 [49280/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 189 [49408/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 189 [49536/50000]\tLoss: 0.0607\tLR: 0.000800\n",
            "Training Epoch: 189 [49664/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 189 [49792/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 189 [49920/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 189 [50000/50000]\tLoss: 0.0838\tLR: 0.000800\n",
            "epoch 189 training time consumed: 33.65s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 210571 GiB | 210570 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 208652 GiB | 208651 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1919 GiB |   1919 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 210571 GiB | 210570 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 208652 GiB | 208651 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1919 GiB |   1919 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 209920 GiB | 209920 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 208002 GiB | 208002 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   1918 GiB |   1918 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 186248 GiB | 186248 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 184329 GiB | 184329 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   1919 GiB |   1919 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   23066 K  |   23066 K  |\n",
            "|       from large pool |      38    |      61    |   10080 K  |   10080 K  |\n",
            "|       from small pool |     187    |     230    |   12985 K  |   12985 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   23066 K  |   23066 K  |\n",
            "|       from large pool |      38    |      61    |   10080 K  |   10080 K  |\n",
            "|       from small pool |     187    |     230    |   12985 K  |   12985 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |    9560 K  |    9560 K  |\n",
            "|       from large pool |      13    |      15    |    4582 K  |    4582 K  |\n",
            "|       from small pool |       7    |      14    |    4978 K  |    4978 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 189, Average loss: 0.0128, Accuracy: 0.7183, Time consumed:2.95s\n",
            "\n",
            "Training Epoch: 190 [128/50000]\tLoss: 0.0464\tLR: 0.000800\n",
            "Training Epoch: 190 [256/50000]\tLoss: 0.0474\tLR: 0.000800\n",
            "Training Epoch: 190 [384/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 190 [512/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 190 [640/50000]\tLoss: 0.0439\tLR: 0.000800\n",
            "Training Epoch: 190 [768/50000]\tLoss: 0.0174\tLR: 0.000800\n",
            "Training Epoch: 190 [896/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 190 [1024/50000]\tLoss: 0.0229\tLR: 0.000800\n",
            "Training Epoch: 190 [1152/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 190 [1280/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 190 [1408/50000]\tLoss: 0.0310\tLR: 0.000800\n",
            "Training Epoch: 190 [1536/50000]\tLoss: 0.0194\tLR: 0.000800\n",
            "Training Epoch: 190 [1664/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 190 [1792/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 190 [1920/50000]\tLoss: 0.0174\tLR: 0.000800\n",
            "Training Epoch: 190 [2048/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 190 [2176/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 190 [2304/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 190 [2432/50000]\tLoss: 0.0301\tLR: 0.000800\n",
            "Training Epoch: 190 [2560/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 190 [2688/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 190 [2816/50000]\tLoss: 0.0679\tLR: 0.000800\n",
            "Training Epoch: 190 [2944/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 190 [3072/50000]\tLoss: 0.0160\tLR: 0.000800\n",
            "Training Epoch: 190 [3200/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 190 [3328/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 190 [3456/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 190 [3584/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 190 [3712/50000]\tLoss: 0.0392\tLR: 0.000800\n",
            "Training Epoch: 190 [3840/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 190 [3968/50000]\tLoss: 0.0236\tLR: 0.000800\n",
            "Training Epoch: 190 [4096/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 190 [4224/50000]\tLoss: 0.0214\tLR: 0.000800\n",
            "Training Epoch: 190 [4352/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 190 [4480/50000]\tLoss: 0.0634\tLR: 0.000800\n",
            "Training Epoch: 190 [4608/50000]\tLoss: 0.0277\tLR: 0.000800\n",
            "Training Epoch: 190 [4736/50000]\tLoss: 0.0696\tLR: 0.000800\n",
            "Training Epoch: 190 [4864/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 190 [4992/50000]\tLoss: 0.0424\tLR: 0.000800\n",
            "Training Epoch: 190 [5120/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 190 [5248/50000]\tLoss: 0.0418\tLR: 0.000800\n",
            "Training Epoch: 190 [5376/50000]\tLoss: 0.0026\tLR: 0.000800\n",
            "Training Epoch: 190 [5504/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 190 [5632/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 190 [5760/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 190 [5888/50000]\tLoss: 0.0181\tLR: 0.000800\n",
            "Training Epoch: 190 [6016/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 190 [6144/50000]\tLoss: 0.1262\tLR: 0.000800\n",
            "Training Epoch: 190 [6272/50000]\tLoss: 0.0560\tLR: 0.000800\n",
            "Training Epoch: 190 [6400/50000]\tLoss: 0.0325\tLR: 0.000800\n",
            "Training Epoch: 190 [6528/50000]\tLoss: 0.0205\tLR: 0.000800\n",
            "Training Epoch: 190 [6656/50000]\tLoss: 0.1137\tLR: 0.000800\n",
            "Training Epoch: 190 [6784/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 190 [6912/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 190 [7040/50000]\tLoss: 0.0523\tLR: 0.000800\n",
            "Training Epoch: 190 [7168/50000]\tLoss: 0.0265\tLR: 0.000800\n",
            "Training Epoch: 190 [7296/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 190 [7424/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 190 [7552/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 190 [7680/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 190 [7808/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 190 [7936/50000]\tLoss: 0.0188\tLR: 0.000800\n",
            "Training Epoch: 190 [8064/50000]\tLoss: 0.0337\tLR: 0.000800\n",
            "Training Epoch: 190 [8192/50000]\tLoss: 0.0191\tLR: 0.000800\n",
            "Training Epoch: 190 [8320/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 190 [8448/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 190 [8576/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 190 [8704/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 190 [8832/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 190 [8960/50000]\tLoss: 0.0301\tLR: 0.000800\n",
            "Training Epoch: 190 [9088/50000]\tLoss: 0.0209\tLR: 0.000800\n",
            "Training Epoch: 190 [9216/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 190 [9344/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 190 [9472/50000]\tLoss: 0.0287\tLR: 0.000800\n",
            "Training Epoch: 190 [9600/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 190 [9728/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 190 [9856/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 190 [9984/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 190 [10112/50000]\tLoss: 0.1028\tLR: 0.000800\n",
            "Training Epoch: 190 [10240/50000]\tLoss: 0.0815\tLR: 0.000800\n",
            "Training Epoch: 190 [10368/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 190 [10496/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 190 [10624/50000]\tLoss: 0.0313\tLR: 0.000800\n",
            "Training Epoch: 190 [10752/50000]\tLoss: 0.0321\tLR: 0.000800\n",
            "Training Epoch: 190 [10880/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 190 [11008/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 190 [11136/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 190 [11264/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 190 [11392/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 190 [11520/50000]\tLoss: 0.0271\tLR: 0.000800\n",
            "Training Epoch: 190 [11648/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 190 [11776/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 190 [11904/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 190 [12032/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 190 [12160/50000]\tLoss: 0.0239\tLR: 0.000800\n",
            "Training Epoch: 190 [12288/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 190 [12416/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 190 [12544/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 190 [12672/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 190 [12800/50000]\tLoss: 0.0214\tLR: 0.000800\n",
            "Training Epoch: 190 [12928/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 190 [13056/50000]\tLoss: 0.0297\tLR: 0.000800\n",
            "Training Epoch: 190 [13184/50000]\tLoss: 0.0167\tLR: 0.000800\n",
            "Training Epoch: 190 [13312/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 190 [13440/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 190 [13568/50000]\tLoss: 0.0984\tLR: 0.000800\n",
            "Training Epoch: 190 [13696/50000]\tLoss: 0.0665\tLR: 0.000800\n",
            "Training Epoch: 190 [13824/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 190 [13952/50000]\tLoss: 0.0503\tLR: 0.000800\n",
            "Training Epoch: 190 [14080/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 190 [14208/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 190 [14336/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 190 [14464/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 190 [14592/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 190 [14720/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 190 [14848/50000]\tLoss: 0.0385\tLR: 0.000800\n",
            "Training Epoch: 190 [14976/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 190 [15104/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 190 [15232/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 190 [15360/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 190 [15488/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 190 [15616/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 190 [15744/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 190 [15872/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 190 [16000/50000]\tLoss: 0.0479\tLR: 0.000800\n",
            "Training Epoch: 190 [16128/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 190 [16256/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 190 [16384/50000]\tLoss: 0.0567\tLR: 0.000800\n",
            "Training Epoch: 190 [16512/50000]\tLoss: 0.0239\tLR: 0.000800\n",
            "Training Epoch: 190 [16640/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 190 [16768/50000]\tLoss: 0.0553\tLR: 0.000800\n",
            "Training Epoch: 190 [16896/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 190 [17024/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 190 [17152/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 190 [17280/50000]\tLoss: 0.0429\tLR: 0.000800\n",
            "Training Epoch: 190 [17408/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 190 [17536/50000]\tLoss: 0.0014\tLR: 0.000800\n",
            "Training Epoch: 190 [17664/50000]\tLoss: 0.0795\tLR: 0.000800\n",
            "Training Epoch: 190 [17792/50000]\tLoss: 0.0309\tLR: 0.000800\n",
            "Training Epoch: 190 [17920/50000]\tLoss: 0.0404\tLR: 0.000800\n",
            "Training Epoch: 190 [18048/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 190 [18176/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 190 [18304/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 190 [18432/50000]\tLoss: 0.0203\tLR: 0.000800\n",
            "Training Epoch: 190 [18560/50000]\tLoss: 0.0216\tLR: 0.000800\n",
            "Training Epoch: 190 [18688/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 190 [18816/50000]\tLoss: 0.0294\tLR: 0.000800\n",
            "Training Epoch: 190 [18944/50000]\tLoss: 0.0451\tLR: 0.000800\n",
            "Training Epoch: 190 [19072/50000]\tLoss: 0.0427\tLR: 0.000800\n",
            "Training Epoch: 190 [19200/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 190 [19328/50000]\tLoss: 0.0356\tLR: 0.000800\n",
            "Training Epoch: 190 [19456/50000]\tLoss: 0.0426\tLR: 0.000800\n",
            "Training Epoch: 190 [19584/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 190 [19712/50000]\tLoss: 0.0405\tLR: 0.000800\n",
            "Training Epoch: 190 [19840/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 190 [19968/50000]\tLoss: 0.0218\tLR: 0.000800\n",
            "Training Epoch: 190 [20096/50000]\tLoss: 0.0537\tLR: 0.000800\n",
            "Training Epoch: 190 [20224/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 190 [20352/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 190 [20480/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 190 [20608/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 190 [20736/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 190 [20864/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 190 [20992/50000]\tLoss: 0.0231\tLR: 0.000800\n",
            "Training Epoch: 190 [21120/50000]\tLoss: 0.0387\tLR: 0.000800\n",
            "Training Epoch: 190 [21248/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 190 [21376/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 190 [21504/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 190 [21632/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 190 [21760/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 190 [21888/50000]\tLoss: 0.0311\tLR: 0.000800\n",
            "Training Epoch: 190 [22016/50000]\tLoss: 0.0194\tLR: 0.000800\n",
            "Training Epoch: 190 [22144/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 190 [22272/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 190 [22400/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 190 [22528/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 190 [22656/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 190 [22784/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 190 [22912/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 190 [23040/50000]\tLoss: 0.0338\tLR: 0.000800\n",
            "Training Epoch: 190 [23168/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 190 [23296/50000]\tLoss: 0.0433\tLR: 0.000800\n",
            "Training Epoch: 190 [23424/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 190 [23552/50000]\tLoss: 0.0538\tLR: 0.000800\n",
            "Training Epoch: 190 [23680/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 190 [23808/50000]\tLoss: 0.0198\tLR: 0.000800\n",
            "Training Epoch: 190 [23936/50000]\tLoss: 0.0346\tLR: 0.000800\n",
            "Training Epoch: 190 [24064/50000]\tLoss: 0.0316\tLR: 0.000800\n",
            "Training Epoch: 190 [24192/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 190 [24320/50000]\tLoss: 0.0311\tLR: 0.000800\n",
            "Training Epoch: 190 [24448/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 190 [24576/50000]\tLoss: 0.0452\tLR: 0.000800\n",
            "Training Epoch: 190 [24704/50000]\tLoss: 0.0481\tLR: 0.000800\n",
            "Training Epoch: 190 [24832/50000]\tLoss: 0.0227\tLR: 0.000800\n",
            "Training Epoch: 190 [24960/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 190 [25088/50000]\tLoss: 0.0491\tLR: 0.000800\n",
            "Training Epoch: 190 [25216/50000]\tLoss: 0.0218\tLR: 0.000800\n",
            "Training Epoch: 190 [25344/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 190 [25472/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 190 [25600/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 190 [25728/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 190 [25856/50000]\tLoss: 0.0327\tLR: 0.000800\n",
            "Training Epoch: 190 [25984/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 190 [26112/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 190 [26240/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 190 [26368/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 190 [26496/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 190 [26624/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 190 [26752/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 190 [26880/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 190 [27008/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 190 [27136/50000]\tLoss: 0.0191\tLR: 0.000800\n",
            "Training Epoch: 190 [27264/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 190 [27392/50000]\tLoss: 0.0394\tLR: 0.000800\n",
            "Training Epoch: 190 [27520/50000]\tLoss: 0.0225\tLR: 0.000800\n",
            "Training Epoch: 190 [27648/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 190 [27776/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 190 [27904/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 190 [28032/50000]\tLoss: 0.0181\tLR: 0.000800\n",
            "Training Epoch: 190 [28160/50000]\tLoss: 0.0422\tLR: 0.000800\n",
            "Training Epoch: 190 [28288/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 190 [28416/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 190 [28544/50000]\tLoss: 0.0607\tLR: 0.000800\n",
            "Training Epoch: 190 [28672/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 190 [28800/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 190 [28928/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 190 [29056/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 190 [29184/50000]\tLoss: 0.0279\tLR: 0.000800\n",
            "Training Epoch: 190 [29312/50000]\tLoss: 0.0250\tLR: 0.000800\n",
            "Training Epoch: 190 [29440/50000]\tLoss: 0.0300\tLR: 0.000800\n",
            "Training Epoch: 190 [29568/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 190 [29696/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 190 [29824/50000]\tLoss: 0.0482\tLR: 0.000800\n",
            "Training Epoch: 190 [29952/50000]\tLoss: 0.0147\tLR: 0.000800\n",
            "Training Epoch: 190 [30080/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 190 [30208/50000]\tLoss: 0.0381\tLR: 0.000800\n",
            "Training Epoch: 190 [30336/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 190 [30464/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 190 [30592/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 190 [30720/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 190 [30848/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 190 [30976/50000]\tLoss: 0.0227\tLR: 0.000800\n",
            "Training Epoch: 190 [31104/50000]\tLoss: 0.0265\tLR: 0.000800\n",
            "Training Epoch: 190 [31232/50000]\tLoss: 0.0329\tLR: 0.000800\n",
            "Training Epoch: 190 [31360/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 190 [31488/50000]\tLoss: 0.0323\tLR: 0.000800\n",
            "Training Epoch: 190 [31616/50000]\tLoss: 0.0398\tLR: 0.000800\n",
            "Training Epoch: 190 [31744/50000]\tLoss: 0.0637\tLR: 0.000800\n",
            "Training Epoch: 190 [31872/50000]\tLoss: 0.0221\tLR: 0.000800\n",
            "Training Epoch: 190 [32000/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 190 [32128/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 190 [32256/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 190 [32384/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 190 [32512/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 190 [32640/50000]\tLoss: 0.0304\tLR: 0.000800\n",
            "Training Epoch: 190 [32768/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 190 [32896/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 190 [33024/50000]\tLoss: 0.0227\tLR: 0.000800\n",
            "Training Epoch: 190 [33152/50000]\tLoss: 0.0403\tLR: 0.000800\n",
            "Training Epoch: 190 [33280/50000]\tLoss: 0.0314\tLR: 0.000800\n",
            "Training Epoch: 190 [33408/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 190 [33536/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 190 [33664/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 190 [33792/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 190 [33920/50000]\tLoss: 0.0347\tLR: 0.000800\n",
            "Training Epoch: 190 [34048/50000]\tLoss: 0.0693\tLR: 0.000800\n",
            "Training Epoch: 190 [34176/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 190 [34304/50000]\tLoss: 0.0301\tLR: 0.000800\n",
            "Training Epoch: 190 [34432/50000]\tLoss: 0.0579\tLR: 0.000800\n",
            "Training Epoch: 190 [34560/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 190 [34688/50000]\tLoss: 0.0443\tLR: 0.000800\n",
            "Training Epoch: 190 [34816/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 190 [34944/50000]\tLoss: 0.0344\tLR: 0.000800\n",
            "Training Epoch: 190 [35072/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 190 [35200/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 190 [35328/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 190 [35456/50000]\tLoss: 0.0990\tLR: 0.000800\n",
            "Training Epoch: 190 [35584/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 190 [35712/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 190 [35840/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 190 [35968/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 190 [36096/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 190 [36224/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 190 [36352/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 190 [36480/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 190 [36608/50000]\tLoss: 0.0168\tLR: 0.000800\n",
            "Training Epoch: 190 [36736/50000]\tLoss: 0.0490\tLR: 0.000800\n",
            "Training Epoch: 190 [36864/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 190 [36992/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 190 [37120/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 190 [37248/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 190 [37376/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 190 [37504/50000]\tLoss: 0.0569\tLR: 0.000800\n",
            "Training Epoch: 190 [37632/50000]\tLoss: 0.0495\tLR: 0.000800\n",
            "Training Epoch: 190 [37760/50000]\tLoss: 0.0165\tLR: 0.000800\n",
            "Training Epoch: 190 [37888/50000]\tLoss: 0.0558\tLR: 0.000800\n",
            "Training Epoch: 190 [38016/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 190 [38144/50000]\tLoss: 0.0335\tLR: 0.000800\n",
            "Training Epoch: 190 [38272/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 190 [38400/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 190 [38528/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 190 [38656/50000]\tLoss: 0.0288\tLR: 0.000800\n",
            "Training Epoch: 190 [38784/50000]\tLoss: 0.0181\tLR: 0.000800\n",
            "Training Epoch: 190 [38912/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 190 [39040/50000]\tLoss: 0.0479\tLR: 0.000800\n",
            "Training Epoch: 190 [39168/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 190 [39296/50000]\tLoss: 0.0220\tLR: 0.000800\n",
            "Training Epoch: 190 [39424/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 190 [39552/50000]\tLoss: 0.0199\tLR: 0.000800\n",
            "Training Epoch: 190 [39680/50000]\tLoss: 0.0281\tLR: 0.000800\n",
            "Training Epoch: 190 [39808/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 190 [39936/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 190 [40064/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 190 [40192/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 190 [40320/50000]\tLoss: 0.0434\tLR: 0.000800\n",
            "Training Epoch: 190 [40448/50000]\tLoss: 0.0285\tLR: 0.000800\n",
            "Training Epoch: 190 [40576/50000]\tLoss: 0.0243\tLR: 0.000800\n",
            "Training Epoch: 190 [40704/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 190 [40832/50000]\tLoss: 0.0429\tLR: 0.000800\n",
            "Training Epoch: 190 [40960/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 190 [41088/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 190 [41216/50000]\tLoss: 0.0610\tLR: 0.000800\n",
            "Training Epoch: 190 [41344/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 190 [41472/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 190 [41600/50000]\tLoss: 0.0344\tLR: 0.000800\n",
            "Training Epoch: 190 [41728/50000]\tLoss: 0.0326\tLR: 0.000800\n",
            "Training Epoch: 190 [41856/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 190 [41984/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 190 [42112/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 190 [42240/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 190 [42368/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 190 [42496/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 190 [42624/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 190 [42752/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 190 [42880/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 190 [43008/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 190 [43136/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 190 [43264/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 190 [43392/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 190 [43520/50000]\tLoss: 0.0554\tLR: 0.000800\n",
            "Training Epoch: 190 [43648/50000]\tLoss: 0.0285\tLR: 0.000800\n",
            "Training Epoch: 190 [43776/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 190 [43904/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 190 [44032/50000]\tLoss: 0.0258\tLR: 0.000800\n",
            "Training Epoch: 190 [44160/50000]\tLoss: 0.0242\tLR: 0.000800\n",
            "Training Epoch: 190 [44288/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 190 [44416/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 190 [44544/50000]\tLoss: 0.0274\tLR: 0.000800\n",
            "Training Epoch: 190 [44672/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 190 [44800/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 190 [44928/50000]\tLoss: 0.0318\tLR: 0.000800\n",
            "Training Epoch: 190 [45056/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 190 [45184/50000]\tLoss: 0.0191\tLR: 0.000800\n",
            "Training Epoch: 190 [45312/50000]\tLoss: 0.0795\tLR: 0.000800\n",
            "Training Epoch: 190 [45440/50000]\tLoss: 0.0437\tLR: 0.000800\n",
            "Training Epoch: 190 [45568/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 190 [45696/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 190 [45824/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 190 [45952/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 190 [46080/50000]\tLoss: 0.0289\tLR: 0.000800\n",
            "Training Epoch: 190 [46208/50000]\tLoss: 0.0271\tLR: 0.000800\n",
            "Training Epoch: 190 [46336/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 190 [46464/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 190 [46592/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 190 [46720/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 190 [46848/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 190 [46976/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 190 [47104/50000]\tLoss: 0.0309\tLR: 0.000800\n",
            "Training Epoch: 190 [47232/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 190 [47360/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 190 [47488/50000]\tLoss: 0.0271\tLR: 0.000800\n",
            "Training Epoch: 190 [47616/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 190 [47744/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 190 [47872/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 190 [48000/50000]\tLoss: 0.0479\tLR: 0.000800\n",
            "Training Epoch: 190 [48128/50000]\tLoss: 0.0262\tLR: 0.000800\n",
            "Training Epoch: 190 [48256/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 190 [48384/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 190 [48512/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 190 [48640/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 190 [48768/50000]\tLoss: 0.0160\tLR: 0.000800\n",
            "Training Epoch: 190 [48896/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 190 [49024/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 190 [49152/50000]\tLoss: 0.0303\tLR: 0.000800\n",
            "Training Epoch: 190 [49280/50000]\tLoss: 0.0399\tLR: 0.000800\n",
            "Training Epoch: 190 [49408/50000]\tLoss: 0.0174\tLR: 0.000800\n",
            "Training Epoch: 190 [49536/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 190 [49664/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 190 [49792/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 190 [49920/50000]\tLoss: 0.0543\tLR: 0.000800\n",
            "Training Epoch: 190 [50000/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "epoch 190 training time consumed: 33.70s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 211685 GiB | 211684 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 209755 GiB | 209755 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1929 GiB |   1929 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 211685 GiB | 211684 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 209755 GiB | 209755 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1929 GiB |   1929 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 211031 GiB | 211031 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 209103 GiB | 209102 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   1928 GiB |   1928 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 187234 GiB | 187234 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 185304 GiB | 185304 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   1929 GiB |   1929 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   23188 K  |   23188 K  |\n",
            "|       from large pool |      38    |      61    |   10134 K  |   10134 K  |\n",
            "|       from small pool |     187    |     230    |   13054 K  |   13053 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   23188 K  |   23188 K  |\n",
            "|       from large pool |      38    |      61    |   10134 K  |   10134 K  |\n",
            "|       from small pool |     187    |     230    |   13054 K  |   13053 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |    9611 K  |    9611 K  |\n",
            "|       from large pool |      13    |      15    |    4606 K  |    4606 K  |\n",
            "|       from small pool |       7    |      14    |    5004 K  |    5004 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 190, Average loss: 0.0129, Accuracy: 0.7172, Time consumed:4.25s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Saturday_04_May_2024_07h_16m_24s/vgg16-190-regular.pth\n",
            "Training Epoch: 191 [128/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 191 [256/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 191 [384/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 191 [512/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 191 [640/50000]\tLoss: 0.0461\tLR: 0.000800\n",
            "Training Epoch: 191 [768/50000]\tLoss: 0.0480\tLR: 0.000800\n",
            "Training Epoch: 191 [896/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 191 [1024/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 191 [1152/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 191 [1280/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 191 [1408/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 191 [1536/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 191 [1664/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 191 [1792/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 191 [1920/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 191 [2048/50000]\tLoss: 0.0600\tLR: 0.000800\n",
            "Training Epoch: 191 [2176/50000]\tLoss: 0.0162\tLR: 0.000800\n",
            "Training Epoch: 191 [2304/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 191 [2432/50000]\tLoss: 0.0275\tLR: 0.000800\n",
            "Training Epoch: 191 [2560/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 191 [2688/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 191 [2816/50000]\tLoss: 0.0450\tLR: 0.000800\n",
            "Training Epoch: 191 [2944/50000]\tLoss: 0.0305\tLR: 0.000800\n",
            "Training Epoch: 191 [3072/50000]\tLoss: 0.0262\tLR: 0.000800\n",
            "Training Epoch: 191 [3200/50000]\tLoss: 0.0207\tLR: 0.000800\n",
            "Training Epoch: 191 [3328/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 191 [3456/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 191 [3584/50000]\tLoss: 0.0157\tLR: 0.000800\n",
            "Training Epoch: 191 [3712/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 191 [3840/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 191 [3968/50000]\tLoss: 0.0262\tLR: 0.000800\n",
            "Training Epoch: 191 [4096/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 191 [4224/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 191 [4352/50000]\tLoss: 0.0503\tLR: 0.000800\n",
            "Training Epoch: 191 [4480/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 191 [4608/50000]\tLoss: 0.0492\tLR: 0.000800\n",
            "Training Epoch: 191 [4736/50000]\tLoss: 0.0317\tLR: 0.000800\n",
            "Training Epoch: 191 [4864/50000]\tLoss: 0.0359\tLR: 0.000800\n",
            "Training Epoch: 191 [4992/50000]\tLoss: 0.0373\tLR: 0.000800\n",
            "Training Epoch: 191 [5120/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 191 [5248/50000]\tLoss: 0.0196\tLR: 0.000800\n",
            "Training Epoch: 191 [5376/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 191 [5504/50000]\tLoss: 0.0160\tLR: 0.000800\n",
            "Training Epoch: 191 [5632/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 191 [5760/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 191 [5888/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 191 [6016/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 191 [6144/50000]\tLoss: 0.0601\tLR: 0.000800\n",
            "Training Epoch: 191 [6272/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 191 [6400/50000]\tLoss: 0.0396\tLR: 0.000800\n",
            "Training Epoch: 191 [6528/50000]\tLoss: 0.0386\tLR: 0.000800\n",
            "Training Epoch: 191 [6656/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 191 [6784/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 191 [6912/50000]\tLoss: 0.0426\tLR: 0.000800\n",
            "Training Epoch: 191 [7040/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 191 [7168/50000]\tLoss: 0.0344\tLR: 0.000800\n",
            "Training Epoch: 191 [7296/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 191 [7424/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 191 [7552/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 191 [7680/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 191 [7808/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 191 [7936/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 191 [8064/50000]\tLoss: 0.0428\tLR: 0.000800\n",
            "Training Epoch: 191 [8192/50000]\tLoss: 0.0366\tLR: 0.000800\n",
            "Training Epoch: 191 [8320/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 191 [8448/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 191 [8576/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 191 [8704/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 191 [8832/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 191 [8960/50000]\tLoss: 0.0214\tLR: 0.000800\n",
            "Training Epoch: 191 [9088/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 191 [9216/50000]\tLoss: 0.0322\tLR: 0.000800\n",
            "Training Epoch: 191 [9344/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 191 [9472/50000]\tLoss: 0.0565\tLR: 0.000800\n",
            "Training Epoch: 191 [9600/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 191 [9728/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 191 [9856/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 191 [9984/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 191 [10112/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 191 [10240/50000]\tLoss: 0.0304\tLR: 0.000800\n",
            "Training Epoch: 191 [10368/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 191 [10496/50000]\tLoss: 0.0366\tLR: 0.000800\n",
            "Training Epoch: 191 [10624/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 191 [10752/50000]\tLoss: 0.0581\tLR: 0.000800\n",
            "Training Epoch: 191 [10880/50000]\tLoss: 0.0196\tLR: 0.000800\n",
            "Training Epoch: 191 [11008/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 191 [11136/50000]\tLoss: 0.0218\tLR: 0.000800\n",
            "Training Epoch: 191 [11264/50000]\tLoss: 0.0420\tLR: 0.000800\n",
            "Training Epoch: 191 [11392/50000]\tLoss: 0.0277\tLR: 0.000800\n",
            "Training Epoch: 191 [11520/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 191 [11648/50000]\tLoss: 0.0312\tLR: 0.000800\n",
            "Training Epoch: 191 [11776/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 191 [11904/50000]\tLoss: 0.0473\tLR: 0.000800\n",
            "Training Epoch: 191 [12032/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 191 [12160/50000]\tLoss: 0.0570\tLR: 0.000800\n",
            "Training Epoch: 191 [12288/50000]\tLoss: 0.0345\tLR: 0.000800\n",
            "Training Epoch: 191 [12416/50000]\tLoss: 0.0323\tLR: 0.000800\n",
            "Training Epoch: 191 [12544/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 191 [12672/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 191 [12800/50000]\tLoss: 0.0475\tLR: 0.000800\n",
            "Training Epoch: 191 [12928/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 191 [13056/50000]\tLoss: 0.0481\tLR: 0.000800\n",
            "Training Epoch: 191 [13184/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 191 [13312/50000]\tLoss: 0.0570\tLR: 0.000800\n",
            "Training Epoch: 191 [13440/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 191 [13568/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 191 [13696/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 191 [13824/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 191 [13952/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 191 [14080/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 191 [14208/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 191 [14336/50000]\tLoss: 0.0323\tLR: 0.000800\n",
            "Training Epoch: 191 [14464/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 191 [14592/50000]\tLoss: 0.0408\tLR: 0.000800\n",
            "Training Epoch: 191 [14720/50000]\tLoss: 0.0493\tLR: 0.000800\n",
            "Training Epoch: 191 [14848/50000]\tLoss: 0.0447\tLR: 0.000800\n",
            "Training Epoch: 191 [14976/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 191 [15104/50000]\tLoss: 0.0262\tLR: 0.000800\n",
            "Training Epoch: 191 [15232/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 191 [15360/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 191 [15488/50000]\tLoss: 0.0236\tLR: 0.000800\n",
            "Training Epoch: 191 [15616/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 191 [15744/50000]\tLoss: 0.0277\tLR: 0.000800\n",
            "Training Epoch: 191 [15872/50000]\tLoss: 0.0276\tLR: 0.000800\n",
            "Training Epoch: 191 [16000/50000]\tLoss: 0.0136\tLR: 0.000800\n",
            "Training Epoch: 191 [16128/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 191 [16256/50000]\tLoss: 0.0740\tLR: 0.000800\n",
            "Training Epoch: 191 [16384/50000]\tLoss: 0.0400\tLR: 0.000800\n",
            "Training Epoch: 191 [16512/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 191 [16640/50000]\tLoss: 0.0367\tLR: 0.000800\n",
            "Training Epoch: 191 [16768/50000]\tLoss: 0.0233\tLR: 0.000800\n",
            "Training Epoch: 191 [16896/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 191 [17024/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 191 [17152/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 191 [17280/50000]\tLoss: 0.0187\tLR: 0.000800\n",
            "Training Epoch: 191 [17408/50000]\tLoss: 0.0142\tLR: 0.000800\n",
            "Training Epoch: 191 [17536/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 191 [17664/50000]\tLoss: 0.0320\tLR: 0.000800\n",
            "Training Epoch: 191 [17792/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 191 [17920/50000]\tLoss: 0.0512\tLR: 0.000800\n",
            "Training Epoch: 191 [18048/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 191 [18176/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 191 [18304/50000]\tLoss: 0.0430\tLR: 0.000800\n",
            "Training Epoch: 191 [18432/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 191 [18560/50000]\tLoss: 0.0200\tLR: 0.000800\n",
            "Training Epoch: 191 [18688/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 191 [18816/50000]\tLoss: 0.0259\tLR: 0.000800\n",
            "Training Epoch: 191 [18944/50000]\tLoss: 0.0242\tLR: 0.000800\n",
            "Training Epoch: 191 [19072/50000]\tLoss: 0.0246\tLR: 0.000800\n",
            "Training Epoch: 191 [19200/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 191 [19328/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 191 [19456/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 191 [19584/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 191 [19712/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 191 [19840/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 191 [19968/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 191 [20096/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 191 [20224/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 191 [20352/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 191 [20480/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 191 [20608/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 191 [20736/50000]\tLoss: 0.0412\tLR: 0.000800\n",
            "Training Epoch: 191 [20864/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 191 [20992/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 191 [21120/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 191 [21248/50000]\tLoss: 0.0244\tLR: 0.000800\n",
            "Training Epoch: 191 [21376/50000]\tLoss: 0.0242\tLR: 0.000800\n",
            "Training Epoch: 191 [21504/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 191 [21632/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 191 [21760/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 191 [21888/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 191 [22016/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 191 [22144/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 191 [22272/50000]\tLoss: 0.0244\tLR: 0.000800\n",
            "Training Epoch: 191 [22400/50000]\tLoss: 0.0326\tLR: 0.000800\n",
            "Training Epoch: 191 [22528/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 191 [22656/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 191 [22784/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 191 [22912/50000]\tLoss: 0.0026\tLR: 0.000800\n",
            "Training Epoch: 191 [23040/50000]\tLoss: 0.0353\tLR: 0.000800\n",
            "Training Epoch: 191 [23168/50000]\tLoss: 0.0624\tLR: 0.000800\n",
            "Training Epoch: 191 [23296/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 191 [23424/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 191 [23552/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 191 [23680/50000]\tLoss: 0.0413\tLR: 0.000800\n",
            "Training Epoch: 191 [23808/50000]\tLoss: 0.0209\tLR: 0.000800\n",
            "Training Epoch: 191 [23936/50000]\tLoss: 0.0233\tLR: 0.000800\n",
            "Training Epoch: 191 [24064/50000]\tLoss: 0.0214\tLR: 0.000800\n",
            "Training Epoch: 191 [24192/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 191 [24320/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 191 [24448/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 191 [24576/50000]\tLoss: 0.0354\tLR: 0.000800\n",
            "Training Epoch: 191 [24704/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 191 [24832/50000]\tLoss: 0.0293\tLR: 0.000800\n",
            "Training Epoch: 191 [24960/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 191 [25088/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 191 [25216/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 191 [25344/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 191 [25472/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 191 [25600/50000]\tLoss: 0.0294\tLR: 0.000800\n",
            "Training Epoch: 191 [25728/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 191 [25856/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 191 [25984/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 191 [26112/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 191 [26240/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 191 [26368/50000]\tLoss: 0.0519\tLR: 0.000800\n",
            "Training Epoch: 191 [26496/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 191 [26624/50000]\tLoss: 0.0380\tLR: 0.000800\n",
            "Training Epoch: 191 [26752/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 191 [26880/50000]\tLoss: 0.0367\tLR: 0.000800\n",
            "Training Epoch: 191 [27008/50000]\tLoss: 0.0253\tLR: 0.000800\n",
            "Training Epoch: 191 [27136/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 191 [27264/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 191 [27392/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 191 [27520/50000]\tLoss: 0.0147\tLR: 0.000800\n",
            "Training Epoch: 191 [27648/50000]\tLoss: 0.0207\tLR: 0.000800\n",
            "Training Epoch: 191 [27776/50000]\tLoss: 0.0780\tLR: 0.000800\n",
            "Training Epoch: 191 [27904/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 191 [28032/50000]\tLoss: 0.0168\tLR: 0.000800\n",
            "Training Epoch: 191 [28160/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 191 [28288/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 191 [28416/50000]\tLoss: 0.0832\tLR: 0.000800\n",
            "Training Epoch: 191 [28544/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 191 [28672/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 191 [28800/50000]\tLoss: 0.0482\tLR: 0.000800\n",
            "Training Epoch: 191 [28928/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 191 [29056/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 191 [29184/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 191 [29312/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 191 [29440/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 191 [29568/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 191 [29696/50000]\tLoss: 0.0167\tLR: 0.000800\n",
            "Training Epoch: 191 [29824/50000]\tLoss: 0.0876\tLR: 0.000800\n",
            "Training Epoch: 191 [29952/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 191 [30080/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 191 [30208/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 191 [30336/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 191 [30464/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 191 [30592/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 191 [30720/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 191 [30848/50000]\tLoss: 0.0329\tLR: 0.000800\n",
            "Training Epoch: 191 [30976/50000]\tLoss: 0.0315\tLR: 0.000800\n",
            "Training Epoch: 191 [31104/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 191 [31232/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 191 [31360/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 191 [31488/50000]\tLoss: 0.0275\tLR: 0.000800\n",
            "Training Epoch: 191 [31616/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 191 [31744/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 191 [31872/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 191 [32000/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 191 [32128/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 191 [32256/50000]\tLoss: 0.0546\tLR: 0.000800\n",
            "Training Epoch: 191 [32384/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 191 [32512/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 191 [32640/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 191 [32768/50000]\tLoss: 0.0022\tLR: 0.000800\n",
            "Training Epoch: 191 [32896/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 191 [33024/50000]\tLoss: 0.0363\tLR: 0.000800\n",
            "Training Epoch: 191 [33152/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 191 [33280/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 191 [33408/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 191 [33536/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 191 [33664/50000]\tLoss: 0.0242\tLR: 0.000800\n",
            "Training Epoch: 191 [33792/50000]\tLoss: 0.0304\tLR: 0.000800\n",
            "Training Epoch: 191 [33920/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 191 [34048/50000]\tLoss: 0.0231\tLR: 0.000800\n",
            "Training Epoch: 191 [34176/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 191 [34304/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 191 [34432/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 191 [34560/50000]\tLoss: 0.0149\tLR: 0.000800\n",
            "Training Epoch: 191 [34688/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 191 [34816/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 191 [34944/50000]\tLoss: 0.0344\tLR: 0.000800\n",
            "Training Epoch: 191 [35072/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 191 [35200/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 191 [35328/50000]\tLoss: 0.0211\tLR: 0.000800\n",
            "Training Epoch: 191 [35456/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 191 [35584/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 191 [35712/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 191 [35840/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 191 [35968/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 191 [36096/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 191 [36224/50000]\tLoss: 0.0668\tLR: 0.000800\n",
            "Training Epoch: 191 [36352/50000]\tLoss: 0.0149\tLR: 0.000800\n",
            "Training Epoch: 191 [36480/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 191 [36608/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 191 [36736/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 191 [36864/50000]\tLoss: 0.0266\tLR: 0.000800\n",
            "Training Epoch: 191 [36992/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 191 [37120/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 191 [37248/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 191 [37376/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 191 [37504/50000]\tLoss: 0.0271\tLR: 0.000800\n",
            "Training Epoch: 191 [37632/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 191 [37760/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 191 [37888/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 191 [38016/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 191 [38144/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 191 [38272/50000]\tLoss: 0.0238\tLR: 0.000800\n",
            "Training Epoch: 191 [38400/50000]\tLoss: 0.0273\tLR: 0.000800\n",
            "Training Epoch: 191 [38528/50000]\tLoss: 0.0291\tLR: 0.000800\n",
            "Training Epoch: 191 [38656/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 191 [38784/50000]\tLoss: 0.0219\tLR: 0.000800\n",
            "Training Epoch: 191 [38912/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 191 [39040/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 191 [39168/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 191 [39296/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 191 [39424/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 191 [39552/50000]\tLoss: 0.0575\tLR: 0.000800\n",
            "Training Epoch: 191 [39680/50000]\tLoss: 0.0256\tLR: 0.000800\n",
            "Training Epoch: 191 [39808/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 191 [39936/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 191 [40064/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 191 [40192/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 191 [40320/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 191 [40448/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 191 [40576/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 191 [40704/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 191 [40832/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 191 [40960/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 191 [41088/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 191 [41216/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 191 [41344/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 191 [41472/50000]\tLoss: 0.0335\tLR: 0.000800\n",
            "Training Epoch: 191 [41600/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 191 [41728/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 191 [41856/50000]\tLoss: 0.0413\tLR: 0.000800\n",
            "Training Epoch: 191 [41984/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 191 [42112/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 191 [42240/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 191 [42368/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 191 [42496/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 191 [42624/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 191 [42752/50000]\tLoss: 0.0278\tLR: 0.000800\n",
            "Training Epoch: 191 [42880/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 191 [43008/50000]\tLoss: 0.0188\tLR: 0.000800\n",
            "Training Epoch: 191 [43136/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 191 [43264/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 191 [43392/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 191 [43520/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 191 [43648/50000]\tLoss: 0.0246\tLR: 0.000800\n",
            "Training Epoch: 191 [43776/50000]\tLoss: 0.0376\tLR: 0.000800\n",
            "Training Epoch: 191 [43904/50000]\tLoss: 0.0357\tLR: 0.000800\n",
            "Training Epoch: 191 [44032/50000]\tLoss: 0.0264\tLR: 0.000800\n",
            "Training Epoch: 191 [44160/50000]\tLoss: 0.0197\tLR: 0.000800\n",
            "Training Epoch: 191 [44288/50000]\tLoss: 0.0223\tLR: 0.000800\n",
            "Training Epoch: 191 [44416/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 191 [44544/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 191 [44672/50000]\tLoss: 0.0338\tLR: 0.000800\n",
            "Training Epoch: 191 [44800/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 191 [44928/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 191 [45056/50000]\tLoss: 0.0312\tLR: 0.000800\n",
            "Training Epoch: 191 [45184/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 191 [45312/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 191 [45440/50000]\tLoss: 0.0290\tLR: 0.000800\n",
            "Training Epoch: 191 [45568/50000]\tLoss: 0.0442\tLR: 0.000800\n",
            "Training Epoch: 191 [45696/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 191 [45824/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 191 [45952/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 191 [46080/50000]\tLoss: 0.0289\tLR: 0.000800\n",
            "Training Epoch: 191 [46208/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 191 [46336/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 191 [46464/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 191 [46592/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 191 [46720/50000]\tLoss: 0.0513\tLR: 0.000800\n",
            "Training Epoch: 191 [46848/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 191 [46976/50000]\tLoss: 0.0267\tLR: 0.000800\n",
            "Training Epoch: 191 [47104/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 191 [47232/50000]\tLoss: 0.0234\tLR: 0.000800\n",
            "Training Epoch: 191 [47360/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 191 [47488/50000]\tLoss: 0.0932\tLR: 0.000800\n",
            "Training Epoch: 191 [47616/50000]\tLoss: 0.0273\tLR: 0.000800\n",
            "Training Epoch: 191 [47744/50000]\tLoss: 0.0339\tLR: 0.000800\n",
            "Training Epoch: 191 [47872/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 191 [48000/50000]\tLoss: 0.0408\tLR: 0.000800\n",
            "Training Epoch: 191 [48128/50000]\tLoss: 0.0535\tLR: 0.000800\n",
            "Training Epoch: 191 [48256/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 191 [48384/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 191 [48512/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 191 [48640/50000]\tLoss: 0.0341\tLR: 0.000800\n",
            "Training Epoch: 191 [48768/50000]\tLoss: 0.0339\tLR: 0.000800\n",
            "Training Epoch: 191 [48896/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 191 [49024/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 191 [49152/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 191 [49280/50000]\tLoss: 0.0224\tLR: 0.000800\n",
            "Training Epoch: 191 [49408/50000]\tLoss: 0.0191\tLR: 0.000800\n",
            "Training Epoch: 191 [49536/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 191 [49664/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 191 [49792/50000]\tLoss: 0.0336\tLR: 0.000800\n",
            "Training Epoch: 191 [49920/50000]\tLoss: 0.0477\tLR: 0.000800\n",
            "Training Epoch: 191 [50000/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "epoch 191 training time consumed: 33.80s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 212799 GiB | 212798 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 210859 GiB | 210859 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1939 GiB |   1939 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 212799 GiB | 212798 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 210859 GiB | 210859 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1939 GiB |   1939 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 212142 GiB | 212141 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 210203 GiB | 210203 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   1938 GiB |   1938 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 188219 GiB | 188219 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 186279 GiB | 186279 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   1939 GiB |   1939 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   23310 K  |   23310 K  |\n",
            "|       from large pool |      38    |      61    |   10187 K  |   10187 K  |\n",
            "|       from small pool |     187    |     230    |   13122 K  |   13122 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   23310 K  |   23310 K  |\n",
            "|       from large pool |      38    |      61    |   10187 K  |   10187 K  |\n",
            "|       from small pool |     187    |     230    |   13122 K  |   13122 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |    9662 K  |    9662 K  |\n",
            "|       from large pool |      13    |      15    |    4630 K  |    4630 K  |\n",
            "|       from small pool |       7    |      14    |    5031 K  |    5031 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 191, Average loss: 0.0127, Accuracy: 0.7198, Time consumed:4.26s\n",
            "\n",
            "Training Epoch: 192 [128/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 192 [256/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 192 [384/50000]\tLoss: 0.0430\tLR: 0.000800\n",
            "Training Epoch: 192 [512/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 192 [640/50000]\tLoss: 0.0253\tLR: 0.000800\n",
            "Training Epoch: 192 [768/50000]\tLoss: 0.0255\tLR: 0.000800\n",
            "Training Epoch: 192 [896/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 192 [1024/50000]\tLoss: 0.0267\tLR: 0.000800\n",
            "Training Epoch: 192 [1152/50000]\tLoss: 0.0360\tLR: 0.000800\n",
            "Training Epoch: 192 [1280/50000]\tLoss: 0.0903\tLR: 0.000800\n",
            "Training Epoch: 192 [1408/50000]\tLoss: 0.0426\tLR: 0.000800\n",
            "Training Epoch: 192 [1536/50000]\tLoss: 0.0251\tLR: 0.000800\n",
            "Training Epoch: 192 [1664/50000]\tLoss: 0.0312\tLR: 0.000800\n",
            "Training Epoch: 192 [1792/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 192 [1920/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 192 [2048/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 192 [2176/50000]\tLoss: 0.0405\tLR: 0.000800\n",
            "Training Epoch: 192 [2304/50000]\tLoss: 0.0295\tLR: 0.000800\n",
            "Training Epoch: 192 [2432/50000]\tLoss: 0.0311\tLR: 0.000800\n",
            "Training Epoch: 192 [2560/50000]\tLoss: 0.0294\tLR: 0.000800\n",
            "Training Epoch: 192 [2688/50000]\tLoss: 0.0259\tLR: 0.000800\n",
            "Training Epoch: 192 [2816/50000]\tLoss: 0.0353\tLR: 0.000800\n",
            "Training Epoch: 192 [2944/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 192 [3072/50000]\tLoss: 0.0252\tLR: 0.000800\n",
            "Training Epoch: 192 [3200/50000]\tLoss: 0.0560\tLR: 0.000800\n",
            "Training Epoch: 192 [3328/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 192 [3456/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 192 [3584/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 192 [3712/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 192 [3840/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 192 [3968/50000]\tLoss: 0.0259\tLR: 0.000800\n",
            "Training Epoch: 192 [4096/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 192 [4224/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 192 [4352/50000]\tLoss: 0.0601\tLR: 0.000800\n",
            "Training Epoch: 192 [4480/50000]\tLoss: 0.0439\tLR: 0.000800\n",
            "Training Epoch: 192 [4608/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 192 [4736/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 192 [4864/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 192 [4992/50000]\tLoss: 0.0390\tLR: 0.000800\n",
            "Training Epoch: 192 [5120/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 192 [5248/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 192 [5376/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 192 [5504/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 192 [5632/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 192 [5760/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 192 [5888/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 192 [6016/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 192 [6144/50000]\tLoss: 0.0256\tLR: 0.000800\n",
            "Training Epoch: 192 [6272/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 192 [6400/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 192 [6528/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 192 [6656/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 192 [6784/50000]\tLoss: 0.0192\tLR: 0.000800\n",
            "Training Epoch: 192 [6912/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 192 [7040/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 192 [7168/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 192 [7296/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 192 [7424/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 192 [7552/50000]\tLoss: 0.0352\tLR: 0.000800\n",
            "Training Epoch: 192 [7680/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 192 [7808/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 192 [7936/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 192 [8064/50000]\tLoss: 0.0417\tLR: 0.000800\n",
            "Training Epoch: 192 [8192/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 192 [8320/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 192 [8448/50000]\tLoss: 0.0024\tLR: 0.000800\n",
            "Training Epoch: 192 [8576/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 192 [8704/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 192 [8832/50000]\tLoss: 0.0387\tLR: 0.000800\n",
            "Training Epoch: 192 [8960/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 192 [9088/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 192 [9216/50000]\tLoss: 0.0340\tLR: 0.000800\n",
            "Training Epoch: 192 [9344/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 192 [9472/50000]\tLoss: 0.0136\tLR: 0.000800\n",
            "Training Epoch: 192 [9600/50000]\tLoss: 0.0168\tLR: 0.000800\n",
            "Training Epoch: 192 [9728/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 192 [9856/50000]\tLoss: 0.0514\tLR: 0.000800\n",
            "Training Epoch: 192 [9984/50000]\tLoss: 0.0293\tLR: 0.000800\n",
            "Training Epoch: 192 [10112/50000]\tLoss: 0.0297\tLR: 0.000800\n",
            "Training Epoch: 192 [10240/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 192 [10368/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 192 [10496/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 192 [10624/50000]\tLoss: 0.0309\tLR: 0.000800\n",
            "Training Epoch: 192 [10752/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 192 [10880/50000]\tLoss: 0.0224\tLR: 0.000800\n",
            "Training Epoch: 192 [11008/50000]\tLoss: 0.0545\tLR: 0.000800\n",
            "Training Epoch: 192 [11136/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 192 [11264/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 192 [11392/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 192 [11520/50000]\tLoss: 0.0149\tLR: 0.000800\n",
            "Training Epoch: 192 [11648/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 192 [11776/50000]\tLoss: 0.0262\tLR: 0.000800\n",
            "Training Epoch: 192 [11904/50000]\tLoss: 0.0407\tLR: 0.000800\n",
            "Training Epoch: 192 [12032/50000]\tLoss: 0.0312\tLR: 0.000800\n",
            "Training Epoch: 192 [12160/50000]\tLoss: 0.0324\tLR: 0.000800\n",
            "Training Epoch: 192 [12288/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 192 [12416/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 192 [12544/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 192 [12672/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 192 [12800/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 192 [12928/50000]\tLoss: 0.0612\tLR: 0.000800\n",
            "Training Epoch: 192 [13056/50000]\tLoss: 0.0250\tLR: 0.000800\n",
            "Training Epoch: 192 [13184/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 192 [13312/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 192 [13440/50000]\tLoss: 0.0320\tLR: 0.000800\n",
            "Training Epoch: 192 [13568/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 192 [13696/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 192 [13824/50000]\tLoss: 0.0246\tLR: 0.000800\n",
            "Training Epoch: 192 [13952/50000]\tLoss: 0.0368\tLR: 0.000800\n",
            "Training Epoch: 192 [14080/50000]\tLoss: 0.0258\tLR: 0.000800\n",
            "Training Epoch: 192 [14208/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 192 [14336/50000]\tLoss: 0.0213\tLR: 0.000800\n",
            "Training Epoch: 192 [14464/50000]\tLoss: 0.0145\tLR: 0.000800\n",
            "Training Epoch: 192 [14592/50000]\tLoss: 0.0686\tLR: 0.000800\n",
            "Training Epoch: 192 [14720/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 192 [14848/50000]\tLoss: 0.0258\tLR: 0.000800\n",
            "Training Epoch: 192 [14976/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 192 [15104/50000]\tLoss: 0.0333\tLR: 0.000800\n",
            "Training Epoch: 192 [15232/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 192 [15360/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 192 [15488/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 192 [15616/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 192 [15744/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 192 [15872/50000]\tLoss: 0.0499\tLR: 0.000800\n",
            "Training Epoch: 192 [16000/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 192 [16128/50000]\tLoss: 0.0281\tLR: 0.000800\n",
            "Training Epoch: 192 [16256/50000]\tLoss: 0.0458\tLR: 0.000800\n",
            "Training Epoch: 192 [16384/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 192 [16512/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 192 [16640/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 192 [16768/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 192 [16896/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 192 [17024/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 192 [17152/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 192 [17280/50000]\tLoss: 0.0209\tLR: 0.000800\n",
            "Training Epoch: 192 [17408/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 192 [17536/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 192 [17664/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 192 [17792/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 192 [17920/50000]\tLoss: 0.0165\tLR: 0.000800\n",
            "Training Epoch: 192 [18048/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 192 [18176/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 192 [18304/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 192 [18432/50000]\tLoss: 0.0358\tLR: 0.000800\n",
            "Training Epoch: 192 [18560/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 192 [18688/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 192 [18816/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 192 [18944/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 192 [19072/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 192 [19200/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 192 [19328/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 192 [19456/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 192 [19584/50000]\tLoss: 0.0172\tLR: 0.000800\n",
            "Training Epoch: 192 [19712/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 192 [19840/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 192 [19968/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 192 [20096/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 192 [20224/50000]\tLoss: 0.0604\tLR: 0.000800\n",
            "Training Epoch: 192 [20352/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 192 [20480/50000]\tLoss: 0.0203\tLR: 0.000800\n",
            "Training Epoch: 192 [20608/50000]\tLoss: 0.0473\tLR: 0.000800\n",
            "Training Epoch: 192 [20736/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 192 [20864/50000]\tLoss: 0.0662\tLR: 0.000800\n",
            "Training Epoch: 192 [20992/50000]\tLoss: 0.0168\tLR: 0.000800\n",
            "Training Epoch: 192 [21120/50000]\tLoss: 0.0219\tLR: 0.000800\n",
            "Training Epoch: 192 [21248/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 192 [21376/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 192 [21504/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 192 [21632/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 192 [21760/50000]\tLoss: 0.0504\tLR: 0.000800\n",
            "Training Epoch: 192 [21888/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 192 [22016/50000]\tLoss: 0.0203\tLR: 0.000800\n",
            "Training Epoch: 192 [22144/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 192 [22272/50000]\tLoss: 0.0145\tLR: 0.000800\n",
            "Training Epoch: 192 [22400/50000]\tLoss: 0.0263\tLR: 0.000800\n",
            "Training Epoch: 192 [22528/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 192 [22656/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 192 [22784/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 192 [22912/50000]\tLoss: 0.0300\tLR: 0.000800\n",
            "Training Epoch: 192 [23040/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 192 [23168/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 192 [23296/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 192 [23424/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 192 [23552/50000]\tLoss: 0.0411\tLR: 0.000800\n",
            "Training Epoch: 192 [23680/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 192 [23808/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 192 [23936/50000]\tLoss: 0.0216\tLR: 0.000800\n",
            "Training Epoch: 192 [24064/50000]\tLoss: 0.0267\tLR: 0.000800\n",
            "Training Epoch: 192 [24192/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 192 [24320/50000]\tLoss: 0.0364\tLR: 0.000800\n",
            "Training Epoch: 192 [24448/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 192 [24576/50000]\tLoss: 0.0162\tLR: 0.000800\n",
            "Training Epoch: 192 [24704/50000]\tLoss: 0.0020\tLR: 0.000800\n",
            "Training Epoch: 192 [24832/50000]\tLoss: 0.0181\tLR: 0.000800\n",
            "Training Epoch: 192 [24960/50000]\tLoss: 0.0418\tLR: 0.000800\n",
            "Training Epoch: 192 [25088/50000]\tLoss: 0.0310\tLR: 0.000800\n",
            "Training Epoch: 192 [25216/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 192 [25344/50000]\tLoss: 0.0145\tLR: 0.000800\n",
            "Training Epoch: 192 [25472/50000]\tLoss: 0.0364\tLR: 0.000800\n",
            "Training Epoch: 192 [25600/50000]\tLoss: 0.0198\tLR: 0.000800\n",
            "Training Epoch: 192 [25728/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 192 [25856/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 192 [25984/50000]\tLoss: 0.0318\tLR: 0.000800\n",
            "Training Epoch: 192 [26112/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 192 [26240/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 192 [26368/50000]\tLoss: 0.0136\tLR: 0.000800\n",
            "Training Epoch: 192 [26496/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 192 [26624/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 192 [26752/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 192 [26880/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 192 [27008/50000]\tLoss: 0.0324\tLR: 0.000800\n",
            "Training Epoch: 192 [27136/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 192 [27264/50000]\tLoss: 0.0231\tLR: 0.000800\n",
            "Training Epoch: 192 [27392/50000]\tLoss: 0.0160\tLR: 0.000800\n",
            "Training Epoch: 192 [27520/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 192 [27648/50000]\tLoss: 0.0251\tLR: 0.000800\n",
            "Training Epoch: 192 [27776/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 192 [27904/50000]\tLoss: 0.0333\tLR: 0.000800\n",
            "Training Epoch: 192 [28032/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 192 [28160/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 192 [28288/50000]\tLoss: 0.0577\tLR: 0.000800\n",
            "Training Epoch: 192 [28416/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 192 [28544/50000]\tLoss: 0.0188\tLR: 0.000800\n",
            "Training Epoch: 192 [28672/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 192 [28800/50000]\tLoss: 0.0399\tLR: 0.000800\n",
            "Training Epoch: 192 [28928/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 192 [29056/50000]\tLoss: 0.0187\tLR: 0.000800\n",
            "Training Epoch: 192 [29184/50000]\tLoss: 0.0270\tLR: 0.000800\n",
            "Training Epoch: 192 [29312/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 192 [29440/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 192 [29568/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 192 [29696/50000]\tLoss: 0.0253\tLR: 0.000800\n",
            "Training Epoch: 192 [29824/50000]\tLoss: 0.0233\tLR: 0.000800\n",
            "Training Epoch: 192 [29952/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 192 [30080/50000]\tLoss: 0.0358\tLR: 0.000800\n",
            "Training Epoch: 192 [30208/50000]\tLoss: 0.0350\tLR: 0.000800\n",
            "Training Epoch: 192 [30336/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 192 [30464/50000]\tLoss: 0.0340\tLR: 0.000800\n",
            "Training Epoch: 192 [30592/50000]\tLoss: 0.0229\tLR: 0.000800\n",
            "Training Epoch: 192 [30720/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 192 [30848/50000]\tLoss: 0.0337\tLR: 0.000800\n",
            "Training Epoch: 192 [30976/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 192 [31104/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 192 [31232/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 192 [31360/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 192 [31488/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 192 [31616/50000]\tLoss: 0.0142\tLR: 0.000800\n",
            "Training Epoch: 192 [31744/50000]\tLoss: 0.0267\tLR: 0.000800\n",
            "Training Epoch: 192 [31872/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 192 [32000/50000]\tLoss: 0.0329\tLR: 0.000800\n",
            "Training Epoch: 192 [32128/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 192 [32256/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 192 [32384/50000]\tLoss: 0.0291\tLR: 0.000800\n",
            "Training Epoch: 192 [32512/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 192 [32640/50000]\tLoss: 0.0311\tLR: 0.000800\n",
            "Training Epoch: 192 [32768/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 192 [32896/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 192 [33024/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 192 [33152/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 192 [33280/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 192 [33408/50000]\tLoss: 0.0233\tLR: 0.000800\n",
            "Training Epoch: 192 [33536/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 192 [33664/50000]\tLoss: 0.0165\tLR: 0.000800\n",
            "Training Epoch: 192 [33792/50000]\tLoss: 0.0197\tLR: 0.000800\n",
            "Training Epoch: 192 [33920/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 192 [34048/50000]\tLoss: 0.0295\tLR: 0.000800\n",
            "Training Epoch: 192 [34176/50000]\tLoss: 0.0645\tLR: 0.000800\n",
            "Training Epoch: 192 [34304/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 192 [34432/50000]\tLoss: 0.0535\tLR: 0.000800\n",
            "Training Epoch: 192 [34560/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 192 [34688/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 192 [34816/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 192 [34944/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 192 [35072/50000]\tLoss: 0.0382\tLR: 0.000800\n",
            "Training Epoch: 192 [35200/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 192 [35328/50000]\tLoss: 0.0219\tLR: 0.000800\n",
            "Training Epoch: 192 [35456/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 192 [35584/50000]\tLoss: 0.0308\tLR: 0.000800\n",
            "Training Epoch: 192 [35712/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 192 [35840/50000]\tLoss: 0.0317\tLR: 0.000800\n",
            "Training Epoch: 192 [35968/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 192 [36096/50000]\tLoss: 0.0225\tLR: 0.000800\n",
            "Training Epoch: 192 [36224/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 192 [36352/50000]\tLoss: 0.0147\tLR: 0.000800\n",
            "Training Epoch: 192 [36480/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 192 [36608/50000]\tLoss: 0.0597\tLR: 0.000800\n",
            "Training Epoch: 192 [36736/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 192 [36864/50000]\tLoss: 0.1051\tLR: 0.000800\n",
            "Training Epoch: 192 [36992/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 192 [37120/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 192 [37248/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 192 [37376/50000]\tLoss: 0.0373\tLR: 0.000800\n",
            "Training Epoch: 192 [37504/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 192 [37632/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 192 [37760/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 192 [37888/50000]\tLoss: 0.0145\tLR: 0.000800\n",
            "Training Epoch: 192 [38016/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 192 [38144/50000]\tLoss: 0.0199\tLR: 0.000800\n",
            "Training Epoch: 192 [38272/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 192 [38400/50000]\tLoss: 0.0643\tLR: 0.000800\n",
            "Training Epoch: 192 [38528/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 192 [38656/50000]\tLoss: 0.0313\tLR: 0.000800\n",
            "Training Epoch: 192 [38784/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 192 [38912/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 192 [39040/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 192 [39168/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 192 [39296/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 192 [39424/50000]\tLoss: 0.0251\tLR: 0.000800\n",
            "Training Epoch: 192 [39552/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 192 [39680/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 192 [39808/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 192 [39936/50000]\tLoss: 0.0397\tLR: 0.000800\n",
            "Training Epoch: 192 [40064/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 192 [40192/50000]\tLoss: 0.0189\tLR: 0.000800\n",
            "Training Epoch: 192 [40320/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 192 [40448/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 192 [40576/50000]\tLoss: 0.0831\tLR: 0.000800\n",
            "Training Epoch: 192 [40704/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 192 [40832/50000]\tLoss: 0.0016\tLR: 0.000800\n",
            "Training Epoch: 192 [40960/50000]\tLoss: 0.0212\tLR: 0.000800\n",
            "Training Epoch: 192 [41088/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 192 [41216/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 192 [41344/50000]\tLoss: 0.0592\tLR: 0.000800\n",
            "Training Epoch: 192 [41472/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 192 [41600/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 192 [41728/50000]\tLoss: 0.0328\tLR: 0.000800\n",
            "Training Epoch: 192 [41856/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 192 [41984/50000]\tLoss: 0.0426\tLR: 0.000800\n",
            "Training Epoch: 192 [42112/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 192 [42240/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 192 [42368/50000]\tLoss: 0.0024\tLR: 0.000800\n",
            "Training Epoch: 192 [42496/50000]\tLoss: 0.0213\tLR: 0.000800\n",
            "Training Epoch: 192 [42624/50000]\tLoss: 0.0212\tLR: 0.000800\n",
            "Training Epoch: 192 [42752/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 192 [42880/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 192 [43008/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 192 [43136/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 192 [43264/50000]\tLoss: 0.0215\tLR: 0.000800\n",
            "Training Epoch: 192 [43392/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 192 [43520/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 192 [43648/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 192 [43776/50000]\tLoss: 0.0223\tLR: 0.000800\n",
            "Training Epoch: 192 [43904/50000]\tLoss: 0.0145\tLR: 0.000800\n",
            "Training Epoch: 192 [44032/50000]\tLoss: 0.0409\tLR: 0.000800\n",
            "Training Epoch: 192 [44160/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 192 [44288/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 192 [44416/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 192 [44544/50000]\tLoss: 0.0193\tLR: 0.000800\n",
            "Training Epoch: 192 [44672/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 192 [44800/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 192 [44928/50000]\tLoss: 0.0386\tLR: 0.000800\n",
            "Training Epoch: 192 [45056/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 192 [45184/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 192 [45312/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 192 [45440/50000]\tLoss: 0.0399\tLR: 0.000800\n",
            "Training Epoch: 192 [45568/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 192 [45696/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 192 [45824/50000]\tLoss: 0.0307\tLR: 0.000800\n",
            "Training Epoch: 192 [45952/50000]\tLoss: 0.0337\tLR: 0.000800\n",
            "Training Epoch: 192 [46080/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 192 [46208/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 192 [46336/50000]\tLoss: 0.0018\tLR: 0.000800\n",
            "Training Epoch: 192 [46464/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 192 [46592/50000]\tLoss: 0.0254\tLR: 0.000800\n",
            "Training Epoch: 192 [46720/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 192 [46848/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 192 [46976/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 192 [47104/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 192 [47232/50000]\tLoss: 0.0293\tLR: 0.000800\n",
            "Training Epoch: 192 [47360/50000]\tLoss: 0.0192\tLR: 0.000800\n",
            "Training Epoch: 192 [47488/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 192 [47616/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 192 [47744/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 192 [47872/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 192 [48000/50000]\tLoss: 0.0241\tLR: 0.000800\n",
            "Training Epoch: 192 [48128/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 192 [48256/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 192 [48384/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 192 [48512/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 192 [48640/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 192 [48768/50000]\tLoss: 0.0220\tLR: 0.000800\n",
            "Training Epoch: 192 [48896/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 192 [49024/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 192 [49152/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 192 [49280/50000]\tLoss: 0.0614\tLR: 0.000800\n",
            "Training Epoch: 192 [49408/50000]\tLoss: 0.0288\tLR: 0.000800\n",
            "Training Epoch: 192 [49536/50000]\tLoss: 0.0149\tLR: 0.000800\n",
            "Training Epoch: 192 [49664/50000]\tLoss: 0.0285\tLR: 0.000800\n",
            "Training Epoch: 192 [49792/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 192 [49920/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 192 [50000/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "epoch 192 training time consumed: 34.35s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 213913 GiB | 213913 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 211963 GiB | 211963 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1949 GiB |   1949 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 213913 GiB | 213913 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 211963 GiB | 211963 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1949 GiB |   1949 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 213253 GiB | 213252 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 211304 GiB | 211303 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   1948 GiB |   1948 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 189205 GiB | 189204 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 187255 GiB | 187254 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   1950 GiB |   1950 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   23432 K  |   23432 K  |\n",
            "|       from large pool |      38    |      61    |   10240 K  |   10240 K  |\n",
            "|       from small pool |     187    |     230    |   13191 K  |   13191 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   23432 K  |   23432 K  |\n",
            "|       from large pool |      38    |      61    |   10240 K  |   10240 K  |\n",
            "|       from small pool |     187    |     230    |   13191 K  |   13191 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |    9713 K  |    9713 K  |\n",
            "|       from large pool |      13    |      15    |    4655 K  |    4655 K  |\n",
            "|       from small pool |       7    |      14    |    5058 K  |    5058 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 192, Average loss: 0.0129, Accuracy: 0.7191, Time consumed:2.92s\n",
            "\n",
            "Training Epoch: 193 [128/50000]\tLoss: 0.0202\tLR: 0.000800\n",
            "Training Epoch: 193 [256/50000]\tLoss: 0.0347\tLR: 0.000800\n",
            "Training Epoch: 193 [384/50000]\tLoss: 0.0470\tLR: 0.000800\n",
            "Training Epoch: 193 [512/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 193 [640/50000]\tLoss: 0.0208\tLR: 0.000800\n",
            "Training Epoch: 193 [768/50000]\tLoss: 0.0710\tLR: 0.000800\n",
            "Training Epoch: 193 [896/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 193 [1024/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 193 [1152/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 193 [1280/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 193 [1408/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 193 [1536/50000]\tLoss: 0.0325\tLR: 0.000800\n",
            "Training Epoch: 193 [1664/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 193 [1792/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 193 [1920/50000]\tLoss: 0.0567\tLR: 0.000800\n",
            "Training Epoch: 193 [2048/50000]\tLoss: 0.0598\tLR: 0.000800\n",
            "Training Epoch: 193 [2176/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 193 [2304/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 193 [2432/50000]\tLoss: 0.0239\tLR: 0.000800\n",
            "Training Epoch: 193 [2560/50000]\tLoss: 0.0245\tLR: 0.000800\n",
            "Training Epoch: 193 [2688/50000]\tLoss: 0.0229\tLR: 0.000800\n",
            "Training Epoch: 193 [2816/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 193 [2944/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 193 [3072/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 193 [3200/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 193 [3328/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 193 [3456/50000]\tLoss: 0.0226\tLR: 0.000800\n",
            "Training Epoch: 193 [3584/50000]\tLoss: 0.0467\tLR: 0.000800\n",
            "Training Epoch: 193 [3712/50000]\tLoss: 0.0308\tLR: 0.000800\n",
            "Training Epoch: 193 [3840/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 193 [3968/50000]\tLoss: 0.0212\tLR: 0.000800\n",
            "Training Epoch: 193 [4096/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 193 [4224/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 193 [4352/50000]\tLoss: 0.0252\tLR: 0.000800\n",
            "Training Epoch: 193 [4480/50000]\tLoss: 0.0333\tLR: 0.000800\n",
            "Training Epoch: 193 [4608/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 193 [4736/50000]\tLoss: 0.0517\tLR: 0.000800\n",
            "Training Epoch: 193 [4864/50000]\tLoss: 0.0162\tLR: 0.000800\n",
            "Training Epoch: 193 [4992/50000]\tLoss: 0.0215\tLR: 0.000800\n",
            "Training Epoch: 193 [5120/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 193 [5248/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 193 [5376/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 193 [5504/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 193 [5632/50000]\tLoss: 0.0401\tLR: 0.000800\n",
            "Training Epoch: 193 [5760/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 193 [5888/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 193 [6016/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 193 [6144/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 193 [6272/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 193 [6400/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 193 [6528/50000]\tLoss: 0.0217\tLR: 0.000800\n",
            "Training Epoch: 193 [6656/50000]\tLoss: 0.0018\tLR: 0.000800\n",
            "Training Epoch: 193 [6784/50000]\tLoss: 0.0517\tLR: 0.000800\n",
            "Training Epoch: 193 [6912/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 193 [7040/50000]\tLoss: 0.0509\tLR: 0.000800\n",
            "Training Epoch: 193 [7168/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 193 [7296/50000]\tLoss: 0.0226\tLR: 0.000800\n",
            "Training Epoch: 193 [7424/50000]\tLoss: 0.0275\tLR: 0.000800\n",
            "Training Epoch: 193 [7552/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 193 [7680/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 193 [7808/50000]\tLoss: 0.0157\tLR: 0.000800\n",
            "Training Epoch: 193 [7936/50000]\tLoss: 0.0274\tLR: 0.000800\n",
            "Training Epoch: 193 [8064/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 193 [8192/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 193 [8320/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 193 [8448/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 193 [8576/50000]\tLoss: 0.0310\tLR: 0.000800\n",
            "Training Epoch: 193 [8704/50000]\tLoss: 0.0231\tLR: 0.000800\n",
            "Training Epoch: 193 [8832/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 193 [8960/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 193 [9088/50000]\tLoss: 0.0016\tLR: 0.000800\n",
            "Training Epoch: 193 [9216/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 193 [9344/50000]\tLoss: 0.0602\tLR: 0.000800\n",
            "Training Epoch: 193 [9472/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 193 [9600/50000]\tLoss: 0.0189\tLR: 0.000800\n",
            "Training Epoch: 193 [9728/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 193 [9856/50000]\tLoss: 0.0306\tLR: 0.000800\n",
            "Training Epoch: 193 [9984/50000]\tLoss: 0.0225\tLR: 0.000800\n",
            "Training Epoch: 193 [10112/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 193 [10240/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 193 [10368/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 193 [10496/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 193 [10624/50000]\tLoss: 0.0718\tLR: 0.000800\n",
            "Training Epoch: 193 [10752/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 193 [10880/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 193 [11008/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 193 [11136/50000]\tLoss: 0.0241\tLR: 0.000800\n",
            "Training Epoch: 193 [11264/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 193 [11392/50000]\tLoss: 0.0316\tLR: 0.000800\n",
            "Training Epoch: 193 [11520/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 193 [11648/50000]\tLoss: 0.0330\tLR: 0.000800\n",
            "Training Epoch: 193 [11776/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 193 [11904/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 193 [12032/50000]\tLoss: 0.0265\tLR: 0.000800\n",
            "Training Epoch: 193 [12160/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 193 [12288/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 193 [12416/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 193 [12544/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 193 [12672/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 193 [12800/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 193 [12928/50000]\tLoss: 0.0264\tLR: 0.000800\n",
            "Training Epoch: 193 [13056/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 193 [13184/50000]\tLoss: 0.0321\tLR: 0.000800\n",
            "Training Epoch: 193 [13312/50000]\tLoss: 0.0351\tLR: 0.000800\n",
            "Training Epoch: 193 [13440/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 193 [13568/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 193 [13696/50000]\tLoss: 0.0174\tLR: 0.000800\n",
            "Training Epoch: 193 [13824/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 193 [13952/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 193 [14080/50000]\tLoss: 0.0252\tLR: 0.000800\n",
            "Training Epoch: 193 [14208/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 193 [14336/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 193 [14464/50000]\tLoss: 0.0553\tLR: 0.000800\n",
            "Training Epoch: 193 [14592/50000]\tLoss: 0.0269\tLR: 0.000800\n",
            "Training Epoch: 193 [14720/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 193 [14848/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 193 [14976/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 193 [15104/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 193 [15232/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 193 [15360/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 193 [15488/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 193 [15616/50000]\tLoss: 0.0262\tLR: 0.000800\n",
            "Training Epoch: 193 [15744/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 193 [15872/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 193 [16000/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 193 [16128/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 193 [16256/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 193 [16384/50000]\tLoss: 0.0200\tLR: 0.000800\n",
            "Training Epoch: 193 [16512/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 193 [16640/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 193 [16768/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 193 [16896/50000]\tLoss: 0.0136\tLR: 0.000800\n",
            "Training Epoch: 193 [17024/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 193 [17152/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 193 [17280/50000]\tLoss: 0.0253\tLR: 0.000800\n",
            "Training Epoch: 193 [17408/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 193 [17536/50000]\tLoss: 0.0205\tLR: 0.000800\n",
            "Training Epoch: 193 [17664/50000]\tLoss: 0.0272\tLR: 0.000800\n",
            "Training Epoch: 193 [17792/50000]\tLoss: 0.0313\tLR: 0.000800\n",
            "Training Epoch: 193 [17920/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 193 [18048/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 193 [18176/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 193 [18304/50000]\tLoss: 0.0149\tLR: 0.000800\n",
            "Training Epoch: 193 [18432/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 193 [18560/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 193 [18688/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 193 [18816/50000]\tLoss: 0.0338\tLR: 0.000800\n",
            "Training Epoch: 193 [18944/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 193 [19072/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 193 [19200/50000]\tLoss: 0.0274\tLR: 0.000800\n",
            "Training Epoch: 193 [19328/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 193 [19456/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 193 [19584/50000]\tLoss: 0.0400\tLR: 0.000800\n",
            "Training Epoch: 193 [19712/50000]\tLoss: 0.0267\tLR: 0.000800\n",
            "Training Epoch: 193 [19840/50000]\tLoss: 0.0425\tLR: 0.000800\n",
            "Training Epoch: 193 [19968/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 193 [20096/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 193 [20224/50000]\tLoss: 0.0386\tLR: 0.000800\n",
            "Training Epoch: 193 [20352/50000]\tLoss: 0.0193\tLR: 0.000800\n",
            "Training Epoch: 193 [20480/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 193 [20608/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 193 [20736/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 193 [20864/50000]\tLoss: 0.0295\tLR: 0.000800\n",
            "Training Epoch: 193 [20992/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 193 [21120/50000]\tLoss: 0.0162\tLR: 0.000800\n",
            "Training Epoch: 193 [21248/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 193 [21376/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 193 [21504/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 193 [21632/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 193 [21760/50000]\tLoss: 0.0234\tLR: 0.000800\n",
            "Training Epoch: 193 [21888/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 193 [22016/50000]\tLoss: 0.0407\tLR: 0.000800\n",
            "Training Epoch: 193 [22144/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 193 [22272/50000]\tLoss: 0.0215\tLR: 0.000800\n",
            "Training Epoch: 193 [22400/50000]\tLoss: 0.0400\tLR: 0.000800\n",
            "Training Epoch: 193 [22528/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 193 [22656/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 193 [22784/50000]\tLoss: 0.0251\tLR: 0.000800\n",
            "Training Epoch: 193 [22912/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 193 [23040/50000]\tLoss: 0.0303\tLR: 0.000800\n",
            "Training Epoch: 193 [23168/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 193 [23296/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 193 [23424/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 193 [23552/50000]\tLoss: 0.0220\tLR: 0.000800\n",
            "Training Epoch: 193 [23680/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 193 [23808/50000]\tLoss: 0.0181\tLR: 0.000800\n",
            "Training Epoch: 193 [23936/50000]\tLoss: 0.0360\tLR: 0.000800\n",
            "Training Epoch: 193 [24064/50000]\tLoss: 0.0315\tLR: 0.000800\n",
            "Training Epoch: 193 [24192/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 193 [24320/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 193 [24448/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 193 [24576/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 193 [24704/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 193 [24832/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 193 [24960/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 193 [25088/50000]\tLoss: 0.0715\tLR: 0.000800\n",
            "Training Epoch: 193 [25216/50000]\tLoss: 0.0290\tLR: 0.000800\n",
            "Training Epoch: 193 [25344/50000]\tLoss: 0.0207\tLR: 0.000800\n",
            "Training Epoch: 193 [25472/50000]\tLoss: 0.0382\tLR: 0.000800\n",
            "Training Epoch: 193 [25600/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 193 [25728/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 193 [25856/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 193 [25984/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 193 [26112/50000]\tLoss: 0.0412\tLR: 0.000800\n",
            "Training Epoch: 193 [26240/50000]\tLoss: 0.0193\tLR: 0.000800\n",
            "Training Epoch: 193 [26368/50000]\tLoss: 0.0231\tLR: 0.000800\n",
            "Training Epoch: 193 [26496/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 193 [26624/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 193 [26752/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 193 [26880/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 193 [27008/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 193 [27136/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 193 [27264/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 193 [27392/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 193 [27520/50000]\tLoss: 0.0348\tLR: 0.000800\n",
            "Training Epoch: 193 [27648/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 193 [27776/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 193 [27904/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 193 [28032/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 193 [28160/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 193 [28288/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 193 [28416/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 193 [28544/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 193 [28672/50000]\tLoss: 0.0285\tLR: 0.000800\n",
            "Training Epoch: 193 [28800/50000]\tLoss: 0.0219\tLR: 0.000800\n",
            "Training Epoch: 193 [28928/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 193 [29056/50000]\tLoss: 0.0408\tLR: 0.000800\n",
            "Training Epoch: 193 [29184/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 193 [29312/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 193 [29440/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 193 [29568/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 193 [29696/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 193 [29824/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 193 [29952/50000]\tLoss: 0.0203\tLR: 0.000800\n",
            "Training Epoch: 193 [30080/50000]\tLoss: 0.0426\tLR: 0.000800\n",
            "Training Epoch: 193 [30208/50000]\tLoss: 0.0302\tLR: 0.000800\n",
            "Training Epoch: 193 [30336/50000]\tLoss: 0.0136\tLR: 0.000800\n",
            "Training Epoch: 193 [30464/50000]\tLoss: 0.0244\tLR: 0.000800\n",
            "Training Epoch: 193 [30592/50000]\tLoss: 0.0577\tLR: 0.000800\n",
            "Training Epoch: 193 [30720/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 193 [30848/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 193 [30976/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 193 [31104/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 193 [31232/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 193 [31360/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 193 [31488/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 193 [31616/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 193 [31744/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 193 [31872/50000]\tLoss: 0.0543\tLR: 0.000800\n",
            "Training Epoch: 193 [32000/50000]\tLoss: 0.0149\tLR: 0.000800\n",
            "Training Epoch: 193 [32128/50000]\tLoss: 0.0295\tLR: 0.000800\n",
            "Training Epoch: 193 [32256/50000]\tLoss: 0.0562\tLR: 0.000800\n",
            "Training Epoch: 193 [32384/50000]\tLoss: 0.0410\tLR: 0.000800\n",
            "Training Epoch: 193 [32512/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 193 [32640/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 193 [32768/50000]\tLoss: 0.0470\tLR: 0.000800\n",
            "Training Epoch: 193 [32896/50000]\tLoss: 0.0340\tLR: 0.000800\n",
            "Training Epoch: 193 [33024/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 193 [33152/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 193 [33280/50000]\tLoss: 0.0215\tLR: 0.000800\n",
            "Training Epoch: 193 [33408/50000]\tLoss: 0.0582\tLR: 0.000800\n",
            "Training Epoch: 193 [33536/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 193 [33664/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 193 [33792/50000]\tLoss: 0.0490\tLR: 0.000800\n",
            "Training Epoch: 193 [33920/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 193 [34048/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 193 [34176/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 193 [34304/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 193 [34432/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 193 [34560/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 193 [34688/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 193 [34816/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 193 [34944/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 193 [35072/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 193 [35200/50000]\tLoss: 0.0243\tLR: 0.000800\n",
            "Training Epoch: 193 [35328/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 193 [35456/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 193 [35584/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 193 [35712/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 193 [35840/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 193 [35968/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 193 [36096/50000]\tLoss: 0.0231\tLR: 0.000800\n",
            "Training Epoch: 193 [36224/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 193 [36352/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 193 [36480/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 193 [36608/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 193 [36736/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 193 [36864/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 193 [36992/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 193 [37120/50000]\tLoss: 0.0413\tLR: 0.000800\n",
            "Training Epoch: 193 [37248/50000]\tLoss: 0.0279\tLR: 0.000800\n",
            "Training Epoch: 193 [37376/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 193 [37504/50000]\tLoss: 0.0145\tLR: 0.000800\n",
            "Training Epoch: 193 [37632/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 193 [37760/50000]\tLoss: 0.0424\tLR: 0.000800\n",
            "Training Epoch: 193 [37888/50000]\tLoss: 0.0281\tLR: 0.000800\n",
            "Training Epoch: 193 [38016/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 193 [38144/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 193 [38272/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 193 [38400/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 193 [38528/50000]\tLoss: 0.0385\tLR: 0.000800\n",
            "Training Epoch: 193 [38656/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 193 [38784/50000]\tLoss: 0.0429\tLR: 0.000800\n",
            "Training Epoch: 193 [38912/50000]\tLoss: 0.0147\tLR: 0.000800\n",
            "Training Epoch: 193 [39040/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 193 [39168/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 193 [39296/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 193 [39424/50000]\tLoss: 0.0483\tLR: 0.000800\n",
            "Training Epoch: 193 [39552/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 193 [39680/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 193 [39808/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 193 [39936/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 193 [40064/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 193 [40192/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 193 [40320/50000]\tLoss: 0.0842\tLR: 0.000800\n",
            "Training Epoch: 193 [40448/50000]\tLoss: 0.0359\tLR: 0.000800\n",
            "Training Epoch: 193 [40576/50000]\tLoss: 0.0360\tLR: 0.000800\n",
            "Training Epoch: 193 [40704/50000]\tLoss: 0.0238\tLR: 0.000800\n",
            "Training Epoch: 193 [40832/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 193 [40960/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 193 [41088/50000]\tLoss: 0.0250\tLR: 0.000800\n",
            "Training Epoch: 193 [41216/50000]\tLoss: 0.0269\tLR: 0.000800\n",
            "Training Epoch: 193 [41344/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 193 [41472/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 193 [41600/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 193 [41728/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 193 [41856/50000]\tLoss: 0.0753\tLR: 0.000800\n",
            "Training Epoch: 193 [41984/50000]\tLoss: 0.0496\tLR: 0.000800\n",
            "Training Epoch: 193 [42112/50000]\tLoss: 0.0350\tLR: 0.000800\n",
            "Training Epoch: 193 [42240/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 193 [42368/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 193 [42496/50000]\tLoss: 0.0237\tLR: 0.000800\n",
            "Training Epoch: 193 [42624/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 193 [42752/50000]\tLoss: 0.0246\tLR: 0.000800\n",
            "Training Epoch: 193 [42880/50000]\tLoss: 0.0296\tLR: 0.000800\n",
            "Training Epoch: 193 [43008/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 193 [43136/50000]\tLoss: 0.0263\tLR: 0.000800\n",
            "Training Epoch: 193 [43264/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 193 [43392/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 193 [43520/50000]\tLoss: 0.0403\tLR: 0.000800\n",
            "Training Epoch: 193 [43648/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 193 [43776/50000]\tLoss: 0.0244\tLR: 0.000800\n",
            "Training Epoch: 193 [43904/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 193 [44032/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 193 [44160/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 193 [44288/50000]\tLoss: 0.0284\tLR: 0.000800\n",
            "Training Epoch: 193 [44416/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 193 [44544/50000]\tLoss: 0.0017\tLR: 0.000800\n",
            "Training Epoch: 193 [44672/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 193 [44800/50000]\tLoss: 0.0356\tLR: 0.000800\n",
            "Training Epoch: 193 [44928/50000]\tLoss: 0.0383\tLR: 0.000800\n",
            "Training Epoch: 193 [45056/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 193 [45184/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 193 [45312/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 193 [45440/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 193 [45568/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 193 [45696/50000]\tLoss: 0.0261\tLR: 0.000800\n",
            "Training Epoch: 193 [45824/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 193 [45952/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 193 [46080/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 193 [46208/50000]\tLoss: 0.0464\tLR: 0.000800\n",
            "Training Epoch: 193 [46336/50000]\tLoss: 0.1013\tLR: 0.000800\n",
            "Training Epoch: 193 [46464/50000]\tLoss: 0.0312\tLR: 0.000800\n",
            "Training Epoch: 193 [46592/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 193 [46720/50000]\tLoss: 0.0665\tLR: 0.000800\n",
            "Training Epoch: 193 [46848/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 193 [46976/50000]\tLoss: 0.0162\tLR: 0.000800\n",
            "Training Epoch: 193 [47104/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 193 [47232/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 193 [47360/50000]\tLoss: 0.0280\tLR: 0.000800\n",
            "Training Epoch: 193 [47488/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 193 [47616/50000]\tLoss: 0.0145\tLR: 0.000800\n",
            "Training Epoch: 193 [47744/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 193 [47872/50000]\tLoss: 0.0267\tLR: 0.000800\n",
            "Training Epoch: 193 [48000/50000]\tLoss: 0.0628\tLR: 0.000800\n",
            "Training Epoch: 193 [48128/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 193 [48256/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 193 [48384/50000]\tLoss: 0.0172\tLR: 0.000800\n",
            "Training Epoch: 193 [48512/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 193 [48640/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 193 [48768/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 193 [48896/50000]\tLoss: 0.0181\tLR: 0.000800\n",
            "Training Epoch: 193 [49024/50000]\tLoss: 0.0315\tLR: 0.000800\n",
            "Training Epoch: 193 [49152/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 193 [49280/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 193 [49408/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 193 [49536/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 193 [49664/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 193 [49792/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 193 [49920/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 193 [50000/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "epoch 193 training time consumed: 33.37s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 215027 GiB | 215027 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 213067 GiB | 213067 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1959 GiB |   1959 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 215027 GiB | 215027 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 213067 GiB | 213067 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1959 GiB |   1959 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 214363 GiB | 214363 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 212404 GiB | 212404 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   1958 GiB |   1958 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 190190 GiB | 190190 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 188230 GiB | 188230 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   1960 GiB |   1960 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   23554 K  |   23554 K  |\n",
            "|       from large pool |      38    |      61    |   10294 K  |   10294 K  |\n",
            "|       from small pool |     187    |     230    |   13260 K  |   13259 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   23554 K  |   23554 K  |\n",
            "|       from large pool |      38    |      61    |   10294 K  |   10294 K  |\n",
            "|       from small pool |     187    |     230    |   13260 K  |   13259 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |    9764 K  |    9764 K  |\n",
            "|       from large pool |      13    |      15    |    4679 K  |    4679 K  |\n",
            "|       from small pool |       7    |      14    |    5085 K  |    5085 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 193, Average loss: 0.0128, Accuracy: 0.7205, Time consumed:2.85s\n",
            "\n",
            "Training Epoch: 194 [128/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 194 [256/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 194 [384/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 194 [512/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 194 [640/50000]\tLoss: 0.0205\tLR: 0.000800\n",
            "Training Epoch: 194 [768/50000]\tLoss: 0.0142\tLR: 0.000800\n",
            "Training Epoch: 194 [896/50000]\tLoss: 0.0197\tLR: 0.000800\n",
            "Training Epoch: 194 [1024/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 194 [1152/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 194 [1280/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 194 [1408/50000]\tLoss: 0.0327\tLR: 0.000800\n",
            "Training Epoch: 194 [1536/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 194 [1664/50000]\tLoss: 0.0499\tLR: 0.000800\n",
            "Training Epoch: 194 [1792/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 194 [1920/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 194 [2048/50000]\tLoss: 0.0324\tLR: 0.000800\n",
            "Training Epoch: 194 [2176/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 194 [2304/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 194 [2432/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 194 [2560/50000]\tLoss: 0.0455\tLR: 0.000800\n",
            "Training Epoch: 194 [2688/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 194 [2816/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 194 [2944/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 194 [3072/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 194 [3200/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 194 [3328/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 194 [3456/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 194 [3584/50000]\tLoss: 0.0675\tLR: 0.000800\n",
            "Training Epoch: 194 [3712/50000]\tLoss: 0.0193\tLR: 0.000800\n",
            "Training Epoch: 194 [3840/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 194 [3968/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 194 [4096/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 194 [4224/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 194 [4352/50000]\tLoss: 0.0382\tLR: 0.000800\n",
            "Training Epoch: 194 [4480/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 194 [4608/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 194 [4736/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 194 [4864/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 194 [4992/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 194 [5120/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 194 [5248/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 194 [5376/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 194 [5504/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 194 [5632/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 194 [5760/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 194 [5888/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 194 [6016/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 194 [6144/50000]\tLoss: 0.0187\tLR: 0.000800\n",
            "Training Epoch: 194 [6272/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 194 [6400/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 194 [6528/50000]\tLoss: 0.0494\tLR: 0.000800\n",
            "Training Epoch: 194 [6656/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 194 [6784/50000]\tLoss: 0.0239\tLR: 0.000800\n",
            "Training Epoch: 194 [6912/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 194 [7040/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 194 [7168/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 194 [7296/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 194 [7424/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 194 [7552/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 194 [7680/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 194 [7808/50000]\tLoss: 0.0405\tLR: 0.000800\n",
            "Training Epoch: 194 [7936/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 194 [8064/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 194 [8192/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 194 [8320/50000]\tLoss: 0.0167\tLR: 0.000800\n",
            "Training Epoch: 194 [8448/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 194 [8576/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 194 [8704/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 194 [8832/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 194 [8960/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 194 [9088/50000]\tLoss: 0.0308\tLR: 0.000800\n",
            "Training Epoch: 194 [9216/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 194 [9344/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 194 [9472/50000]\tLoss: 0.0349\tLR: 0.000800\n",
            "Training Epoch: 194 [9600/50000]\tLoss: 0.1133\tLR: 0.000800\n",
            "Training Epoch: 194 [9728/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 194 [9856/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 194 [9984/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 194 [10112/50000]\tLoss: 0.0168\tLR: 0.000800\n",
            "Training Epoch: 194 [10240/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 194 [10368/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 194 [10496/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 194 [10624/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 194 [10752/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 194 [10880/50000]\tLoss: 0.0724\tLR: 0.000800\n",
            "Training Epoch: 194 [11008/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 194 [11136/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 194 [11264/50000]\tLoss: 0.0457\tLR: 0.000800\n",
            "Training Epoch: 194 [11392/50000]\tLoss: 0.0204\tLR: 0.000800\n",
            "Training Epoch: 194 [11520/50000]\tLoss: 0.0350\tLR: 0.000800\n",
            "Training Epoch: 194 [11648/50000]\tLoss: 0.0460\tLR: 0.000800\n",
            "Training Epoch: 194 [11776/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 194 [11904/50000]\tLoss: 0.0238\tLR: 0.000800\n",
            "Training Epoch: 194 [12032/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 194 [12160/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 194 [12288/50000]\tLoss: 0.0257\tLR: 0.000800\n",
            "Training Epoch: 194 [12416/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 194 [12544/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 194 [12672/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 194 [12800/50000]\tLoss: 0.0288\tLR: 0.000800\n",
            "Training Epoch: 194 [12928/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 194 [13056/50000]\tLoss: 0.0149\tLR: 0.000800\n",
            "Training Epoch: 194 [13184/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 194 [13312/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 194 [13440/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 194 [13568/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 194 [13696/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 194 [13824/50000]\tLoss: 0.0323\tLR: 0.000800\n",
            "Training Epoch: 194 [13952/50000]\tLoss: 0.0260\tLR: 0.000800\n",
            "Training Epoch: 194 [14080/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 194 [14208/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 194 [14336/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 194 [14464/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 194 [14592/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 194 [14720/50000]\tLoss: 0.0329\tLR: 0.000800\n",
            "Training Epoch: 194 [14848/50000]\tLoss: 0.0257\tLR: 0.000800\n",
            "Training Epoch: 194 [14976/50000]\tLoss: 0.0576\tLR: 0.000800\n",
            "Training Epoch: 194 [15104/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 194 [15232/50000]\tLoss: 0.0369\tLR: 0.000800\n",
            "Training Epoch: 194 [15360/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 194 [15488/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 194 [15616/50000]\tLoss: 0.0895\tLR: 0.000800\n",
            "Training Epoch: 194 [15744/50000]\tLoss: 0.0430\tLR: 0.000800\n",
            "Training Epoch: 194 [15872/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 194 [16000/50000]\tLoss: 0.0187\tLR: 0.000800\n",
            "Training Epoch: 194 [16128/50000]\tLoss: 0.0544\tLR: 0.000800\n",
            "Training Epoch: 194 [16256/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 194 [16384/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 194 [16512/50000]\tLoss: 0.0333\tLR: 0.000800\n",
            "Training Epoch: 194 [16640/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 194 [16768/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 194 [16896/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 194 [17024/50000]\tLoss: 0.0168\tLR: 0.000800\n",
            "Training Epoch: 194 [17152/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 194 [17280/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 194 [17408/50000]\tLoss: 0.0523\tLR: 0.000800\n",
            "Training Epoch: 194 [17536/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 194 [17664/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 194 [17792/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 194 [17920/50000]\tLoss: 0.0311\tLR: 0.000800\n",
            "Training Epoch: 194 [18048/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 194 [18176/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 194 [18304/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 194 [18432/50000]\tLoss: 0.0282\tLR: 0.000800\n",
            "Training Epoch: 194 [18560/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 194 [18688/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 194 [18816/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 194 [18944/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 194 [19072/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 194 [19200/50000]\tLoss: 0.0162\tLR: 0.000800\n",
            "Training Epoch: 194 [19328/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 194 [19456/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 194 [19584/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 194 [19712/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 194 [19840/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 194 [19968/50000]\tLoss: 0.0309\tLR: 0.000800\n",
            "Training Epoch: 194 [20096/50000]\tLoss: 0.0216\tLR: 0.000800\n",
            "Training Epoch: 194 [20224/50000]\tLoss: 0.0218\tLR: 0.000800\n",
            "Training Epoch: 194 [20352/50000]\tLoss: 0.0170\tLR: 0.000800\n",
            "Training Epoch: 194 [20480/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 194 [20608/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 194 [20736/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 194 [20864/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 194 [20992/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 194 [21120/50000]\tLoss: 0.0638\tLR: 0.000800\n",
            "Training Epoch: 194 [21248/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 194 [21376/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 194 [21504/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 194 [21632/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 194 [21760/50000]\tLoss: 0.0238\tLR: 0.000800\n",
            "Training Epoch: 194 [21888/50000]\tLoss: 0.0468\tLR: 0.000800\n",
            "Training Epoch: 194 [22016/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 194 [22144/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 194 [22272/50000]\tLoss: 0.0455\tLR: 0.000800\n",
            "Training Epoch: 194 [22400/50000]\tLoss: 0.0316\tLR: 0.000800\n",
            "Training Epoch: 194 [22528/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 194 [22656/50000]\tLoss: 0.0403\tLR: 0.000800\n",
            "Training Epoch: 194 [22784/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 194 [22912/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 194 [23040/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 194 [23168/50000]\tLoss: 0.0282\tLR: 0.000800\n",
            "Training Epoch: 194 [23296/50000]\tLoss: 0.0581\tLR: 0.000800\n",
            "Training Epoch: 194 [23424/50000]\tLoss: 0.0265\tLR: 0.000800\n",
            "Training Epoch: 194 [23552/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 194 [23680/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 194 [23808/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 194 [23936/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 194 [24064/50000]\tLoss: 0.0174\tLR: 0.000800\n",
            "Training Epoch: 194 [24192/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 194 [24320/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 194 [24448/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 194 [24576/50000]\tLoss: 0.0519\tLR: 0.000800\n",
            "Training Epoch: 194 [24704/50000]\tLoss: 0.0483\tLR: 0.000800\n",
            "Training Epoch: 194 [24832/50000]\tLoss: 0.0583\tLR: 0.000800\n",
            "Training Epoch: 194 [24960/50000]\tLoss: 0.0219\tLR: 0.000800\n",
            "Training Epoch: 194 [25088/50000]\tLoss: 0.0214\tLR: 0.000800\n",
            "Training Epoch: 194 [25216/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 194 [25344/50000]\tLoss: 0.0522\tLR: 0.000800\n",
            "Training Epoch: 194 [25472/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 194 [25600/50000]\tLoss: 0.0332\tLR: 0.000800\n",
            "Training Epoch: 194 [25728/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 194 [25856/50000]\tLoss: 0.0226\tLR: 0.000800\n",
            "Training Epoch: 194 [25984/50000]\tLoss: 0.0019\tLR: 0.000800\n",
            "Training Epoch: 194 [26112/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 194 [26240/50000]\tLoss: 0.0497\tLR: 0.000800\n",
            "Training Epoch: 194 [26368/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 194 [26496/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 194 [26624/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 194 [26752/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 194 [26880/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 194 [27008/50000]\tLoss: 0.0216\tLR: 0.000800\n",
            "Training Epoch: 194 [27136/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 194 [27264/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 194 [27392/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 194 [27520/50000]\tLoss: 0.0466\tLR: 0.000800\n",
            "Training Epoch: 194 [27648/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 194 [27776/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 194 [27904/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 194 [28032/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 194 [28160/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 194 [28288/50000]\tLoss: 0.0393\tLR: 0.000800\n",
            "Training Epoch: 194 [28416/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 194 [28544/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 194 [28672/50000]\tLoss: 0.0199\tLR: 0.000800\n",
            "Training Epoch: 194 [28800/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 194 [28928/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 194 [29056/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 194 [29184/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 194 [29312/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 194 [29440/50000]\tLoss: 0.0281\tLR: 0.000800\n",
            "Training Epoch: 194 [29568/50000]\tLoss: 0.0973\tLR: 0.000800\n",
            "Training Epoch: 194 [29696/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 194 [29824/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 194 [29952/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 194 [30080/50000]\tLoss: 0.0267\tLR: 0.000800\n",
            "Training Epoch: 194 [30208/50000]\tLoss: 0.0198\tLR: 0.000800\n",
            "Training Epoch: 194 [30336/50000]\tLoss: 0.0514\tLR: 0.000800\n",
            "Training Epoch: 194 [30464/50000]\tLoss: 0.0193\tLR: 0.000800\n",
            "Training Epoch: 194 [30592/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 194 [30720/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 194 [30848/50000]\tLoss: 0.0302\tLR: 0.000800\n",
            "Training Epoch: 194 [30976/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 194 [31104/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 194 [31232/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 194 [31360/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 194 [31488/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 194 [31616/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 194 [31744/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 194 [31872/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 194 [32000/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 194 [32128/50000]\tLoss: 0.0714\tLR: 0.000800\n",
            "Training Epoch: 194 [32256/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 194 [32384/50000]\tLoss: 0.0622\tLR: 0.000800\n",
            "Training Epoch: 194 [32512/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 194 [32640/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 194 [32768/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 194 [32896/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 194 [33024/50000]\tLoss: 0.0191\tLR: 0.000800\n",
            "Training Epoch: 194 [33152/50000]\tLoss: 0.0383\tLR: 0.000800\n",
            "Training Epoch: 194 [33280/50000]\tLoss: 0.0328\tLR: 0.000800\n",
            "Training Epoch: 194 [33408/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 194 [33536/50000]\tLoss: 0.0255\tLR: 0.000800\n",
            "Training Epoch: 194 [33664/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 194 [33792/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 194 [33920/50000]\tLoss: 0.0157\tLR: 0.000800\n",
            "Training Epoch: 194 [34048/50000]\tLoss: 0.0804\tLR: 0.000800\n",
            "Training Epoch: 194 [34176/50000]\tLoss: 0.0374\tLR: 0.000800\n",
            "Training Epoch: 194 [34304/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 194 [34432/50000]\tLoss: 0.0532\tLR: 0.000800\n",
            "Training Epoch: 194 [34560/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 194 [34688/50000]\tLoss: 0.0315\tLR: 0.000800\n",
            "Training Epoch: 194 [34816/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 194 [34944/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 194 [35072/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 194 [35200/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 194 [35328/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 194 [35456/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 194 [35584/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 194 [35712/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 194 [35840/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 194 [35968/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 194 [36096/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 194 [36224/50000]\tLoss: 0.0208\tLR: 0.000800\n",
            "Training Epoch: 194 [36352/50000]\tLoss: 0.0187\tLR: 0.000800\n",
            "Training Epoch: 194 [36480/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 194 [36608/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 194 [36736/50000]\tLoss: 0.0312\tLR: 0.000800\n",
            "Training Epoch: 194 [36864/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 194 [36992/50000]\tLoss: 0.0142\tLR: 0.000800\n",
            "Training Epoch: 194 [37120/50000]\tLoss: 0.0308\tLR: 0.000800\n",
            "Training Epoch: 194 [37248/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 194 [37376/50000]\tLoss: 0.0296\tLR: 0.000800\n",
            "Training Epoch: 194 [37504/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 194 [37632/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 194 [37760/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 194 [37888/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 194 [38016/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 194 [38144/50000]\tLoss: 0.0305\tLR: 0.000800\n",
            "Training Epoch: 194 [38272/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 194 [38400/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 194 [38528/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 194 [38656/50000]\tLoss: 0.0256\tLR: 0.000800\n",
            "Training Epoch: 194 [38784/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 194 [38912/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 194 [39040/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 194 [39168/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 194 [39296/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 194 [39424/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 194 [39552/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 194 [39680/50000]\tLoss: 0.0462\tLR: 0.000800\n",
            "Training Epoch: 194 [39808/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 194 [39936/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 194 [40064/50000]\tLoss: 0.0265\tLR: 0.000800\n",
            "Training Epoch: 194 [40192/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 194 [40320/50000]\tLoss: 0.0215\tLR: 0.000800\n",
            "Training Epoch: 194 [40448/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 194 [40576/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 194 [40704/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 194 [40832/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 194 [40960/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 194 [41088/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 194 [41216/50000]\tLoss: 0.0661\tLR: 0.000800\n",
            "Training Epoch: 194 [41344/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 194 [41472/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 194 [41600/50000]\tLoss: 0.0327\tLR: 0.000800\n",
            "Training Epoch: 194 [41728/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 194 [41856/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 194 [41984/50000]\tLoss: 0.0535\tLR: 0.000800\n",
            "Training Epoch: 194 [42112/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 194 [42240/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 194 [42368/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 194 [42496/50000]\tLoss: 0.0299\tLR: 0.000800\n",
            "Training Epoch: 194 [42624/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 194 [42752/50000]\tLoss: 0.0373\tLR: 0.000800\n",
            "Training Epoch: 194 [42880/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 194 [43008/50000]\tLoss: 0.0357\tLR: 0.000800\n",
            "Training Epoch: 194 [43136/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 194 [43264/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 194 [43392/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 194 [43520/50000]\tLoss: 0.0238\tLR: 0.000800\n",
            "Training Epoch: 194 [43648/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 194 [43776/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 194 [43904/50000]\tLoss: 0.0392\tLR: 0.000800\n",
            "Training Epoch: 194 [44032/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 194 [44160/50000]\tLoss: 0.0266\tLR: 0.000800\n",
            "Training Epoch: 194 [44288/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 194 [44416/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 194 [44544/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 194 [44672/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 194 [44800/50000]\tLoss: 0.0216\tLR: 0.000800\n",
            "Training Epoch: 194 [44928/50000]\tLoss: 0.0332\tLR: 0.000800\n",
            "Training Epoch: 194 [45056/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 194 [45184/50000]\tLoss: 0.0023\tLR: 0.000800\n",
            "Training Epoch: 194 [45312/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 194 [45440/50000]\tLoss: 0.0353\tLR: 0.000800\n",
            "Training Epoch: 194 [45568/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 194 [45696/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 194 [45824/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 194 [45952/50000]\tLoss: 0.1127\tLR: 0.000800\n",
            "Training Epoch: 194 [46080/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 194 [46208/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 194 [46336/50000]\tLoss: 0.0474\tLR: 0.000800\n",
            "Training Epoch: 194 [46464/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 194 [46592/50000]\tLoss: 0.0025\tLR: 0.000800\n",
            "Training Epoch: 194 [46720/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 194 [46848/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 194 [46976/50000]\tLoss: 0.0260\tLR: 0.000800\n",
            "Training Epoch: 194 [47104/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 194 [47232/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 194 [47360/50000]\tLoss: 0.0949\tLR: 0.000800\n",
            "Training Epoch: 194 [47488/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 194 [47616/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 194 [47744/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 194 [47872/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 194 [48000/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 194 [48128/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 194 [48256/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 194 [48384/50000]\tLoss: 0.0254\tLR: 0.000800\n",
            "Training Epoch: 194 [48512/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 194 [48640/50000]\tLoss: 0.0363\tLR: 0.000800\n",
            "Training Epoch: 194 [48768/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 194 [48896/50000]\tLoss: 0.0594\tLR: 0.000800\n",
            "Training Epoch: 194 [49024/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 194 [49152/50000]\tLoss: 0.0456\tLR: 0.000800\n",
            "Training Epoch: 194 [49280/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 194 [49408/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 194 [49536/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 194 [49664/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 194 [49792/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 194 [49920/50000]\tLoss: 0.0167\tLR: 0.000800\n",
            "Training Epoch: 194 [50000/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "epoch 194 training time consumed: 35.38s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 216141 GiB | 216141 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 214171 GiB | 214171 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1969 GiB |   1969 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 216141 GiB | 216141 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 214171 GiB | 214171 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1969 GiB |   1969 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 215474 GiB | 215473 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 213505 GiB | 213505 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   1968 GiB |   1968 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 191176 GiB | 191175 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 189205 GiB | 189205 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   1970 GiB |   1970 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   23676 K  |   23676 K  |\n",
            "|       from large pool |      38    |      61    |   10347 K  |   10347 K  |\n",
            "|       from small pool |     187    |     230    |   13328 K  |   13328 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   23676 K  |   23676 K  |\n",
            "|       from large pool |      38    |      61    |   10347 K  |   10347 K  |\n",
            "|       from small pool |     187    |     230    |   13328 K  |   13328 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |    9816 K  |    9816 K  |\n",
            "|       from large pool |      13    |      15    |    4703 K  |    4703 K  |\n",
            "|       from small pool |       7    |      14    |    5112 K  |    5112 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 194, Average loss: 0.0129, Accuracy: 0.7187, Time consumed:3.07s\n",
            "\n",
            "Training Epoch: 195 [128/50000]\tLoss: 0.0330\tLR: 0.000800\n",
            "Training Epoch: 195 [256/50000]\tLoss: 0.0725\tLR: 0.000800\n",
            "Training Epoch: 195 [384/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 195 [512/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 195 [640/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 195 [768/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 195 [896/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 195 [1024/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 195 [1152/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 195 [1280/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 195 [1408/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 195 [1536/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 195 [1664/50000]\tLoss: 0.0735\tLR: 0.000800\n",
            "Training Epoch: 195 [1792/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 195 [1920/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 195 [2048/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 195 [2176/50000]\tLoss: 0.0096\tLR: 0.000800\n",
            "Training Epoch: 195 [2304/50000]\tLoss: 0.0224\tLR: 0.000800\n",
            "Training Epoch: 195 [2432/50000]\tLoss: 0.0428\tLR: 0.000800\n",
            "Training Epoch: 195 [2560/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 195 [2688/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 195 [2816/50000]\tLoss: 0.0170\tLR: 0.000800\n",
            "Training Epoch: 195 [2944/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 195 [3072/50000]\tLoss: 0.0197\tLR: 0.000800\n",
            "Training Epoch: 195 [3200/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 195 [3328/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 195 [3456/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 195 [3584/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 195 [3712/50000]\tLoss: 0.0333\tLR: 0.000800\n",
            "Training Epoch: 195 [3840/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 195 [3968/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 195 [4096/50000]\tLoss: 0.0818\tLR: 0.000800\n",
            "Training Epoch: 195 [4224/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 195 [4352/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 195 [4480/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 195 [4608/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 195 [4736/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 195 [4864/50000]\tLoss: 0.0257\tLR: 0.000800\n",
            "Training Epoch: 195 [4992/50000]\tLoss: 0.0409\tLR: 0.000800\n",
            "Training Epoch: 195 [5120/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 195 [5248/50000]\tLoss: 0.0320\tLR: 0.000800\n",
            "Training Epoch: 195 [5376/50000]\tLoss: 0.0364\tLR: 0.000800\n",
            "Training Epoch: 195 [5504/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 195 [5632/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 195 [5760/50000]\tLoss: 0.0233\tLR: 0.000800\n",
            "Training Epoch: 195 [5888/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 195 [6016/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 195 [6144/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 195 [6272/50000]\tLoss: 0.0223\tLR: 0.000800\n",
            "Training Epoch: 195 [6400/50000]\tLoss: 0.0257\tLR: 0.000800\n",
            "Training Epoch: 195 [6528/50000]\tLoss: 0.0145\tLR: 0.000800\n",
            "Training Epoch: 195 [6656/50000]\tLoss: 0.0257\tLR: 0.000800\n",
            "Training Epoch: 195 [6784/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 195 [6912/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 195 [7040/50000]\tLoss: 0.0410\tLR: 0.000800\n",
            "Training Epoch: 195 [7168/50000]\tLoss: 0.0459\tLR: 0.000800\n",
            "Training Epoch: 195 [7296/50000]\tLoss: 0.0370\tLR: 0.000800\n",
            "Training Epoch: 195 [7424/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 195 [7552/50000]\tLoss: 0.0189\tLR: 0.000800\n",
            "Training Epoch: 195 [7680/50000]\tLoss: 0.0449\tLR: 0.000800\n",
            "Training Epoch: 195 [7808/50000]\tLoss: 0.0203\tLR: 0.000800\n",
            "Training Epoch: 195 [7936/50000]\tLoss: 0.0200\tLR: 0.000800\n",
            "Training Epoch: 195 [8064/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 195 [8192/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 195 [8320/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 195 [8448/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 195 [8576/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 195 [8704/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 195 [8832/50000]\tLoss: 0.0204\tLR: 0.000800\n",
            "Training Epoch: 195 [8960/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 195 [9088/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 195 [9216/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 195 [9344/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 195 [9472/50000]\tLoss: 0.0338\tLR: 0.000800\n",
            "Training Epoch: 195 [9600/50000]\tLoss: 0.0311\tLR: 0.000800\n",
            "Training Epoch: 195 [9728/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 195 [9856/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 195 [9984/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 195 [10112/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 195 [10240/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 195 [10368/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 195 [10496/50000]\tLoss: 0.0022\tLR: 0.000800\n",
            "Training Epoch: 195 [10624/50000]\tLoss: 0.0399\tLR: 0.000800\n",
            "Training Epoch: 195 [10752/50000]\tLoss: 0.0454\tLR: 0.000800\n",
            "Training Epoch: 195 [10880/50000]\tLoss: 0.0316\tLR: 0.000800\n",
            "Training Epoch: 195 [11008/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 195 [11136/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 195 [11264/50000]\tLoss: 0.0365\tLR: 0.000800\n",
            "Training Epoch: 195 [11392/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 195 [11520/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 195 [11648/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 195 [11776/50000]\tLoss: 0.0221\tLR: 0.000800\n",
            "Training Epoch: 195 [11904/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 195 [12032/50000]\tLoss: 0.0275\tLR: 0.000800\n",
            "Training Epoch: 195 [12160/50000]\tLoss: 0.0392\tLR: 0.000800\n",
            "Training Epoch: 195 [12288/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 195 [12416/50000]\tLoss: 0.0362\tLR: 0.000800\n",
            "Training Epoch: 195 [12544/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 195 [12672/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 195 [12800/50000]\tLoss: 0.0174\tLR: 0.000800\n",
            "Training Epoch: 195 [12928/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 195 [13056/50000]\tLoss: 0.0292\tLR: 0.000800\n",
            "Training Epoch: 195 [13184/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 195 [13312/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 195 [13440/50000]\tLoss: 0.0423\tLR: 0.000800\n",
            "Training Epoch: 195 [13568/50000]\tLoss: 0.0370\tLR: 0.000800\n",
            "Training Epoch: 195 [13696/50000]\tLoss: 0.0335\tLR: 0.000800\n",
            "Training Epoch: 195 [13824/50000]\tLoss: 0.0209\tLR: 0.000800\n",
            "Training Epoch: 195 [13952/50000]\tLoss: 0.0435\tLR: 0.000800\n",
            "Training Epoch: 195 [14080/50000]\tLoss: 0.0471\tLR: 0.000800\n",
            "Training Epoch: 195 [14208/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 195 [14336/50000]\tLoss: 0.0441\tLR: 0.000800\n",
            "Training Epoch: 195 [14464/50000]\tLoss: 0.0365\tLR: 0.000800\n",
            "Training Epoch: 195 [14592/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 195 [14720/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 195 [14848/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 195 [14976/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 195 [15104/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 195 [15232/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 195 [15360/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 195 [15488/50000]\tLoss: 0.0299\tLR: 0.000800\n",
            "Training Epoch: 195 [15616/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 195 [15744/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 195 [15872/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 195 [16000/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 195 [16128/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 195 [16256/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 195 [16384/50000]\tLoss: 0.0355\tLR: 0.000800\n",
            "Training Epoch: 195 [16512/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 195 [16640/50000]\tLoss: 0.0545\tLR: 0.000800\n",
            "Training Epoch: 195 [16768/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 195 [16896/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 195 [17024/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 195 [17152/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 195 [17280/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 195 [17408/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 195 [17536/50000]\tLoss: 0.0142\tLR: 0.000800\n",
            "Training Epoch: 195 [17664/50000]\tLoss: 0.0729\tLR: 0.000800\n",
            "Training Epoch: 195 [17792/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 195 [17920/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 195 [18048/50000]\tLoss: 0.0218\tLR: 0.000800\n",
            "Training Epoch: 195 [18176/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 195 [18304/50000]\tLoss: 0.0225\tLR: 0.000800\n",
            "Training Epoch: 195 [18432/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 195 [18560/50000]\tLoss: 0.0170\tLR: 0.000800\n",
            "Training Epoch: 195 [18688/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 195 [18816/50000]\tLoss: 0.0506\tLR: 0.000800\n",
            "Training Epoch: 195 [18944/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 195 [19072/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 195 [19200/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 195 [19328/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 195 [19456/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 195 [19584/50000]\tLoss: 0.0096\tLR: 0.000800\n",
            "Training Epoch: 195 [19712/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 195 [19840/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 195 [19968/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 195 [20096/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 195 [20224/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 195 [20352/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 195 [20480/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 195 [20608/50000]\tLoss: 0.0207\tLR: 0.000800\n",
            "Training Epoch: 195 [20736/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 195 [20864/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 195 [20992/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 195 [21120/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 195 [21248/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 195 [21376/50000]\tLoss: 0.0404\tLR: 0.000800\n",
            "Training Epoch: 195 [21504/50000]\tLoss: 0.0357\tLR: 0.000800\n",
            "Training Epoch: 195 [21632/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 195 [21760/50000]\tLoss: 0.0253\tLR: 0.000800\n",
            "Training Epoch: 195 [21888/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 195 [22016/50000]\tLoss: 0.0194\tLR: 0.000800\n",
            "Training Epoch: 195 [22144/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 195 [22272/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 195 [22400/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 195 [22528/50000]\tLoss: 0.0270\tLR: 0.000800\n",
            "Training Epoch: 195 [22656/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 195 [22784/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 195 [22912/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 195 [23040/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 195 [23168/50000]\tLoss: 0.0566\tLR: 0.000800\n",
            "Training Epoch: 195 [23296/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 195 [23424/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 195 [23552/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 195 [23680/50000]\tLoss: 0.0254\tLR: 0.000800\n",
            "Training Epoch: 195 [23808/50000]\tLoss: 0.0706\tLR: 0.000800\n",
            "Training Epoch: 195 [23936/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 195 [24064/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 195 [24192/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 195 [24320/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 195 [24448/50000]\tLoss: 0.0204\tLR: 0.000800\n",
            "Training Epoch: 195 [24576/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 195 [24704/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 195 [24832/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 195 [24960/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 195 [25088/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 195 [25216/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 195 [25344/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 195 [25472/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 195 [25600/50000]\tLoss: 0.0492\tLR: 0.000800\n",
            "Training Epoch: 195 [25728/50000]\tLoss: 0.0448\tLR: 0.000800\n",
            "Training Epoch: 195 [25856/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 195 [25984/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 195 [26112/50000]\tLoss: 0.0468\tLR: 0.000800\n",
            "Training Epoch: 195 [26240/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 195 [26368/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 195 [26496/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 195 [26624/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 195 [26752/50000]\tLoss: 0.0760\tLR: 0.000800\n",
            "Training Epoch: 195 [26880/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 195 [27008/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 195 [27136/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 195 [27264/50000]\tLoss: 0.0189\tLR: 0.000800\n",
            "Training Epoch: 195 [27392/50000]\tLoss: 0.0399\tLR: 0.000800\n",
            "Training Epoch: 195 [27520/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 195 [27648/50000]\tLoss: 0.0290\tLR: 0.000800\n",
            "Training Epoch: 195 [27776/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 195 [27904/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 195 [28032/50000]\tLoss: 0.0348\tLR: 0.000800\n",
            "Training Epoch: 195 [28160/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 195 [28288/50000]\tLoss: 0.0136\tLR: 0.000800\n",
            "Training Epoch: 195 [28416/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 195 [28544/50000]\tLoss: 0.0530\tLR: 0.000800\n",
            "Training Epoch: 195 [28672/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 195 [28800/50000]\tLoss: 0.0218\tLR: 0.000800\n",
            "Training Epoch: 195 [28928/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 195 [29056/50000]\tLoss: 0.0627\tLR: 0.000800\n",
            "Training Epoch: 195 [29184/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 195 [29312/50000]\tLoss: 0.0432\tLR: 0.000800\n",
            "Training Epoch: 195 [29440/50000]\tLoss: 0.0313\tLR: 0.000800\n",
            "Training Epoch: 195 [29568/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 195 [29696/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 195 [29824/50000]\tLoss: 0.0441\tLR: 0.000800\n",
            "Training Epoch: 195 [29952/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 195 [30080/50000]\tLoss: 0.0274\tLR: 0.000800\n",
            "Training Epoch: 195 [30208/50000]\tLoss: 0.0191\tLR: 0.000800\n",
            "Training Epoch: 195 [30336/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 195 [30464/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 195 [30592/50000]\tLoss: 0.0778\tLR: 0.000800\n",
            "Training Epoch: 195 [30720/50000]\tLoss: 0.0301\tLR: 0.000800\n",
            "Training Epoch: 195 [30848/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 195 [30976/50000]\tLoss: 0.0200\tLR: 0.000800\n",
            "Training Epoch: 195 [31104/50000]\tLoss: 0.0263\tLR: 0.000800\n",
            "Training Epoch: 195 [31232/50000]\tLoss: 0.0468\tLR: 0.000800\n",
            "Training Epoch: 195 [31360/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 195 [31488/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 195 [31616/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 195 [31744/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 195 [31872/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 195 [32000/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 195 [32128/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 195 [32256/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 195 [32384/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 195 [32512/50000]\tLoss: 0.0515\tLR: 0.000800\n",
            "Training Epoch: 195 [32640/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 195 [32768/50000]\tLoss: 0.0226\tLR: 0.000800\n",
            "Training Epoch: 195 [32896/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 195 [33024/50000]\tLoss: 0.0217\tLR: 0.000800\n",
            "Training Epoch: 195 [33152/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 195 [33280/50000]\tLoss: 0.0025\tLR: 0.000800\n",
            "Training Epoch: 195 [33408/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 195 [33536/50000]\tLoss: 0.0555\tLR: 0.000800\n",
            "Training Epoch: 195 [33664/50000]\tLoss: 0.0266\tLR: 0.000800\n",
            "Training Epoch: 195 [33792/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 195 [33920/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 195 [34048/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 195 [34176/50000]\tLoss: 0.0256\tLR: 0.000800\n",
            "Training Epoch: 195 [34304/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 195 [34432/50000]\tLoss: 0.0256\tLR: 0.000800\n",
            "Training Epoch: 195 [34560/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 195 [34688/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 195 [34816/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 195 [34944/50000]\tLoss: 0.0517\tLR: 0.000800\n",
            "Training Epoch: 195 [35072/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 195 [35200/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 195 [35328/50000]\tLoss: 0.0231\tLR: 0.000800\n",
            "Training Epoch: 195 [35456/50000]\tLoss: 0.0355\tLR: 0.000800\n",
            "Training Epoch: 195 [35584/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 195 [35712/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 195 [35840/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 195 [35968/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 195 [36096/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 195 [36224/50000]\tLoss: 0.0309\tLR: 0.000800\n",
            "Training Epoch: 195 [36352/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 195 [36480/50000]\tLoss: 0.0205\tLR: 0.000800\n",
            "Training Epoch: 195 [36608/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 195 [36736/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 195 [36864/50000]\tLoss: 0.0382\tLR: 0.000800\n",
            "Training Epoch: 195 [36992/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 195 [37120/50000]\tLoss: 0.0188\tLR: 0.000800\n",
            "Training Epoch: 195 [37248/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 195 [37376/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 195 [37504/50000]\tLoss: 0.0608\tLR: 0.000800\n",
            "Training Epoch: 195 [37632/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 195 [37760/50000]\tLoss: 0.0242\tLR: 0.000800\n",
            "Training Epoch: 195 [37888/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 195 [38016/50000]\tLoss: 0.0157\tLR: 0.000800\n",
            "Training Epoch: 195 [38144/50000]\tLoss: 0.0193\tLR: 0.000800\n",
            "Training Epoch: 195 [38272/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 195 [38400/50000]\tLoss: 0.0399\tLR: 0.000800\n",
            "Training Epoch: 195 [38528/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 195 [38656/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 195 [38784/50000]\tLoss: 0.0409\tLR: 0.000800\n",
            "Training Epoch: 195 [38912/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 195 [39040/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 195 [39168/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 195 [39296/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 195 [39424/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 195 [39552/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 195 [39680/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 195 [39808/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 195 [39936/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 195 [40064/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 195 [40192/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 195 [40320/50000]\tLoss: 0.0264\tLR: 0.000800\n",
            "Training Epoch: 195 [40448/50000]\tLoss: 0.0287\tLR: 0.000800\n",
            "Training Epoch: 195 [40576/50000]\tLoss: 0.0189\tLR: 0.000800\n",
            "Training Epoch: 195 [40704/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 195 [40832/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 195 [40960/50000]\tLoss: 0.0409\tLR: 0.000800\n",
            "Training Epoch: 195 [41088/50000]\tLoss: 0.0196\tLR: 0.000800\n",
            "Training Epoch: 195 [41216/50000]\tLoss: 0.0196\tLR: 0.000800\n",
            "Training Epoch: 195 [41344/50000]\tLoss: 0.0142\tLR: 0.000800\n",
            "Training Epoch: 195 [41472/50000]\tLoss: 0.0355\tLR: 0.000800\n",
            "Training Epoch: 195 [41600/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 195 [41728/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 195 [41856/50000]\tLoss: 0.0330\tLR: 0.000800\n",
            "Training Epoch: 195 [41984/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 195 [42112/50000]\tLoss: 0.0624\tLR: 0.000800\n",
            "Training Epoch: 195 [42240/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 195 [42368/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 195 [42496/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 195 [42624/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 195 [42752/50000]\tLoss: 0.0256\tLR: 0.000800\n",
            "Training Epoch: 195 [42880/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 195 [43008/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 195 [43136/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 195 [43264/50000]\tLoss: 0.0688\tLR: 0.000800\n",
            "Training Epoch: 195 [43392/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 195 [43520/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 195 [43648/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 195 [43776/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 195 [43904/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 195 [44032/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 195 [44160/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 195 [44288/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 195 [44416/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 195 [44544/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 195 [44672/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 195 [44800/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 195 [44928/50000]\tLoss: 0.0221\tLR: 0.000800\n",
            "Training Epoch: 195 [45056/50000]\tLoss: 0.0205\tLR: 0.000800\n",
            "Training Epoch: 195 [45184/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 195 [45312/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 195 [45440/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 195 [45568/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 195 [45696/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 195 [45824/50000]\tLoss: 0.0340\tLR: 0.000800\n",
            "Training Epoch: 195 [45952/50000]\tLoss: 0.0406\tLR: 0.000800\n",
            "Training Epoch: 195 [46080/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 195 [46208/50000]\tLoss: 0.0588\tLR: 0.000800\n",
            "Training Epoch: 195 [46336/50000]\tLoss: 0.0260\tLR: 0.000800\n",
            "Training Epoch: 195 [46464/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 195 [46592/50000]\tLoss: 0.0550\tLR: 0.000800\n",
            "Training Epoch: 195 [46720/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 195 [46848/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 195 [46976/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 195 [47104/50000]\tLoss: 0.0394\tLR: 0.000800\n",
            "Training Epoch: 195 [47232/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 195 [47360/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 195 [47488/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 195 [47616/50000]\tLoss: 0.0527\tLR: 0.000800\n",
            "Training Epoch: 195 [47744/50000]\tLoss: 0.0259\tLR: 0.000800\n",
            "Training Epoch: 195 [47872/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 195 [48000/50000]\tLoss: 0.0024\tLR: 0.000800\n",
            "Training Epoch: 195 [48128/50000]\tLoss: 0.0366\tLR: 0.000800\n",
            "Training Epoch: 195 [48256/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 195 [48384/50000]\tLoss: 0.0502\tLR: 0.000800\n",
            "Training Epoch: 195 [48512/50000]\tLoss: 0.0193\tLR: 0.000800\n",
            "Training Epoch: 195 [48640/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 195 [48768/50000]\tLoss: 0.0383\tLR: 0.000800\n",
            "Training Epoch: 195 [48896/50000]\tLoss: 0.0278\tLR: 0.000800\n",
            "Training Epoch: 195 [49024/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 195 [49152/50000]\tLoss: 0.0365\tLR: 0.000800\n",
            "Training Epoch: 195 [49280/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 195 [49408/50000]\tLoss: 0.0557\tLR: 0.000800\n",
            "Training Epoch: 195 [49536/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 195 [49664/50000]\tLoss: 0.0422\tLR: 0.000800\n",
            "Training Epoch: 195 [49792/50000]\tLoss: 0.0482\tLR: 0.000800\n",
            "Training Epoch: 195 [49920/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 195 [50000/50000]\tLoss: 0.0199\tLR: 0.000800\n",
            "epoch 195 training time consumed: 33.51s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 217255 GiB | 217255 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 215275 GiB | 215275 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1980 GiB |   1980 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 217255 GiB | 217255 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 215275 GiB | 215275 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1980 GiB |   1980 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 216585 GiB | 216584 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 214605 GiB | 214605 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   1979 GiB |   1979 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 192161 GiB | 192161 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 190181 GiB | 190180 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   1980 GiB |   1980 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   23798 K  |   23798 K  |\n",
            "|       from large pool |      38    |      61    |   10400 K  |   10400 K  |\n",
            "|       from small pool |     187    |     230    |   13397 K  |   13397 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   23798 K  |   23798 K  |\n",
            "|       from large pool |      38    |      61    |   10400 K  |   10400 K  |\n",
            "|       from small pool |     187    |     230    |   13397 K  |   13397 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |    9867 K  |    9867 K  |\n",
            "|       from large pool |      13    |      15    |    4727 K  |    4727 K  |\n",
            "|       from small pool |       7    |      14    |    5139 K  |    5139 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 195, Average loss: 0.0130, Accuracy: 0.7171, Time consumed:3.29s\n",
            "\n",
            "Training Epoch: 196 [128/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 196 [256/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 196 [384/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 196 [512/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 196 [640/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 196 [768/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 196 [896/50000]\tLoss: 0.0366\tLR: 0.000800\n",
            "Training Epoch: 196 [1024/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 196 [1152/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 196 [1280/50000]\tLoss: 0.0302\tLR: 0.000800\n",
            "Training Epoch: 196 [1408/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 196 [1536/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 196 [1664/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 196 [1792/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 196 [1920/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 196 [2048/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 196 [2176/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 196 [2304/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 196 [2432/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 196 [2560/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 196 [2688/50000]\tLoss: 0.0306\tLR: 0.000800\n",
            "Training Epoch: 196 [2816/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 196 [2944/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 196 [3072/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 196 [3200/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 196 [3328/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 196 [3456/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 196 [3584/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 196 [3712/50000]\tLoss: 0.0591\tLR: 0.000800\n",
            "Training Epoch: 196 [3840/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 196 [3968/50000]\tLoss: 0.0509\tLR: 0.000800\n",
            "Training Epoch: 196 [4096/50000]\tLoss: 0.0282\tLR: 0.000800\n",
            "Training Epoch: 196 [4224/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 196 [4352/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 196 [4480/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 196 [4608/50000]\tLoss: 0.0355\tLR: 0.000800\n",
            "Training Epoch: 196 [4736/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 196 [4864/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 196 [4992/50000]\tLoss: 0.0357\tLR: 0.000800\n",
            "Training Epoch: 196 [5120/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 196 [5248/50000]\tLoss: 0.0373\tLR: 0.000800\n",
            "Training Epoch: 196 [5376/50000]\tLoss: 0.0199\tLR: 0.000800\n",
            "Training Epoch: 196 [5504/50000]\tLoss: 0.0335\tLR: 0.000800\n",
            "Training Epoch: 196 [5632/50000]\tLoss: 0.0309\tLR: 0.000800\n",
            "Training Epoch: 196 [5760/50000]\tLoss: 0.0527\tLR: 0.000800\n",
            "Training Epoch: 196 [5888/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 196 [6016/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 196 [6144/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 196 [6272/50000]\tLoss: 0.0244\tLR: 0.000800\n",
            "Training Epoch: 196 [6400/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 196 [6528/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 196 [6656/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 196 [6784/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 196 [6912/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 196 [7040/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 196 [7168/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 196 [7296/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 196 [7424/50000]\tLoss: 0.0283\tLR: 0.000800\n",
            "Training Epoch: 196 [7552/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 196 [7680/50000]\tLoss: 0.0637\tLR: 0.000800\n",
            "Training Epoch: 196 [7808/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 196 [7936/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 196 [8064/50000]\tLoss: 0.0285\tLR: 0.000800\n",
            "Training Epoch: 196 [8192/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 196 [8320/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 196 [8448/50000]\tLoss: 0.0198\tLR: 0.000800\n",
            "Training Epoch: 196 [8576/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 196 [8704/50000]\tLoss: 0.0227\tLR: 0.000800\n",
            "Training Epoch: 196 [8832/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 196 [8960/50000]\tLoss: 0.0392\tLR: 0.000800\n",
            "Training Epoch: 196 [9088/50000]\tLoss: 0.0304\tLR: 0.000800\n",
            "Training Epoch: 196 [9216/50000]\tLoss: 0.0149\tLR: 0.000800\n",
            "Training Epoch: 196 [9344/50000]\tLoss: 0.0451\tLR: 0.000800\n",
            "Training Epoch: 196 [9472/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 196 [9600/50000]\tLoss: 0.0445\tLR: 0.000800\n",
            "Training Epoch: 196 [9728/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 196 [9856/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 196 [9984/50000]\tLoss: 0.0369\tLR: 0.000800\n",
            "Training Epoch: 196 [10112/50000]\tLoss: 0.0211\tLR: 0.000800\n",
            "Training Epoch: 196 [10240/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 196 [10368/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 196 [10496/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 196 [10624/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 196 [10752/50000]\tLoss: 0.0461\tLR: 0.000800\n",
            "Training Epoch: 196 [10880/50000]\tLoss: 0.0593\tLR: 0.000800\n",
            "Training Epoch: 196 [11008/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 196 [11136/50000]\tLoss: 0.0993\tLR: 0.000800\n",
            "Training Epoch: 196 [11264/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 196 [11392/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 196 [11520/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 196 [11648/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 196 [11776/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 196 [11904/50000]\tLoss: 0.0252\tLR: 0.000800\n",
            "Training Epoch: 196 [12032/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 196 [12160/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 196 [12288/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 196 [12416/50000]\tLoss: 0.0359\tLR: 0.000800\n",
            "Training Epoch: 196 [12544/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 196 [12672/50000]\tLoss: 0.0477\tLR: 0.000800\n",
            "Training Epoch: 196 [12800/50000]\tLoss: 0.0212\tLR: 0.000800\n",
            "Training Epoch: 196 [12928/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 196 [13056/50000]\tLoss: 0.0258\tLR: 0.000800\n",
            "Training Epoch: 196 [13184/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 196 [13312/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 196 [13440/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 196 [13568/50000]\tLoss: 0.0255\tLR: 0.000800\n",
            "Training Epoch: 196 [13696/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 196 [13824/50000]\tLoss: 0.0275\tLR: 0.000800\n",
            "Training Epoch: 196 [13952/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 196 [14080/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 196 [14208/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 196 [14336/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 196 [14464/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 196 [14592/50000]\tLoss: 0.0162\tLR: 0.000800\n",
            "Training Epoch: 196 [14720/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 196 [14848/50000]\tLoss: 0.0255\tLR: 0.000800\n",
            "Training Epoch: 196 [14976/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 196 [15104/50000]\tLoss: 0.0320\tLR: 0.000800\n",
            "Training Epoch: 196 [15232/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 196 [15360/50000]\tLoss: 0.0400\tLR: 0.000800\n",
            "Training Epoch: 196 [15488/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 196 [15616/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 196 [15744/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 196 [15872/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 196 [16000/50000]\tLoss: 0.0412\tLR: 0.000800\n",
            "Training Epoch: 196 [16128/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 196 [16256/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 196 [16384/50000]\tLoss: 0.0280\tLR: 0.000800\n",
            "Training Epoch: 196 [16512/50000]\tLoss: 0.0431\tLR: 0.000800\n",
            "Training Epoch: 196 [16640/50000]\tLoss: 0.0621\tLR: 0.000800\n",
            "Training Epoch: 196 [16768/50000]\tLoss: 0.0306\tLR: 0.000800\n",
            "Training Epoch: 196 [16896/50000]\tLoss: 0.0513\tLR: 0.000800\n",
            "Training Epoch: 196 [17024/50000]\tLoss: 0.0427\tLR: 0.000800\n",
            "Training Epoch: 196 [17152/50000]\tLoss: 0.0400\tLR: 0.000800\n",
            "Training Epoch: 196 [17280/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 196 [17408/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 196 [17536/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 196 [17664/50000]\tLoss: 0.0289\tLR: 0.000800\n",
            "Training Epoch: 196 [17792/50000]\tLoss: 0.0261\tLR: 0.000800\n",
            "Training Epoch: 196 [17920/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 196 [18048/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 196 [18176/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 196 [18304/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 196 [18432/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 196 [18560/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 196 [18688/50000]\tLoss: 0.0269\tLR: 0.000800\n",
            "Training Epoch: 196 [18816/50000]\tLoss: 0.0484\tLR: 0.000800\n",
            "Training Epoch: 196 [18944/50000]\tLoss: 0.0372\tLR: 0.000800\n",
            "Training Epoch: 196 [19072/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 196 [19200/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 196 [19328/50000]\tLoss: 0.0273\tLR: 0.000800\n",
            "Training Epoch: 196 [19456/50000]\tLoss: 0.0441\tLR: 0.000800\n",
            "Training Epoch: 196 [19584/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 196 [19712/50000]\tLoss: 0.0458\tLR: 0.000800\n",
            "Training Epoch: 196 [19840/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 196 [19968/50000]\tLoss: 0.0264\tLR: 0.000800\n",
            "Training Epoch: 196 [20096/50000]\tLoss: 0.0274\tLR: 0.000800\n",
            "Training Epoch: 196 [20224/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 196 [20352/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 196 [20480/50000]\tLoss: 0.0309\tLR: 0.000800\n",
            "Training Epoch: 196 [20608/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 196 [20736/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 196 [20864/50000]\tLoss: 0.0245\tLR: 0.000800\n",
            "Training Epoch: 196 [20992/50000]\tLoss: 0.0516\tLR: 0.000800\n",
            "Training Epoch: 196 [21120/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 196 [21248/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 196 [21376/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 196 [21504/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 196 [21632/50000]\tLoss: 0.0255\tLR: 0.000800\n",
            "Training Epoch: 196 [21760/50000]\tLoss: 0.0342\tLR: 0.000800\n",
            "Training Epoch: 196 [21888/50000]\tLoss: 0.0311\tLR: 0.000800\n",
            "Training Epoch: 196 [22016/50000]\tLoss: 0.0272\tLR: 0.000800\n",
            "Training Epoch: 196 [22144/50000]\tLoss: 0.0291\tLR: 0.000800\n",
            "Training Epoch: 196 [22272/50000]\tLoss: 0.0261\tLR: 0.000800\n",
            "Training Epoch: 196 [22400/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 196 [22528/50000]\tLoss: 0.0263\tLR: 0.000800\n",
            "Training Epoch: 196 [22656/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 196 [22784/50000]\tLoss: 0.0343\tLR: 0.000800\n",
            "Training Epoch: 196 [22912/50000]\tLoss: 0.0026\tLR: 0.000800\n",
            "Training Epoch: 196 [23040/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 196 [23168/50000]\tLoss: 0.0417\tLR: 0.000800\n",
            "Training Epoch: 196 [23296/50000]\tLoss: 0.0299\tLR: 0.000800\n",
            "Training Epoch: 196 [23424/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 196 [23552/50000]\tLoss: 0.0224\tLR: 0.000800\n",
            "Training Epoch: 196 [23680/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 196 [23808/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 196 [23936/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 196 [24064/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 196 [24192/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 196 [24320/50000]\tLoss: 0.0303\tLR: 0.000800\n",
            "Training Epoch: 196 [24448/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 196 [24576/50000]\tLoss: 0.0281\tLR: 0.000800\n",
            "Training Epoch: 196 [24704/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 196 [24832/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 196 [24960/50000]\tLoss: 0.0650\tLR: 0.000800\n",
            "Training Epoch: 196 [25088/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 196 [25216/50000]\tLoss: 0.0020\tLR: 0.000800\n",
            "Training Epoch: 196 [25344/50000]\tLoss: 0.0791\tLR: 0.000800\n",
            "Training Epoch: 196 [25472/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 196 [25600/50000]\tLoss: 0.0261\tLR: 0.000800\n",
            "Training Epoch: 196 [25728/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 196 [25856/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 196 [25984/50000]\tLoss: 0.0226\tLR: 0.000800\n",
            "Training Epoch: 196 [26112/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 196 [26240/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 196 [26368/50000]\tLoss: 0.0208\tLR: 0.000800\n",
            "Training Epoch: 196 [26496/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 196 [26624/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 196 [26752/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 196 [26880/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 196 [27008/50000]\tLoss: 0.0310\tLR: 0.000800\n",
            "Training Epoch: 196 [27136/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 196 [27264/50000]\tLoss: 0.0322\tLR: 0.000800\n",
            "Training Epoch: 196 [27392/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 196 [27520/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 196 [27648/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 196 [27776/50000]\tLoss: 0.0383\tLR: 0.000800\n",
            "Training Epoch: 196 [27904/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 196 [28032/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 196 [28160/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 196 [28288/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 196 [28416/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 196 [28544/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 196 [28672/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 196 [28800/50000]\tLoss: 0.0505\tLR: 0.000800\n",
            "Training Epoch: 196 [28928/50000]\tLoss: 0.0301\tLR: 0.000800\n",
            "Training Epoch: 196 [29056/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 196 [29184/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 196 [29312/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 196 [29440/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 196 [29568/50000]\tLoss: 0.0327\tLR: 0.000800\n",
            "Training Epoch: 196 [29696/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 196 [29824/50000]\tLoss: 0.0225\tLR: 0.000800\n",
            "Training Epoch: 196 [29952/50000]\tLoss: 0.0520\tLR: 0.000800\n",
            "Training Epoch: 196 [30080/50000]\tLoss: 0.0170\tLR: 0.000800\n",
            "Training Epoch: 196 [30208/50000]\tLoss: 0.0327\tLR: 0.000800\n",
            "Training Epoch: 196 [30336/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 196 [30464/50000]\tLoss: 0.0366\tLR: 0.000800\n",
            "Training Epoch: 196 [30592/50000]\tLoss: 0.0385\tLR: 0.000800\n",
            "Training Epoch: 196 [30720/50000]\tLoss: 0.0303\tLR: 0.000800\n",
            "Training Epoch: 196 [30848/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 196 [30976/50000]\tLoss: 0.0434\tLR: 0.000800\n",
            "Training Epoch: 196 [31104/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 196 [31232/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 196 [31360/50000]\tLoss: 0.0682\tLR: 0.000800\n",
            "Training Epoch: 196 [31488/50000]\tLoss: 0.0466\tLR: 0.000800\n",
            "Training Epoch: 196 [31616/50000]\tLoss: 0.0203\tLR: 0.000800\n",
            "Training Epoch: 196 [31744/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 196 [31872/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 196 [32000/50000]\tLoss: 0.0181\tLR: 0.000800\n",
            "Training Epoch: 196 [32128/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 196 [32256/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 196 [32384/50000]\tLoss: 0.0216\tLR: 0.000800\n",
            "Training Epoch: 196 [32512/50000]\tLoss: 0.0455\tLR: 0.000800\n",
            "Training Epoch: 196 [32640/50000]\tLoss: 0.0283\tLR: 0.000800\n",
            "Training Epoch: 196 [32768/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 196 [32896/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 196 [33024/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 196 [33152/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 196 [33280/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 196 [33408/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 196 [33536/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 196 [33664/50000]\tLoss: 0.0384\tLR: 0.000800\n",
            "Training Epoch: 196 [33792/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 196 [33920/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 196 [34048/50000]\tLoss: 0.0344\tLR: 0.000800\n",
            "Training Epoch: 196 [34176/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 196 [34304/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 196 [34432/50000]\tLoss: 0.0371\tLR: 0.000800\n",
            "Training Epoch: 196 [34560/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 196 [34688/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 196 [34816/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 196 [34944/50000]\tLoss: 0.0257\tLR: 0.000800\n",
            "Training Epoch: 196 [35072/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 196 [35200/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 196 [35328/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 196 [35456/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 196 [35584/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 196 [35712/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 196 [35840/50000]\tLoss: 0.0470\tLR: 0.000800\n",
            "Training Epoch: 196 [35968/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 196 [36096/50000]\tLoss: 0.0363\tLR: 0.000800\n",
            "Training Epoch: 196 [36224/50000]\tLoss: 0.0272\tLR: 0.000800\n",
            "Training Epoch: 196 [36352/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 196 [36480/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 196 [36608/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 196 [36736/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 196 [36864/50000]\tLoss: 0.0463\tLR: 0.000800\n",
            "Training Epoch: 196 [36992/50000]\tLoss: 0.0315\tLR: 0.000800\n",
            "Training Epoch: 196 [37120/50000]\tLoss: 0.0215\tLR: 0.000800\n",
            "Training Epoch: 196 [37248/50000]\tLoss: 0.0369\tLR: 0.000800\n",
            "Training Epoch: 196 [37376/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 196 [37504/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 196 [37632/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 196 [37760/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 196 [37888/50000]\tLoss: 0.0345\tLR: 0.000800\n",
            "Training Epoch: 196 [38016/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 196 [38144/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 196 [38272/50000]\tLoss: 0.0358\tLR: 0.000800\n",
            "Training Epoch: 196 [38400/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 196 [38528/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 196 [38656/50000]\tLoss: 0.0257\tLR: 0.000800\n",
            "Training Epoch: 196 [38784/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 196 [38912/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 196 [39040/50000]\tLoss: 0.0255\tLR: 0.000800\n",
            "Training Epoch: 196 [39168/50000]\tLoss: 0.0290\tLR: 0.000800\n",
            "Training Epoch: 196 [39296/50000]\tLoss: 0.0971\tLR: 0.000800\n",
            "Training Epoch: 196 [39424/50000]\tLoss: 0.0310\tLR: 0.000800\n",
            "Training Epoch: 196 [39552/50000]\tLoss: 0.0136\tLR: 0.000800\n",
            "Training Epoch: 196 [39680/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 196 [39808/50000]\tLoss: 0.0266\tLR: 0.000800\n",
            "Training Epoch: 196 [39936/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 196 [40064/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 196 [40192/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 196 [40320/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 196 [40448/50000]\tLoss: 0.0209\tLR: 0.000800\n",
            "Training Epoch: 196 [40576/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 196 [40704/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 196 [40832/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 196 [40960/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 196 [41088/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 196 [41216/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 196 [41344/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 196 [41472/50000]\tLoss: 0.0613\tLR: 0.000800\n",
            "Training Epoch: 196 [41600/50000]\tLoss: 0.0307\tLR: 0.000800\n",
            "Training Epoch: 196 [41728/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 196 [41856/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 196 [41984/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 196 [42112/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 196 [42240/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 196 [42368/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 196 [42496/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 196 [42624/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 196 [42752/50000]\tLoss: 0.0287\tLR: 0.000800\n",
            "Training Epoch: 196 [42880/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 196 [43008/50000]\tLoss: 0.0652\tLR: 0.000800\n",
            "Training Epoch: 196 [43136/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 196 [43264/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 196 [43392/50000]\tLoss: 0.0456\tLR: 0.000800\n",
            "Training Epoch: 196 [43520/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 196 [43648/50000]\tLoss: 0.0334\tLR: 0.000800\n",
            "Training Epoch: 196 [43776/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 196 [43904/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 196 [44032/50000]\tLoss: 0.0372\tLR: 0.000800\n",
            "Training Epoch: 196 [44160/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 196 [44288/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 196 [44416/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 196 [44544/50000]\tLoss: 0.0484\tLR: 0.000800\n",
            "Training Epoch: 196 [44672/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 196 [44800/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 196 [44928/50000]\tLoss: 0.0305\tLR: 0.000800\n",
            "Training Epoch: 196 [45056/50000]\tLoss: 0.0465\tLR: 0.000800\n",
            "Training Epoch: 196 [45184/50000]\tLoss: 0.0207\tLR: 0.000800\n",
            "Training Epoch: 196 [45312/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 196 [45440/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 196 [45568/50000]\tLoss: 0.0497\tLR: 0.000800\n",
            "Training Epoch: 196 [45696/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 196 [45824/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 196 [45952/50000]\tLoss: 0.0283\tLR: 0.000800\n",
            "Training Epoch: 196 [46080/50000]\tLoss: 0.0537\tLR: 0.000800\n",
            "Training Epoch: 196 [46208/50000]\tLoss: 0.0618\tLR: 0.000800\n",
            "Training Epoch: 196 [46336/50000]\tLoss: 0.0293\tLR: 0.000800\n",
            "Training Epoch: 196 [46464/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 196 [46592/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 196 [46720/50000]\tLoss: 0.0325\tLR: 0.000800\n",
            "Training Epoch: 196 [46848/50000]\tLoss: 0.0367\tLR: 0.000800\n",
            "Training Epoch: 196 [46976/50000]\tLoss: 0.0343\tLR: 0.000800\n",
            "Training Epoch: 196 [47104/50000]\tLoss: 0.0554\tLR: 0.000800\n",
            "Training Epoch: 196 [47232/50000]\tLoss: 0.0172\tLR: 0.000800\n",
            "Training Epoch: 196 [47360/50000]\tLoss: 0.0262\tLR: 0.000800\n",
            "Training Epoch: 196 [47488/50000]\tLoss: 0.0499\tLR: 0.000800\n",
            "Training Epoch: 196 [47616/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 196 [47744/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 196 [47872/50000]\tLoss: 0.0590\tLR: 0.000800\n",
            "Training Epoch: 196 [48000/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 196 [48128/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 196 [48256/50000]\tLoss: 0.0405\tLR: 0.000800\n",
            "Training Epoch: 196 [48384/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 196 [48512/50000]\tLoss: 0.0359\tLR: 0.000800\n",
            "Training Epoch: 196 [48640/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 196 [48768/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 196 [48896/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 196 [49024/50000]\tLoss: 0.0352\tLR: 0.000800\n",
            "Training Epoch: 196 [49152/50000]\tLoss: 0.0420\tLR: 0.000800\n",
            "Training Epoch: 196 [49280/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 196 [49408/50000]\tLoss: 0.0355\tLR: 0.000800\n",
            "Training Epoch: 196 [49536/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 196 [49664/50000]\tLoss: 0.0726\tLR: 0.000800\n",
            "Training Epoch: 196 [49792/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 196 [49920/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 196 [50000/50000]\tLoss: 0.0196\tLR: 0.000800\n",
            "epoch 196 training time consumed: 35.66s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 218370 GiB | 218369 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 216379 GiB | 216379 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1990 GiB |   1990 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 218370 GiB | 218369 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 216379 GiB | 216379 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   1990 GiB |   1990 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 217695 GiB | 217695 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 215706 GiB | 215706 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   1989 GiB |   1989 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 193146 GiB | 193146 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 191156 GiB | 191156 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   1990 GiB |   1990 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   23920 K  |   23920 K  |\n",
            "|       from large pool |      38    |      61    |   10454 K  |   10454 K  |\n",
            "|       from small pool |     187    |     230    |   13466 K  |   13466 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   23920 K  |   23920 K  |\n",
            "|       from large pool |      38    |      61    |   10454 K  |   10454 K  |\n",
            "|       from small pool |     187    |     230    |   13466 K  |   13466 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |    9917 K  |    9917 K  |\n",
            "|       from large pool |      13    |      15    |    4752 K  |    4752 K  |\n",
            "|       from small pool |       7    |      14    |    5165 K  |    5165 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 196, Average loss: 0.0129, Accuracy: 0.7172, Time consumed:2.94s\n",
            "\n",
            "Training Epoch: 197 [128/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 197 [256/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 197 [384/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 197 [512/50000]\tLoss: 0.0360\tLR: 0.000800\n",
            "Training Epoch: 197 [640/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 197 [768/50000]\tLoss: 0.0361\tLR: 0.000800\n",
            "Training Epoch: 197 [896/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 197 [1024/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 197 [1152/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 197 [1280/50000]\tLoss: 0.0633\tLR: 0.000800\n",
            "Training Epoch: 197 [1408/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 197 [1536/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 197 [1664/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 197 [1792/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 197 [1920/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 197 [2048/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 197 [2176/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 197 [2304/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 197 [2432/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 197 [2560/50000]\tLoss: 0.0271\tLR: 0.000800\n",
            "Training Epoch: 197 [2688/50000]\tLoss: 0.0199\tLR: 0.000800\n",
            "Training Epoch: 197 [2816/50000]\tLoss: 0.0096\tLR: 0.000800\n",
            "Training Epoch: 197 [2944/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 197 [3072/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 197 [3200/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 197 [3328/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 197 [3456/50000]\tLoss: 0.0257\tLR: 0.000800\n",
            "Training Epoch: 197 [3584/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 197 [3712/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 197 [3840/50000]\tLoss: 0.0318\tLR: 0.000800\n",
            "Training Epoch: 197 [3968/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 197 [4096/50000]\tLoss: 0.1051\tLR: 0.000800\n",
            "Training Epoch: 197 [4224/50000]\tLoss: 0.0463\tLR: 0.000800\n",
            "Training Epoch: 197 [4352/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 197 [4480/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 197 [4608/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 197 [4736/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 197 [4864/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 197 [4992/50000]\tLoss: 0.0895\tLR: 0.000800\n",
            "Training Epoch: 197 [5120/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 197 [5248/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 197 [5376/50000]\tLoss: 0.0202\tLR: 0.000800\n",
            "Training Epoch: 197 [5504/50000]\tLoss: 0.0355\tLR: 0.000800\n",
            "Training Epoch: 197 [5632/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 197 [5760/50000]\tLoss: 0.0378\tLR: 0.000800\n",
            "Training Epoch: 197 [5888/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 197 [6016/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 197 [6144/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 197 [6272/50000]\tLoss: 0.0235\tLR: 0.000800\n",
            "Training Epoch: 197 [6400/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 197 [6528/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 197 [6656/50000]\tLoss: 0.0346\tLR: 0.000800\n",
            "Training Epoch: 197 [6784/50000]\tLoss: 0.0467\tLR: 0.000800\n",
            "Training Epoch: 197 [6912/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 197 [7040/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 197 [7168/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 197 [7296/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 197 [7424/50000]\tLoss: 0.0671\tLR: 0.000800\n",
            "Training Epoch: 197 [7552/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 197 [7680/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 197 [7808/50000]\tLoss: 0.0346\tLR: 0.000800\n",
            "Training Epoch: 197 [7936/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 197 [8064/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 197 [8192/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 197 [8320/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 197 [8448/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 197 [8576/50000]\tLoss: 0.0382\tLR: 0.000800\n",
            "Training Epoch: 197 [8704/50000]\tLoss: 0.0172\tLR: 0.000800\n",
            "Training Epoch: 197 [8832/50000]\tLoss: 0.0434\tLR: 0.000800\n",
            "Training Epoch: 197 [8960/50000]\tLoss: 0.0478\tLR: 0.000800\n",
            "Training Epoch: 197 [9088/50000]\tLoss: 0.0233\tLR: 0.000800\n",
            "Training Epoch: 197 [9216/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 197 [9344/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 197 [9472/50000]\tLoss: 0.0444\tLR: 0.000800\n",
            "Training Epoch: 197 [9600/50000]\tLoss: 0.0491\tLR: 0.000800\n",
            "Training Epoch: 197 [9728/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 197 [9856/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 197 [9984/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 197 [10112/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 197 [10240/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 197 [10368/50000]\tLoss: 0.0215\tLR: 0.000800\n",
            "Training Epoch: 197 [10496/50000]\tLoss: 0.0025\tLR: 0.000800\n",
            "Training Epoch: 197 [10624/50000]\tLoss: 0.0298\tLR: 0.000800\n",
            "Training Epoch: 197 [10752/50000]\tLoss: 0.0992\tLR: 0.000800\n",
            "Training Epoch: 197 [10880/50000]\tLoss: 0.0441\tLR: 0.000800\n",
            "Training Epoch: 197 [11008/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 197 [11136/50000]\tLoss: 0.0237\tLR: 0.000800\n",
            "Training Epoch: 197 [11264/50000]\tLoss: 0.0305\tLR: 0.000800\n",
            "Training Epoch: 197 [11392/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 197 [11520/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 197 [11648/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 197 [11776/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 197 [11904/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 197 [12032/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 197 [12160/50000]\tLoss: 0.0162\tLR: 0.000800\n",
            "Training Epoch: 197 [12288/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 197 [12416/50000]\tLoss: 0.0323\tLR: 0.000800\n",
            "Training Epoch: 197 [12544/50000]\tLoss: 0.0203\tLR: 0.000800\n",
            "Training Epoch: 197 [12672/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 197 [12800/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 197 [12928/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 197 [13056/50000]\tLoss: 0.0243\tLR: 0.000800\n",
            "Training Epoch: 197 [13184/50000]\tLoss: 0.0269\tLR: 0.000800\n",
            "Training Epoch: 197 [13312/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 197 [13440/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 197 [13568/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 197 [13696/50000]\tLoss: 0.0526\tLR: 0.000800\n",
            "Training Epoch: 197 [13824/50000]\tLoss: 0.0205\tLR: 0.000800\n",
            "Training Epoch: 197 [13952/50000]\tLoss: 0.0514\tLR: 0.000800\n",
            "Training Epoch: 197 [14080/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 197 [14208/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 197 [14336/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 197 [14464/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 197 [14592/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 197 [14720/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 197 [14848/50000]\tLoss: 0.0290\tLR: 0.000800\n",
            "Training Epoch: 197 [14976/50000]\tLoss: 0.0643\tLR: 0.000800\n",
            "Training Epoch: 197 [15104/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 197 [15232/50000]\tLoss: 0.0366\tLR: 0.000800\n",
            "Training Epoch: 197 [15360/50000]\tLoss: 0.0282\tLR: 0.000800\n",
            "Training Epoch: 197 [15488/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 197 [15616/50000]\tLoss: 0.0572\tLR: 0.000800\n",
            "Training Epoch: 197 [15744/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 197 [15872/50000]\tLoss: 0.0400\tLR: 0.000800\n",
            "Training Epoch: 197 [16000/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 197 [16128/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 197 [16256/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 197 [16384/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 197 [16512/50000]\tLoss: 0.0421\tLR: 0.000800\n",
            "Training Epoch: 197 [16640/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 197 [16768/50000]\tLoss: 0.0255\tLR: 0.000800\n",
            "Training Epoch: 197 [16896/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 197 [17024/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 197 [17152/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 197 [17280/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 197 [17408/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 197 [17536/50000]\tLoss: 0.0378\tLR: 0.000800\n",
            "Training Epoch: 197 [17664/50000]\tLoss: 0.0454\tLR: 0.000800\n",
            "Training Epoch: 197 [17792/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 197 [17920/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 197 [18048/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 197 [18176/50000]\tLoss: 0.0513\tLR: 0.000800\n",
            "Training Epoch: 197 [18304/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 197 [18432/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 197 [18560/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 197 [18688/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 197 [18816/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 197 [18944/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 197 [19072/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 197 [19200/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 197 [19328/50000]\tLoss: 0.0522\tLR: 0.000800\n",
            "Training Epoch: 197 [19456/50000]\tLoss: 0.0328\tLR: 0.000800\n",
            "Training Epoch: 197 [19584/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 197 [19712/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 197 [19840/50000]\tLoss: 0.0293\tLR: 0.000800\n",
            "Training Epoch: 197 [19968/50000]\tLoss: 0.0713\tLR: 0.000800\n",
            "Training Epoch: 197 [20096/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 197 [20224/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 197 [20352/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 197 [20480/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 197 [20608/50000]\tLoss: 0.0310\tLR: 0.000800\n",
            "Training Epoch: 197 [20736/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 197 [20864/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 197 [20992/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 197 [21120/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 197 [21248/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 197 [21376/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 197 [21504/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 197 [21632/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 197 [21760/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 197 [21888/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 197 [22016/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 197 [22144/50000]\tLoss: 0.0147\tLR: 0.000800\n",
            "Training Epoch: 197 [22272/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 197 [22400/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 197 [22528/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 197 [22656/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 197 [22784/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 197 [22912/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 197 [23040/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 197 [23168/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 197 [23296/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 197 [23424/50000]\tLoss: 0.0585\tLR: 0.000800\n",
            "Training Epoch: 197 [23552/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 197 [23680/50000]\tLoss: 0.0470\tLR: 0.000800\n",
            "Training Epoch: 197 [23808/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 197 [23936/50000]\tLoss: 0.0951\tLR: 0.000800\n",
            "Training Epoch: 197 [24064/50000]\tLoss: 0.0410\tLR: 0.000800\n",
            "Training Epoch: 197 [24192/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 197 [24320/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 197 [24448/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 197 [24576/50000]\tLoss: 0.0388\tLR: 0.000800\n",
            "Training Epoch: 197 [24704/50000]\tLoss: 0.0560\tLR: 0.000800\n",
            "Training Epoch: 197 [24832/50000]\tLoss: 0.0160\tLR: 0.000800\n",
            "Training Epoch: 197 [24960/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 197 [25088/50000]\tLoss: 0.0192\tLR: 0.000800\n",
            "Training Epoch: 197 [25216/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 197 [25344/50000]\tLoss: 0.0212\tLR: 0.000800\n",
            "Training Epoch: 197 [25472/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 197 [25600/50000]\tLoss: 0.0313\tLR: 0.000800\n",
            "Training Epoch: 197 [25728/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 197 [25856/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 197 [25984/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 197 [26112/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 197 [26240/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 197 [26368/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 197 [26496/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 197 [26624/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 197 [26752/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 197 [26880/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 197 [27008/50000]\tLoss: 0.0280\tLR: 0.000800\n",
            "Training Epoch: 197 [27136/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 197 [27264/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 197 [27392/50000]\tLoss: 0.0915\tLR: 0.000800\n",
            "Training Epoch: 197 [27520/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 197 [27648/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 197 [27776/50000]\tLoss: 0.0349\tLR: 0.000800\n",
            "Training Epoch: 197 [27904/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 197 [28032/50000]\tLoss: 0.0823\tLR: 0.000800\n",
            "Training Epoch: 197 [28160/50000]\tLoss: 0.0333\tLR: 0.000800\n",
            "Training Epoch: 197 [28288/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 197 [28416/50000]\tLoss: 0.0142\tLR: 0.000800\n",
            "Training Epoch: 197 [28544/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 197 [28672/50000]\tLoss: 0.0418\tLR: 0.000800\n",
            "Training Epoch: 197 [28800/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 197 [28928/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 197 [29056/50000]\tLoss: 0.0327\tLR: 0.000800\n",
            "Training Epoch: 197 [29184/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 197 [29312/50000]\tLoss: 0.0484\tLR: 0.000800\n",
            "Training Epoch: 197 [29440/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 197 [29568/50000]\tLoss: 0.0618\tLR: 0.000800\n",
            "Training Epoch: 197 [29696/50000]\tLoss: 0.0425\tLR: 0.000800\n",
            "Training Epoch: 197 [29824/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 197 [29952/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 197 [30080/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 197 [30208/50000]\tLoss: 0.0356\tLR: 0.000800\n",
            "Training Epoch: 197 [30336/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 197 [30464/50000]\tLoss: 0.0190\tLR: 0.000800\n",
            "Training Epoch: 197 [30592/50000]\tLoss: 0.0251\tLR: 0.000800\n",
            "Training Epoch: 197 [30720/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 197 [30848/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 197 [30976/50000]\tLoss: 0.0096\tLR: 0.000800\n",
            "Training Epoch: 197 [31104/50000]\tLoss: 0.0204\tLR: 0.000800\n",
            "Training Epoch: 197 [31232/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 197 [31360/50000]\tLoss: 0.0214\tLR: 0.000800\n",
            "Training Epoch: 197 [31488/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 197 [31616/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 197 [31744/50000]\tLoss: 0.0380\tLR: 0.000800\n",
            "Training Epoch: 197 [31872/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 197 [32000/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 197 [32128/50000]\tLoss: 0.0293\tLR: 0.000800\n",
            "Training Epoch: 197 [32256/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 197 [32384/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 197 [32512/50000]\tLoss: 0.0205\tLR: 0.000800\n",
            "Training Epoch: 197 [32640/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 197 [32768/50000]\tLoss: 0.0491\tLR: 0.000800\n",
            "Training Epoch: 197 [32896/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 197 [33024/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 197 [33152/50000]\tLoss: 0.0251\tLR: 0.000800\n",
            "Training Epoch: 197 [33280/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 197 [33408/50000]\tLoss: 0.0506\tLR: 0.000800\n",
            "Training Epoch: 197 [33536/50000]\tLoss: 0.0336\tLR: 0.000800\n",
            "Training Epoch: 197 [33664/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 197 [33792/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 197 [33920/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 197 [34048/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 197 [34176/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 197 [34304/50000]\tLoss: 0.0541\tLR: 0.000800\n",
            "Training Epoch: 197 [34432/50000]\tLoss: 0.0447\tLR: 0.000800\n",
            "Training Epoch: 197 [34560/50000]\tLoss: 0.0204\tLR: 0.000800\n",
            "Training Epoch: 197 [34688/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 197 [34816/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 197 [34944/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 197 [35072/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 197 [35200/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 197 [35328/50000]\tLoss: 0.0403\tLR: 0.000800\n",
            "Training Epoch: 197 [35456/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 197 [35584/50000]\tLoss: 0.0898\tLR: 0.000800\n",
            "Training Epoch: 197 [35712/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 197 [35840/50000]\tLoss: 0.0316\tLR: 0.000800\n",
            "Training Epoch: 197 [35968/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 197 [36096/50000]\tLoss: 0.0223\tLR: 0.000800\n",
            "Training Epoch: 197 [36224/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 197 [36352/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 197 [36480/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 197 [36608/50000]\tLoss: 0.0243\tLR: 0.000800\n",
            "Training Epoch: 197 [36736/50000]\tLoss: 0.0356\tLR: 0.000800\n",
            "Training Epoch: 197 [36864/50000]\tLoss: 0.0489\tLR: 0.000800\n",
            "Training Epoch: 197 [36992/50000]\tLoss: 0.0172\tLR: 0.000800\n",
            "Training Epoch: 197 [37120/50000]\tLoss: 0.0532\tLR: 0.000800\n",
            "Training Epoch: 197 [37248/50000]\tLoss: 0.0616\tLR: 0.000800\n",
            "Training Epoch: 197 [37376/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 197 [37504/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 197 [37632/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 197 [37760/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 197 [37888/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 197 [38016/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 197 [38144/50000]\tLoss: 0.0638\tLR: 0.000800\n",
            "Training Epoch: 197 [38272/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 197 [38400/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 197 [38528/50000]\tLoss: 0.0388\tLR: 0.000800\n",
            "Training Epoch: 197 [38656/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 197 [38784/50000]\tLoss: 0.0893\tLR: 0.000800\n",
            "Training Epoch: 197 [38912/50000]\tLoss: 0.0334\tLR: 0.000800\n",
            "Training Epoch: 197 [39040/50000]\tLoss: 0.0213\tLR: 0.000800\n",
            "Training Epoch: 197 [39168/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 197 [39296/50000]\tLoss: 0.0202\tLR: 0.000800\n",
            "Training Epoch: 197 [39424/50000]\tLoss: 0.0220\tLR: 0.000800\n",
            "Training Epoch: 197 [39552/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 197 [39680/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 197 [39808/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 197 [39936/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 197 [40064/50000]\tLoss: 0.0385\tLR: 0.000800\n",
            "Training Epoch: 197 [40192/50000]\tLoss: 0.0165\tLR: 0.000800\n",
            "Training Epoch: 197 [40320/50000]\tLoss: 0.0343\tLR: 0.000800\n",
            "Training Epoch: 197 [40448/50000]\tLoss: 0.0257\tLR: 0.000800\n",
            "Training Epoch: 197 [40576/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 197 [40704/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 197 [40832/50000]\tLoss: 0.0465\tLR: 0.000800\n",
            "Training Epoch: 197 [40960/50000]\tLoss: 0.0720\tLR: 0.000800\n",
            "Training Epoch: 197 [41088/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 197 [41216/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 197 [41344/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 197 [41472/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 197 [41600/50000]\tLoss: 0.0392\tLR: 0.000800\n",
            "Training Epoch: 197 [41728/50000]\tLoss: 0.0441\tLR: 0.000800\n",
            "Training Epoch: 197 [41856/50000]\tLoss: 0.0198\tLR: 0.000800\n",
            "Training Epoch: 197 [41984/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 197 [42112/50000]\tLoss: 0.0303\tLR: 0.000800\n",
            "Training Epoch: 197 [42240/50000]\tLoss: 0.0200\tLR: 0.000800\n",
            "Training Epoch: 197 [42368/50000]\tLoss: 0.0393\tLR: 0.000800\n",
            "Training Epoch: 197 [42496/50000]\tLoss: 0.0581\tLR: 0.000800\n",
            "Training Epoch: 197 [42624/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 197 [42752/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 197 [42880/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 197 [43008/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 197 [43136/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 197 [43264/50000]\tLoss: 0.0265\tLR: 0.000800\n",
            "Training Epoch: 197 [43392/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 197 [43520/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 197 [43648/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 197 [43776/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 197 [43904/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 197 [44032/50000]\tLoss: 0.0488\tLR: 0.000800\n",
            "Training Epoch: 197 [44160/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 197 [44288/50000]\tLoss: 0.0545\tLR: 0.000800\n",
            "Training Epoch: 197 [44416/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 197 [44544/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 197 [44672/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 197 [44800/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 197 [44928/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 197 [45056/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 197 [45184/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 197 [45312/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 197 [45440/50000]\tLoss: 0.0453\tLR: 0.000800\n",
            "Training Epoch: 197 [45568/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 197 [45696/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 197 [45824/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 197 [45952/50000]\tLoss: 0.0370\tLR: 0.000800\n",
            "Training Epoch: 197 [46080/50000]\tLoss: 0.0527\tLR: 0.000800\n",
            "Training Epoch: 197 [46208/50000]\tLoss: 0.0451\tLR: 0.000800\n",
            "Training Epoch: 197 [46336/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 197 [46464/50000]\tLoss: 0.0198\tLR: 0.000800\n",
            "Training Epoch: 197 [46592/50000]\tLoss: 0.0402\tLR: 0.000800\n",
            "Training Epoch: 197 [46720/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 197 [46848/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 197 [46976/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 197 [47104/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 197 [47232/50000]\tLoss: 0.0459\tLR: 0.000800\n",
            "Training Epoch: 197 [47360/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 197 [47488/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 197 [47616/50000]\tLoss: 0.0296\tLR: 0.000800\n",
            "Training Epoch: 197 [47744/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 197 [47872/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 197 [48000/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 197 [48128/50000]\tLoss: 0.0256\tLR: 0.000800\n",
            "Training Epoch: 197 [48256/50000]\tLoss: 0.0426\tLR: 0.000800\n",
            "Training Epoch: 197 [48384/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 197 [48512/50000]\tLoss: 0.0474\tLR: 0.000800\n",
            "Training Epoch: 197 [48640/50000]\tLoss: 0.0362\tLR: 0.000800\n",
            "Training Epoch: 197 [48768/50000]\tLoss: 0.0209\tLR: 0.000800\n",
            "Training Epoch: 197 [48896/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 197 [49024/50000]\tLoss: 0.0256\tLR: 0.000800\n",
            "Training Epoch: 197 [49152/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 197 [49280/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 197 [49408/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 197 [49536/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 197 [49664/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 197 [49792/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 197 [49920/50000]\tLoss: 0.0252\tLR: 0.000800\n",
            "Training Epoch: 197 [50000/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "epoch 197 training time consumed: 33.01s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 219484 GiB | 219483 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 217483 GiB | 217483 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   2000 GiB |   2000 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 219484 GiB | 219483 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 217483 GiB | 217483 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   2000 GiB |   2000 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 218806 GiB | 218806 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 216807 GiB | 216806 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   1999 GiB |   1999 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 194132 GiB | 194132 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 192131 GiB | 192131 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   2000 GiB |   2000 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   24042 K  |   24042 K  |\n",
            "|       from large pool |      38    |      61    |   10507 K  |   10507 K  |\n",
            "|       from small pool |     187    |     230    |   13534 K  |   13534 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   24042 K  |   24042 K  |\n",
            "|       from large pool |      38    |      61    |   10507 K  |   10507 K  |\n",
            "|       from small pool |     187    |     230    |   13534 K  |   13534 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |    9969 K  |    9969 K  |\n",
            "|       from large pool |      13    |      15    |    4776 K  |    4776 K  |\n",
            "|       from small pool |       7    |      14    |    5193 K  |    5193 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 197, Average loss: 0.0129, Accuracy: 0.7181, Time consumed:4.46s\n",
            "\n",
            "Training Epoch: 198 [128/50000]\tLoss: 0.0305\tLR: 0.000800\n",
            "Training Epoch: 198 [256/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 198 [384/50000]\tLoss: 0.0615\tLR: 0.000800\n",
            "Training Epoch: 198 [512/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 198 [640/50000]\tLoss: 0.0157\tLR: 0.000800\n",
            "Training Epoch: 198 [768/50000]\tLoss: 0.0296\tLR: 0.000800\n",
            "Training Epoch: 198 [896/50000]\tLoss: 0.0026\tLR: 0.000800\n",
            "Training Epoch: 198 [1024/50000]\tLoss: 0.0212\tLR: 0.000800\n",
            "Training Epoch: 198 [1152/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 198 [1280/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 198 [1408/50000]\tLoss: 0.0194\tLR: 0.000800\n",
            "Training Epoch: 198 [1536/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 198 [1664/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 198 [1792/50000]\tLoss: 0.0564\tLR: 0.000800\n",
            "Training Epoch: 198 [1920/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 198 [2048/50000]\tLoss: 0.0136\tLR: 0.000800\n",
            "Training Epoch: 198 [2176/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 198 [2304/50000]\tLoss: 0.0377\tLR: 0.000800\n",
            "Training Epoch: 198 [2432/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 198 [2560/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 198 [2688/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 198 [2816/50000]\tLoss: 0.0188\tLR: 0.000800\n",
            "Training Epoch: 198 [2944/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 198 [3072/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 198 [3200/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 198 [3328/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 198 [3456/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 198 [3584/50000]\tLoss: 0.0659\tLR: 0.000800\n",
            "Training Epoch: 198 [3712/50000]\tLoss: 0.0428\tLR: 0.000800\n",
            "Training Epoch: 198 [3840/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 198 [3968/50000]\tLoss: 0.0223\tLR: 0.000800\n",
            "Training Epoch: 198 [4096/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 198 [4224/50000]\tLoss: 0.0172\tLR: 0.000800\n",
            "Training Epoch: 198 [4352/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 198 [4480/50000]\tLoss: 0.0404\tLR: 0.000800\n",
            "Training Epoch: 198 [4608/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 198 [4736/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 198 [4864/50000]\tLoss: 0.0242\tLR: 0.000800\n",
            "Training Epoch: 198 [4992/50000]\tLoss: 0.0212\tLR: 0.000800\n",
            "Training Epoch: 198 [5120/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 198 [5248/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 198 [5376/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 198 [5504/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 198 [5632/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 198 [5760/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 198 [5888/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 198 [6016/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 198 [6144/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 198 [6272/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 198 [6400/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 198 [6528/50000]\tLoss: 0.0226\tLR: 0.000800\n",
            "Training Epoch: 198 [6656/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 198 [6784/50000]\tLoss: 0.0462\tLR: 0.000800\n",
            "Training Epoch: 198 [6912/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 198 [7040/50000]\tLoss: 0.0325\tLR: 0.000800\n",
            "Training Epoch: 198 [7168/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 198 [7296/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 198 [7424/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 198 [7552/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 198 [7680/50000]\tLoss: 0.0174\tLR: 0.000800\n",
            "Training Epoch: 198 [7808/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 198 [7936/50000]\tLoss: 0.0275\tLR: 0.000800\n",
            "Training Epoch: 198 [8064/50000]\tLoss: 0.0467\tLR: 0.000800\n",
            "Training Epoch: 198 [8192/50000]\tLoss: 0.0301\tLR: 0.000800\n",
            "Training Epoch: 198 [8320/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 198 [8448/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 198 [8576/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 198 [8704/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 198 [8832/50000]\tLoss: 0.0244\tLR: 0.000800\n",
            "Training Epoch: 198 [8960/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 198 [9088/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 198 [9216/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 198 [9344/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 198 [9472/50000]\tLoss: 0.0147\tLR: 0.000800\n",
            "Training Epoch: 198 [9600/50000]\tLoss: 0.0303\tLR: 0.000800\n",
            "Training Epoch: 198 [9728/50000]\tLoss: 0.0223\tLR: 0.000800\n",
            "Training Epoch: 198 [9856/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 198 [9984/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 198 [10112/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 198 [10240/50000]\tLoss: 0.0284\tLR: 0.000800\n",
            "Training Epoch: 198 [10368/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 198 [10496/50000]\tLoss: 0.0726\tLR: 0.000800\n",
            "Training Epoch: 198 [10624/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 198 [10752/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 198 [10880/50000]\tLoss: 0.0290\tLR: 0.000800\n",
            "Training Epoch: 198 [11008/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 198 [11136/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 198 [11264/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 198 [11392/50000]\tLoss: 0.0486\tLR: 0.000800\n",
            "Training Epoch: 198 [11520/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 198 [11648/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 198 [11776/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 198 [11904/50000]\tLoss: 0.0155\tLR: 0.000800\n",
            "Training Epoch: 198 [12032/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 198 [12160/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 198 [12288/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 198 [12416/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 198 [12544/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 198 [12672/50000]\tLoss: 0.0375\tLR: 0.000800\n",
            "Training Epoch: 198 [12800/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 198 [12928/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 198 [13056/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 198 [13184/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 198 [13312/50000]\tLoss: 0.0282\tLR: 0.000800\n",
            "Training Epoch: 198 [13440/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 198 [13568/50000]\tLoss: 0.0026\tLR: 0.000800\n",
            "Training Epoch: 198 [13696/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 198 [13824/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 198 [13952/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 198 [14080/50000]\tLoss: 0.0273\tLR: 0.000800\n",
            "Training Epoch: 198 [14208/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 198 [14336/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 198 [14464/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 198 [14592/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 198 [14720/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 198 [14848/50000]\tLoss: 0.0665\tLR: 0.000800\n",
            "Training Epoch: 198 [14976/50000]\tLoss: 0.0775\tLR: 0.000800\n",
            "Training Epoch: 198 [15104/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 198 [15232/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 198 [15360/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 198 [15488/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 198 [15616/50000]\tLoss: 0.0204\tLR: 0.000800\n",
            "Training Epoch: 198 [15744/50000]\tLoss: 0.0214\tLR: 0.000800\n",
            "Training Epoch: 198 [15872/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 198 [16000/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 198 [16128/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 198 [16256/50000]\tLoss: 0.0284\tLR: 0.000800\n",
            "Training Epoch: 198 [16384/50000]\tLoss: 0.0251\tLR: 0.000800\n",
            "Training Epoch: 198 [16512/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 198 [16640/50000]\tLoss: 0.0449\tLR: 0.000800\n",
            "Training Epoch: 198 [16768/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 198 [16896/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 198 [17024/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 198 [17152/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 198 [17280/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 198 [17408/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 198 [17536/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 198 [17664/50000]\tLoss: 0.0584\tLR: 0.000800\n",
            "Training Epoch: 198 [17792/50000]\tLoss: 0.0224\tLR: 0.000800\n",
            "Training Epoch: 198 [17920/50000]\tLoss: 0.0217\tLR: 0.000800\n",
            "Training Epoch: 198 [18048/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 198 [18176/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 198 [18304/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 198 [18432/50000]\tLoss: 0.0177\tLR: 0.000800\n",
            "Training Epoch: 198 [18560/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 198 [18688/50000]\tLoss: 0.0645\tLR: 0.000800\n",
            "Training Epoch: 198 [18816/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 198 [18944/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 198 [19072/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 198 [19200/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 198 [19328/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 198 [19456/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 198 [19584/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 198 [19712/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 198 [19840/50000]\tLoss: 0.0020\tLR: 0.000800\n",
            "Training Epoch: 198 [19968/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 198 [20096/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 198 [20224/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 198 [20352/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 198 [20480/50000]\tLoss: 0.0371\tLR: 0.000800\n",
            "Training Epoch: 198 [20608/50000]\tLoss: 0.0342\tLR: 0.000800\n",
            "Training Epoch: 198 [20736/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 198 [20864/50000]\tLoss: 0.0026\tLR: 0.000800\n",
            "Training Epoch: 198 [20992/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 198 [21120/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 198 [21248/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 198 [21376/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 198 [21504/50000]\tLoss: 0.0278\tLR: 0.000800\n",
            "Training Epoch: 198 [21632/50000]\tLoss: 0.0025\tLR: 0.000800\n",
            "Training Epoch: 198 [21760/50000]\tLoss: 0.0270\tLR: 0.000800\n",
            "Training Epoch: 198 [21888/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 198 [22016/50000]\tLoss: 0.0502\tLR: 0.000800\n",
            "Training Epoch: 198 [22144/50000]\tLoss: 0.0839\tLR: 0.000800\n",
            "Training Epoch: 198 [22272/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 198 [22400/50000]\tLoss: 0.0189\tLR: 0.000800\n",
            "Training Epoch: 198 [22528/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 198 [22656/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 198 [22784/50000]\tLoss: 0.0272\tLR: 0.000800\n",
            "Training Epoch: 198 [22912/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 198 [23040/50000]\tLoss: 0.0145\tLR: 0.000800\n",
            "Training Epoch: 198 [23168/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 198 [23296/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 198 [23424/50000]\tLoss: 0.0389\tLR: 0.000800\n",
            "Training Epoch: 198 [23552/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 198 [23680/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 198 [23808/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 198 [23936/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 198 [24064/50000]\tLoss: 0.0583\tLR: 0.000800\n",
            "Training Epoch: 198 [24192/50000]\tLoss: 0.0280\tLR: 0.000800\n",
            "Training Epoch: 198 [24320/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 198 [24448/50000]\tLoss: 0.0337\tLR: 0.000800\n",
            "Training Epoch: 198 [24576/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 198 [24704/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 198 [24832/50000]\tLoss: 0.0423\tLR: 0.000800\n",
            "Training Epoch: 198 [24960/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 198 [25088/50000]\tLoss: 0.0204\tLR: 0.000800\n",
            "Training Epoch: 198 [25216/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 198 [25344/50000]\tLoss: 0.0442\tLR: 0.000800\n",
            "Training Epoch: 198 [25472/50000]\tLoss: 0.0116\tLR: 0.000800\n",
            "Training Epoch: 198 [25600/50000]\tLoss: 0.0491\tLR: 0.000800\n",
            "Training Epoch: 198 [25728/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 198 [25856/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 198 [25984/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 198 [26112/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 198 [26240/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 198 [26368/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 198 [26496/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 198 [26624/50000]\tLoss: 0.0242\tLR: 0.000800\n",
            "Training Epoch: 198 [26752/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 198 [26880/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 198 [27008/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 198 [27136/50000]\tLoss: 0.0487\tLR: 0.000800\n",
            "Training Epoch: 198 [27264/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 198 [27392/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 198 [27520/50000]\tLoss: 0.0406\tLR: 0.000800\n",
            "Training Epoch: 198 [27648/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 198 [27776/50000]\tLoss: 0.0340\tLR: 0.000800\n",
            "Training Epoch: 198 [27904/50000]\tLoss: 0.0375\tLR: 0.000800\n",
            "Training Epoch: 198 [28032/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 198 [28160/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 198 [28288/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 198 [28416/50000]\tLoss: 0.0648\tLR: 0.000800\n",
            "Training Epoch: 198 [28544/50000]\tLoss: 0.0165\tLR: 0.000800\n",
            "Training Epoch: 198 [28672/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 198 [28800/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 198 [28928/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 198 [29056/50000]\tLoss: 0.0352\tLR: 0.000800\n",
            "Training Epoch: 198 [29184/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 198 [29312/50000]\tLoss: 0.0358\tLR: 0.000800\n",
            "Training Epoch: 198 [29440/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 198 [29568/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 198 [29696/50000]\tLoss: 0.0233\tLR: 0.000800\n",
            "Training Epoch: 198 [29824/50000]\tLoss: 0.0309\tLR: 0.000800\n",
            "Training Epoch: 198 [29952/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 198 [30080/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 198 [30208/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 198 [30336/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 198 [30464/50000]\tLoss: 0.0444\tLR: 0.000800\n",
            "Training Epoch: 198 [30592/50000]\tLoss: 0.0239\tLR: 0.000800\n",
            "Training Epoch: 198 [30720/50000]\tLoss: 0.0270\tLR: 0.000800\n",
            "Training Epoch: 198 [30848/50000]\tLoss: 0.0191\tLR: 0.000800\n",
            "Training Epoch: 198 [30976/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 198 [31104/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 198 [31232/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 198 [31360/50000]\tLoss: 0.0142\tLR: 0.000800\n",
            "Training Epoch: 198 [31488/50000]\tLoss: 0.0250\tLR: 0.000800\n",
            "Training Epoch: 198 [31616/50000]\tLoss: 0.0223\tLR: 0.000800\n",
            "Training Epoch: 198 [31744/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 198 [31872/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 198 [32000/50000]\tLoss: 0.0507\tLR: 0.000800\n",
            "Training Epoch: 198 [32128/50000]\tLoss: 0.0364\tLR: 0.000800\n",
            "Training Epoch: 198 [32256/50000]\tLoss: 0.0273\tLR: 0.000800\n",
            "Training Epoch: 198 [32384/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 198 [32512/50000]\tLoss: 0.0211\tLR: 0.000800\n",
            "Training Epoch: 198 [32640/50000]\tLoss: 0.0166\tLR: 0.000800\n",
            "Training Epoch: 198 [32768/50000]\tLoss: 0.0020\tLR: 0.000800\n",
            "Training Epoch: 198 [32896/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 198 [33024/50000]\tLoss: 0.0323\tLR: 0.000800\n",
            "Training Epoch: 198 [33152/50000]\tLoss: 0.0437\tLR: 0.000800\n",
            "Training Epoch: 198 [33280/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 198 [33408/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 198 [33536/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 198 [33664/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 198 [33792/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 198 [33920/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 198 [34048/50000]\tLoss: 0.0547\tLR: 0.000800\n",
            "Training Epoch: 198 [34176/50000]\tLoss: 0.0251\tLR: 0.000800\n",
            "Training Epoch: 198 [34304/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 198 [34432/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 198 [34560/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 198 [34688/50000]\tLoss: 0.0499\tLR: 0.000800\n",
            "Training Epoch: 198 [34816/50000]\tLoss: 0.0229\tLR: 0.000800\n",
            "Training Epoch: 198 [34944/50000]\tLoss: 0.0478\tLR: 0.000800\n",
            "Training Epoch: 198 [35072/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 198 [35200/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 198 [35328/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 198 [35456/50000]\tLoss: 0.0374\tLR: 0.000800\n",
            "Training Epoch: 198 [35584/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 198 [35712/50000]\tLoss: 0.0128\tLR: 0.000800\n",
            "Training Epoch: 198 [35840/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 198 [35968/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 198 [36096/50000]\tLoss: 0.0187\tLR: 0.000800\n",
            "Training Epoch: 198 [36224/50000]\tLoss: 0.0545\tLR: 0.000800\n",
            "Training Epoch: 198 [36352/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 198 [36480/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 198 [36608/50000]\tLoss: 0.0460\tLR: 0.000800\n",
            "Training Epoch: 198 [36736/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 198 [36864/50000]\tLoss: 0.0337\tLR: 0.000800\n",
            "Training Epoch: 198 [36992/50000]\tLoss: 0.0221\tLR: 0.000800\n",
            "Training Epoch: 198 [37120/50000]\tLoss: 0.0163\tLR: 0.000800\n",
            "Training Epoch: 198 [37248/50000]\tLoss: 0.0570\tLR: 0.000800\n",
            "Training Epoch: 198 [37376/50000]\tLoss: 0.0204\tLR: 0.000800\n",
            "Training Epoch: 198 [37504/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 198 [37632/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 198 [37760/50000]\tLoss: 0.0227\tLR: 0.000800\n",
            "Training Epoch: 198 [37888/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 198 [38016/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 198 [38144/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 198 [38272/50000]\tLoss: 0.0734\tLR: 0.000800\n",
            "Training Epoch: 198 [38400/50000]\tLoss: 0.0518\tLR: 0.000800\n",
            "Training Epoch: 198 [38528/50000]\tLoss: 0.0426\tLR: 0.000800\n",
            "Training Epoch: 198 [38656/50000]\tLoss: 0.0189\tLR: 0.000800\n",
            "Training Epoch: 198 [38784/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 198 [38912/50000]\tLoss: 0.0228\tLR: 0.000800\n",
            "Training Epoch: 198 [39040/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 198 [39168/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 198 [39296/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 198 [39424/50000]\tLoss: 0.0273\tLR: 0.000800\n",
            "Training Epoch: 198 [39552/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 198 [39680/50000]\tLoss: 0.0187\tLR: 0.000800\n",
            "Training Epoch: 198 [39808/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 198 [39936/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 198 [40064/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 198 [40192/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 198 [40320/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 198 [40448/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 198 [40576/50000]\tLoss: 0.0238\tLR: 0.000800\n",
            "Training Epoch: 198 [40704/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 198 [40832/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 198 [40960/50000]\tLoss: 0.0502\tLR: 0.000800\n",
            "Training Epoch: 198 [41088/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 198 [41216/50000]\tLoss: 0.0220\tLR: 0.000800\n",
            "Training Epoch: 198 [41344/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 198 [41472/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 198 [41600/50000]\tLoss: 0.0165\tLR: 0.000800\n",
            "Training Epoch: 198 [41728/50000]\tLoss: 0.0374\tLR: 0.000800\n",
            "Training Epoch: 198 [41856/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 198 [41984/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 198 [42112/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 198 [42240/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 198 [42368/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 198 [42496/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 198 [42624/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 198 [42752/50000]\tLoss: 0.0197\tLR: 0.000800\n",
            "Training Epoch: 198 [42880/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 198 [43008/50000]\tLoss: 0.0302\tLR: 0.000800\n",
            "Training Epoch: 198 [43136/50000]\tLoss: 0.0016\tLR: 0.000800\n",
            "Training Epoch: 198 [43264/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 198 [43392/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 198 [43520/50000]\tLoss: 0.0690\tLR: 0.000800\n",
            "Training Epoch: 198 [43648/50000]\tLoss: 0.0469\tLR: 0.000800\n",
            "Training Epoch: 198 [43776/50000]\tLoss: 0.0193\tLR: 0.000800\n",
            "Training Epoch: 198 [43904/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 198 [44032/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 198 [44160/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 198 [44288/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 198 [44416/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 198 [44544/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 198 [44672/50000]\tLoss: 0.0283\tLR: 0.000800\n",
            "Training Epoch: 198 [44800/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 198 [44928/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 198 [45056/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 198 [45184/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 198 [45312/50000]\tLoss: 0.0412\tLR: 0.000800\n",
            "Training Epoch: 198 [45440/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 198 [45568/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 198 [45696/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 198 [45824/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 198 [45952/50000]\tLoss: 0.0244\tLR: 0.000800\n",
            "Training Epoch: 198 [46080/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 198 [46208/50000]\tLoss: 0.0879\tLR: 0.000800\n",
            "Training Epoch: 198 [46336/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 198 [46464/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 198 [46592/50000]\tLoss: 0.0287\tLR: 0.000800\n",
            "Training Epoch: 198 [46720/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 198 [46848/50000]\tLoss: 0.0295\tLR: 0.000800\n",
            "Training Epoch: 198 [46976/50000]\tLoss: 0.0225\tLR: 0.000800\n",
            "Training Epoch: 198 [47104/50000]\tLoss: 0.0211\tLR: 0.000800\n",
            "Training Epoch: 198 [47232/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 198 [47360/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 198 [47488/50000]\tLoss: 0.0266\tLR: 0.000800\n",
            "Training Epoch: 198 [47616/50000]\tLoss: 0.0356\tLR: 0.000800\n",
            "Training Epoch: 198 [47744/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 198 [47872/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 198 [48000/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 198 [48128/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 198 [48256/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 198 [48384/50000]\tLoss: 0.0207\tLR: 0.000800\n",
            "Training Epoch: 198 [48512/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 198 [48640/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 198 [48768/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 198 [48896/50000]\tLoss: 0.0382\tLR: 0.000800\n",
            "Training Epoch: 198 [49024/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 198 [49152/50000]\tLoss: 0.0225\tLR: 0.000800\n",
            "Training Epoch: 198 [49280/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 198 [49408/50000]\tLoss: 0.0218\tLR: 0.000800\n",
            "Training Epoch: 198 [49536/50000]\tLoss: 0.0227\tLR: 0.000800\n",
            "Training Epoch: 198 [49664/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 198 [49792/50000]\tLoss: 0.0333\tLR: 0.000800\n",
            "Training Epoch: 198 [49920/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 198 [50000/50000]\tLoss: 0.0292\tLR: 0.000800\n",
            "epoch 198 training time consumed: 33.39s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 220598 GiB | 220597 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 218587 GiB | 218587 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   2010 GiB |   2010 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 220598 GiB | 220597 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 218587 GiB | 218587 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   2010 GiB |   2010 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 219917 GiB | 219916 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 217907 GiB | 217907 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   2009 GiB |   2009 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 195117 GiB | 195117 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 193106 GiB | 193106 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   2010 GiB |   2010 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   24164 K  |   24164 K  |\n",
            "|       from large pool |      38    |      61    |   10561 K  |   10560 K  |\n",
            "|       from small pool |     187    |     230    |   13603 K  |   13603 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   24164 K  |   24164 K  |\n",
            "|       from large pool |      38    |      61    |   10561 K  |   10560 K  |\n",
            "|       from small pool |     187    |     230    |   13603 K  |   13603 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |   10020 K  |   10020 K  |\n",
            "|       from large pool |      13    |      15    |    4800 K  |    4800 K  |\n",
            "|       from small pool |       7    |      14    |    5219 K  |    5219 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 198, Average loss: 0.0129, Accuracy: 0.7185, Time consumed:2.96s\n",
            "\n",
            "Training Epoch: 199 [128/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 199 [256/50000]\tLoss: 0.0342\tLR: 0.000800\n",
            "Training Epoch: 199 [384/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 199 [512/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 199 [640/50000]\tLoss: 0.0167\tLR: 0.000800\n",
            "Training Epoch: 199 [768/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 199 [896/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 199 [1024/50000]\tLoss: 0.0377\tLR: 0.000800\n",
            "Training Epoch: 199 [1152/50000]\tLoss: 0.0369\tLR: 0.000800\n",
            "Training Epoch: 199 [1280/50000]\tLoss: 0.0258\tLR: 0.000800\n",
            "Training Epoch: 199 [1408/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 199 [1536/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 199 [1664/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 199 [1792/50000]\tLoss: 0.0242\tLR: 0.000800\n",
            "Training Epoch: 199 [1920/50000]\tLoss: 0.0022\tLR: 0.000800\n",
            "Training Epoch: 199 [2048/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 199 [2176/50000]\tLoss: 0.0555\tLR: 0.000800\n",
            "Training Epoch: 199 [2304/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 199 [2432/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 199 [2560/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 199 [2688/50000]\tLoss: 0.0243\tLR: 0.000800\n",
            "Training Epoch: 199 [2816/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 199 [2944/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 199 [3072/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 199 [3200/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 199 [3328/50000]\tLoss: 0.0322\tLR: 0.000800\n",
            "Training Epoch: 199 [3456/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 199 [3584/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 199 [3712/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 199 [3840/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 199 [3968/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 199 [4096/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 199 [4224/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 199 [4352/50000]\tLoss: 0.0467\tLR: 0.000800\n",
            "Training Epoch: 199 [4480/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 199 [4608/50000]\tLoss: 0.0518\tLR: 0.000800\n",
            "Training Epoch: 199 [4736/50000]\tLoss: 0.0279\tLR: 0.000800\n",
            "Training Epoch: 199 [4864/50000]\tLoss: 0.0214\tLR: 0.000800\n",
            "Training Epoch: 199 [4992/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 199 [5120/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 199 [5248/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 199 [5376/50000]\tLoss: 0.0273\tLR: 0.000800\n",
            "Training Epoch: 199 [5504/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 199 [5632/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 199 [5760/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 199 [5888/50000]\tLoss: 0.0022\tLR: 0.000800\n",
            "Training Epoch: 199 [6016/50000]\tLoss: 0.0441\tLR: 0.000800\n",
            "Training Epoch: 199 [6144/50000]\tLoss: 0.0172\tLR: 0.000800\n",
            "Training Epoch: 199 [6272/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 199 [6400/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 199 [6528/50000]\tLoss: 0.0477\tLR: 0.000800\n",
            "Training Epoch: 199 [6656/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 199 [6784/50000]\tLoss: 0.0624\tLR: 0.000800\n",
            "Training Epoch: 199 [6912/50000]\tLoss: 0.0167\tLR: 0.000800\n",
            "Training Epoch: 199 [7040/50000]\tLoss: 0.0494\tLR: 0.000800\n",
            "Training Epoch: 199 [7168/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 199 [7296/50000]\tLoss: 0.0212\tLR: 0.000800\n",
            "Training Epoch: 199 [7424/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 199 [7552/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 199 [7680/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 199 [7808/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 199 [7936/50000]\tLoss: 0.0285\tLR: 0.000800\n",
            "Training Epoch: 199 [8064/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 199 [8192/50000]\tLoss: 0.0344\tLR: 0.000800\n",
            "Training Epoch: 199 [8320/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 199 [8448/50000]\tLoss: 0.0065\tLR: 0.000800\n",
            "Training Epoch: 199 [8576/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 199 [8704/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 199 [8832/50000]\tLoss: 0.0181\tLR: 0.000800\n",
            "Training Epoch: 199 [8960/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 199 [9088/50000]\tLoss: 0.0214\tLR: 0.000800\n",
            "Training Epoch: 199 [9216/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 199 [9344/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 199 [9472/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 199 [9600/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 199 [9728/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 199 [9856/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 199 [9984/50000]\tLoss: 0.0380\tLR: 0.000800\n",
            "Training Epoch: 199 [10112/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 199 [10240/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 199 [10368/50000]\tLoss: 0.0302\tLR: 0.000800\n",
            "Training Epoch: 199 [10496/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 199 [10624/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 199 [10752/50000]\tLoss: 0.0117\tLR: 0.000800\n",
            "Training Epoch: 199 [10880/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 199 [11008/50000]\tLoss: 0.0018\tLR: 0.000800\n",
            "Training Epoch: 199 [11136/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 199 [11264/50000]\tLoss: 0.0310\tLR: 0.000800\n",
            "Training Epoch: 199 [11392/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 199 [11520/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 199 [11648/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 199 [11776/50000]\tLoss: 0.0221\tLR: 0.000800\n",
            "Training Epoch: 199 [11904/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 199 [12032/50000]\tLoss: 0.0015\tLR: 0.000800\n",
            "Training Epoch: 199 [12160/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 199 [12288/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 199 [12416/50000]\tLoss: 0.0538\tLR: 0.000800\n",
            "Training Epoch: 199 [12544/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 199 [12672/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 199 [12800/50000]\tLoss: 0.0461\tLR: 0.000800\n",
            "Training Epoch: 199 [12928/50000]\tLoss: 0.0322\tLR: 0.000800\n",
            "Training Epoch: 199 [13056/50000]\tLoss: 0.0096\tLR: 0.000800\n",
            "Training Epoch: 199 [13184/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 199 [13312/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 199 [13440/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 199 [13568/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 199 [13696/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 199 [13824/50000]\tLoss: 0.0262\tLR: 0.000800\n",
            "Training Epoch: 199 [13952/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 199 [14080/50000]\tLoss: 0.0234\tLR: 0.000800\n",
            "Training Epoch: 199 [14208/50000]\tLoss: 0.0340\tLR: 0.000800\n",
            "Training Epoch: 199 [14336/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 199 [14464/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 199 [14592/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 199 [14720/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 199 [14848/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 199 [14976/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 199 [15104/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 199 [15232/50000]\tLoss: 0.0212\tLR: 0.000800\n",
            "Training Epoch: 199 [15360/50000]\tLoss: 0.0477\tLR: 0.000800\n",
            "Training Epoch: 199 [15488/50000]\tLoss: 0.0029\tLR: 0.000800\n",
            "Training Epoch: 199 [15616/50000]\tLoss: 0.0193\tLR: 0.000800\n",
            "Training Epoch: 199 [15744/50000]\tLoss: 0.0462\tLR: 0.000800\n",
            "Training Epoch: 199 [15872/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 199 [16000/50000]\tLoss: 0.0411\tLR: 0.000800\n",
            "Training Epoch: 199 [16128/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 199 [16256/50000]\tLoss: 0.0395\tLR: 0.000800\n",
            "Training Epoch: 199 [16384/50000]\tLoss: 0.0119\tLR: 0.000800\n",
            "Training Epoch: 199 [16512/50000]\tLoss: 0.0371\tLR: 0.000800\n",
            "Training Epoch: 199 [16640/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 199 [16768/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 199 [16896/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 199 [17024/50000]\tLoss: 0.0443\tLR: 0.000800\n",
            "Training Epoch: 199 [17152/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 199 [17280/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 199 [17408/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 199 [17536/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 199 [17664/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 199 [17792/50000]\tLoss: 0.0298\tLR: 0.000800\n",
            "Training Epoch: 199 [17920/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 199 [18048/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 199 [18176/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 199 [18304/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 199 [18432/50000]\tLoss: 0.0244\tLR: 0.000800\n",
            "Training Epoch: 199 [18560/50000]\tLoss: 0.0147\tLR: 0.000800\n",
            "Training Epoch: 199 [18688/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 199 [18816/50000]\tLoss: 0.0288\tLR: 0.000800\n",
            "Training Epoch: 199 [18944/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 199 [19072/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 199 [19200/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 199 [19328/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 199 [19456/50000]\tLoss: 0.0025\tLR: 0.000800\n",
            "Training Epoch: 199 [19584/50000]\tLoss: 0.0174\tLR: 0.000800\n",
            "Training Epoch: 199 [19712/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 199 [19840/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 199 [19968/50000]\tLoss: 0.0184\tLR: 0.000800\n",
            "Training Epoch: 199 [20096/50000]\tLoss: 0.0224\tLR: 0.000800\n",
            "Training Epoch: 199 [20224/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 199 [20352/50000]\tLoss: 0.0597\tLR: 0.000800\n",
            "Training Epoch: 199 [20480/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 199 [20608/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 199 [20736/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 199 [20864/50000]\tLoss: 0.0146\tLR: 0.000800\n",
            "Training Epoch: 199 [20992/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 199 [21120/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 199 [21248/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 199 [21376/50000]\tLoss: 0.0172\tLR: 0.000800\n",
            "Training Epoch: 199 [21504/50000]\tLoss: 0.0195\tLR: 0.000800\n",
            "Training Epoch: 199 [21632/50000]\tLoss: 0.0096\tLR: 0.000800\n",
            "Training Epoch: 199 [21760/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 199 [21888/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 199 [22016/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 199 [22144/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 199 [22272/50000]\tLoss: 0.0069\tLR: 0.000800\n",
            "Training Epoch: 199 [22400/50000]\tLoss: 0.0258\tLR: 0.000800\n",
            "Training Epoch: 199 [22528/50000]\tLoss: 0.0399\tLR: 0.000800\n",
            "Training Epoch: 199 [22656/50000]\tLoss: 0.0291\tLR: 0.000800\n",
            "Training Epoch: 199 [22784/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 199 [22912/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 199 [23040/50000]\tLoss: 0.0281\tLR: 0.000800\n",
            "Training Epoch: 199 [23168/50000]\tLoss: 0.0631\tLR: 0.000800\n",
            "Training Epoch: 199 [23296/50000]\tLoss: 0.0485\tLR: 0.000800\n",
            "Training Epoch: 199 [23424/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 199 [23552/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 199 [23680/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 199 [23808/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 199 [23936/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 199 [24064/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 199 [24192/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 199 [24320/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 199 [24448/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 199 [24576/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 199 [24704/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 199 [24832/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 199 [24960/50000]\tLoss: 0.0307\tLR: 0.000800\n",
            "Training Epoch: 199 [25088/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 199 [25216/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 199 [25344/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 199 [25472/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 199 [25600/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 199 [25728/50000]\tLoss: 0.0221\tLR: 0.000800\n",
            "Training Epoch: 199 [25856/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 199 [25984/50000]\tLoss: 0.0284\tLR: 0.000800\n",
            "Training Epoch: 199 [26112/50000]\tLoss: 0.0033\tLR: 0.000800\n",
            "Training Epoch: 199 [26240/50000]\tLoss: 0.0022\tLR: 0.000800\n",
            "Training Epoch: 199 [26368/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 199 [26496/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 199 [26624/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 199 [26752/50000]\tLoss: 0.0272\tLR: 0.000800\n",
            "Training Epoch: 199 [26880/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 199 [27008/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 199 [27136/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 199 [27264/50000]\tLoss: 0.0319\tLR: 0.000800\n",
            "Training Epoch: 199 [27392/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 199 [27520/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 199 [27648/50000]\tLoss: 0.0879\tLR: 0.000800\n",
            "Training Epoch: 199 [27776/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 199 [27904/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 199 [28032/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 199 [28160/50000]\tLoss: 0.0612\tLR: 0.000800\n",
            "Training Epoch: 199 [28288/50000]\tLoss: 0.0136\tLR: 0.000800\n",
            "Training Epoch: 199 [28416/50000]\tLoss: 0.0280\tLR: 0.000800\n",
            "Training Epoch: 199 [28544/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 199 [28672/50000]\tLoss: 0.0094\tLR: 0.000800\n",
            "Training Epoch: 199 [28800/50000]\tLoss: 0.0192\tLR: 0.000800\n",
            "Training Epoch: 199 [28928/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 199 [29056/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 199 [29184/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 199 [29312/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 199 [29440/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 199 [29568/50000]\tLoss: 0.0471\tLR: 0.000800\n",
            "Training Epoch: 199 [29696/50000]\tLoss: 0.0209\tLR: 0.000800\n",
            "Training Epoch: 199 [29824/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 199 [29952/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 199 [30080/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 199 [30208/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 199 [30336/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 199 [30464/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 199 [30592/50000]\tLoss: 0.0456\tLR: 0.000800\n",
            "Training Epoch: 199 [30720/50000]\tLoss: 0.0154\tLR: 0.000800\n",
            "Training Epoch: 199 [30848/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 199 [30976/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 199 [31104/50000]\tLoss: 0.0254\tLR: 0.000800\n",
            "Training Epoch: 199 [31232/50000]\tLoss: 0.0265\tLR: 0.000800\n",
            "Training Epoch: 199 [31360/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 199 [31488/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 199 [31616/50000]\tLoss: 0.0159\tLR: 0.000800\n",
            "Training Epoch: 199 [31744/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 199 [31872/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 199 [32000/50000]\tLoss: 0.0204\tLR: 0.000800\n",
            "Training Epoch: 199 [32128/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 199 [32256/50000]\tLoss: 0.0759\tLR: 0.000800\n",
            "Training Epoch: 199 [32384/50000]\tLoss: 0.0200\tLR: 0.000800\n",
            "Training Epoch: 199 [32512/50000]\tLoss: 0.0382\tLR: 0.000800\n",
            "Training Epoch: 199 [32640/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 199 [32768/50000]\tLoss: 0.0222\tLR: 0.000800\n",
            "Training Epoch: 199 [32896/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 199 [33024/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 199 [33152/50000]\tLoss: 0.0276\tLR: 0.000800\n",
            "Training Epoch: 199 [33280/50000]\tLoss: 0.0292\tLR: 0.000800\n",
            "Training Epoch: 199 [33408/50000]\tLoss: 0.0341\tLR: 0.000800\n",
            "Training Epoch: 199 [33536/50000]\tLoss: 0.0243\tLR: 0.000800\n",
            "Training Epoch: 199 [33664/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 199 [33792/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 199 [33920/50000]\tLoss: 0.0648\tLR: 0.000800\n",
            "Training Epoch: 199 [34048/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 199 [34176/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 199 [34304/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 199 [34432/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 199 [34560/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 199 [34688/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 199 [34816/50000]\tLoss: 0.0131\tLR: 0.000800\n",
            "Training Epoch: 199 [34944/50000]\tLoss: 0.0367\tLR: 0.000800\n",
            "Training Epoch: 199 [35072/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 199 [35200/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 199 [35328/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 199 [35456/50000]\tLoss: 0.0110\tLR: 0.000800\n",
            "Training Epoch: 199 [35584/50000]\tLoss: 0.0041\tLR: 0.000800\n",
            "Training Epoch: 199 [35712/50000]\tLoss: 0.0349\tLR: 0.000800\n",
            "Training Epoch: 199 [35840/50000]\tLoss: 0.0096\tLR: 0.000800\n",
            "Training Epoch: 199 [35968/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 199 [36096/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 199 [36224/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 199 [36352/50000]\tLoss: 0.0251\tLR: 0.000800\n",
            "Training Epoch: 199 [36480/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 199 [36608/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 199 [36736/50000]\tLoss: 0.0245\tLR: 0.000800\n",
            "Training Epoch: 199 [36864/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 199 [36992/50000]\tLoss: 0.0082\tLR: 0.000800\n",
            "Training Epoch: 199 [37120/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 199 [37248/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 199 [37376/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 199 [37504/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 199 [37632/50000]\tLoss: 0.0455\tLR: 0.000800\n",
            "Training Epoch: 199 [37760/50000]\tLoss: 0.0207\tLR: 0.000800\n",
            "Training Epoch: 199 [37888/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 199 [38016/50000]\tLoss: 0.0338\tLR: 0.000800\n",
            "Training Epoch: 199 [38144/50000]\tLoss: 0.0059\tLR: 0.000800\n",
            "Training Epoch: 199 [38272/50000]\tLoss: 0.0260\tLR: 0.000800\n",
            "Training Epoch: 199 [38400/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 199 [38528/50000]\tLoss: 0.0541\tLR: 0.000800\n",
            "Training Epoch: 199 [38656/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 199 [38784/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 199 [38912/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 199 [39040/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 199 [39168/50000]\tLoss: 0.0325\tLR: 0.000800\n",
            "Training Epoch: 199 [39296/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 199 [39424/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 199 [39552/50000]\tLoss: 0.0169\tLR: 0.000800\n",
            "Training Epoch: 199 [39680/50000]\tLoss: 0.0625\tLR: 0.000800\n",
            "Training Epoch: 199 [39808/50000]\tLoss: 0.0122\tLR: 0.000800\n",
            "Training Epoch: 199 [39936/50000]\tLoss: 0.0021\tLR: 0.000800\n",
            "Training Epoch: 199 [40064/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 199 [40192/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 199 [40320/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 199 [40448/50000]\tLoss: 0.0197\tLR: 0.000800\n",
            "Training Epoch: 199 [40576/50000]\tLoss: 0.0423\tLR: 0.000800\n",
            "Training Epoch: 199 [40704/50000]\tLoss: 0.0028\tLR: 0.000800\n",
            "Training Epoch: 199 [40832/50000]\tLoss: 0.0238\tLR: 0.000800\n",
            "Training Epoch: 199 [40960/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 199 [41088/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 199 [41216/50000]\tLoss: 0.0021\tLR: 0.000800\n",
            "Training Epoch: 199 [41344/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 199 [41472/50000]\tLoss: 0.0161\tLR: 0.000800\n",
            "Training Epoch: 199 [41600/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 199 [41728/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 199 [41856/50000]\tLoss: 0.0157\tLR: 0.000800\n",
            "Training Epoch: 199 [41984/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 199 [42112/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 199 [42240/50000]\tLoss: 0.0203\tLR: 0.000800\n",
            "Training Epoch: 199 [42368/50000]\tLoss: 0.0308\tLR: 0.000800\n",
            "Training Epoch: 199 [42496/50000]\tLoss: 0.0150\tLR: 0.000800\n",
            "Training Epoch: 199 [42624/50000]\tLoss: 0.0456\tLR: 0.000800\n",
            "Training Epoch: 199 [42752/50000]\tLoss: 0.0249\tLR: 0.000800\n",
            "Training Epoch: 199 [42880/50000]\tLoss: 0.0062\tLR: 0.000800\n",
            "Training Epoch: 199 [43008/50000]\tLoss: 0.0520\tLR: 0.000800\n",
            "Training Epoch: 199 [43136/50000]\tLoss: 0.0763\tLR: 0.000800\n",
            "Training Epoch: 199 [43264/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 199 [43392/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 199 [43520/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 199 [43648/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 199 [43776/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 199 [43904/50000]\tLoss: 0.0365\tLR: 0.000800\n",
            "Training Epoch: 199 [44032/50000]\tLoss: 0.0215\tLR: 0.000800\n",
            "Training Epoch: 199 [44160/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 199 [44288/50000]\tLoss: 0.0172\tLR: 0.000800\n",
            "Training Epoch: 199 [44416/50000]\tLoss: 0.0459\tLR: 0.000800\n",
            "Training Epoch: 199 [44544/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 199 [44672/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 199 [44800/50000]\tLoss: 0.0394\tLR: 0.000800\n",
            "Training Epoch: 199 [44928/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 199 [45056/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 199 [45184/50000]\tLoss: 0.0162\tLR: 0.000800\n",
            "Training Epoch: 199 [45312/50000]\tLoss: 0.0121\tLR: 0.000800\n",
            "Training Epoch: 199 [45440/50000]\tLoss: 0.0247\tLR: 0.000800\n",
            "Training Epoch: 199 [45568/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 199 [45696/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 199 [45824/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 199 [45952/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 199 [46080/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 199 [46208/50000]\tLoss: 0.0352\tLR: 0.000800\n",
            "Training Epoch: 199 [46336/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 199 [46464/50000]\tLoss: 0.0296\tLR: 0.000800\n",
            "Training Epoch: 199 [46592/50000]\tLoss: 0.0050\tLR: 0.000800\n",
            "Training Epoch: 199 [46720/50000]\tLoss: 0.0292\tLR: 0.000800\n",
            "Training Epoch: 199 [46848/50000]\tLoss: 0.0165\tLR: 0.000800\n",
            "Training Epoch: 199 [46976/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 199 [47104/50000]\tLoss: 0.0030\tLR: 0.000800\n",
            "Training Epoch: 199 [47232/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 199 [47360/50000]\tLoss: 0.0356\tLR: 0.000800\n",
            "Training Epoch: 199 [47488/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 199 [47616/50000]\tLoss: 0.0435\tLR: 0.000800\n",
            "Training Epoch: 199 [47744/50000]\tLoss: 0.0201\tLR: 0.000800\n",
            "Training Epoch: 199 [47872/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 199 [48000/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 199 [48128/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 199 [48256/50000]\tLoss: 0.0945\tLR: 0.000800\n",
            "Training Epoch: 199 [48384/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 199 [48512/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 199 [48640/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 199 [48768/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 199 [48896/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 199 [49024/50000]\tLoss: 0.0271\tLR: 0.000800\n",
            "Training Epoch: 199 [49152/50000]\tLoss: 0.0325\tLR: 0.000800\n",
            "Training Epoch: 199 [49280/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 199 [49408/50000]\tLoss: 0.0616\tLR: 0.000800\n",
            "Training Epoch: 199 [49536/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 199 [49664/50000]\tLoss: 0.0167\tLR: 0.000800\n",
            "Training Epoch: 199 [49792/50000]\tLoss: 0.0500\tLR: 0.000800\n",
            "Training Epoch: 199 [49920/50000]\tLoss: 0.0188\tLR: 0.000800\n",
            "Training Epoch: 199 [50000/50000]\tLoss: 0.0398\tLR: 0.000800\n",
            "epoch 199 training time consumed: 34.20s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 221712 GiB | 221712 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 219691 GiB | 219691 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   2020 GiB |   2020 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 221712 GiB | 221712 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 219691 GiB | 219691 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   2020 GiB |   2020 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 221027 GiB | 221027 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 219008 GiB | 219007 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   2019 GiB |   2019 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 196103 GiB | 196103 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 194082 GiB | 194081 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   2021 GiB |   2021 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   24286 K  |   24286 K  |\n",
            "|       from large pool |      38    |      61    |   10614 K  |   10614 K  |\n",
            "|       from small pool |     187    |     230    |   13672 K  |   13672 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   24286 K  |   24286 K  |\n",
            "|       from large pool |      38    |      61    |   10614 K  |   10614 K  |\n",
            "|       from small pool |     187    |     230    |   13672 K  |   13672 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |   10071 K  |   10071 K  |\n",
            "|       from large pool |      13    |      15    |    4824 K  |    4824 K  |\n",
            "|       from small pool |       7    |      14    |    5246 K  |    5246 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 199, Average loss: 0.0129, Accuracy: 0.7202, Time consumed:3.52s\n",
            "\n",
            "Training Epoch: 200 [128/50000]\tLoss: 0.0167\tLR: 0.000800\n",
            "Training Epoch: 200 [256/50000]\tLoss: 0.0352\tLR: 0.000800\n",
            "Training Epoch: 200 [384/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 200 [512/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 200 [640/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 200 [768/50000]\tLoss: 0.0272\tLR: 0.000800\n",
            "Training Epoch: 200 [896/50000]\tLoss: 0.0168\tLR: 0.000800\n",
            "Training Epoch: 200 [1024/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 200 [1152/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 200 [1280/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 200 [1408/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 200 [1536/50000]\tLoss: 0.0258\tLR: 0.000800\n",
            "Training Epoch: 200 [1664/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 200 [1792/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 200 [1920/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 200 [2048/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 200 [2176/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 200 [2304/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 200 [2432/50000]\tLoss: 0.0264\tLR: 0.000800\n",
            "Training Epoch: 200 [2560/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 200 [2688/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 200 [2816/50000]\tLoss: 0.0314\tLR: 0.000800\n",
            "Training Epoch: 200 [2944/50000]\tLoss: 0.0314\tLR: 0.000800\n",
            "Training Epoch: 200 [3072/50000]\tLoss: 0.0283\tLR: 0.000800\n",
            "Training Epoch: 200 [3200/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 200 [3328/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 200 [3456/50000]\tLoss: 0.0326\tLR: 0.000800\n",
            "Training Epoch: 200 [3584/50000]\tLoss: 0.0354\tLR: 0.000800\n",
            "Training Epoch: 200 [3712/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 200 [3840/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 200 [3968/50000]\tLoss: 0.0032\tLR: 0.000800\n",
            "Training Epoch: 200 [4096/50000]\tLoss: 0.0274\tLR: 0.000800\n",
            "Training Epoch: 200 [4224/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 200 [4352/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 200 [4480/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 200 [4608/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 200 [4736/50000]\tLoss: 0.0024\tLR: 0.000800\n",
            "Training Epoch: 200 [4864/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 200 [4992/50000]\tLoss: 0.0078\tLR: 0.000800\n",
            "Training Epoch: 200 [5120/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 200 [5248/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 200 [5376/50000]\tLoss: 0.0232\tLR: 0.000800\n",
            "Training Epoch: 200 [5504/50000]\tLoss: 0.0164\tLR: 0.000800\n",
            "Training Epoch: 200 [5632/50000]\tLoss: 0.0210\tLR: 0.000800\n",
            "Training Epoch: 200 [5760/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 200 [5888/50000]\tLoss: 0.0093\tLR: 0.000800\n",
            "Training Epoch: 200 [6016/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 200 [6144/50000]\tLoss: 0.0135\tLR: 0.000800\n",
            "Training Epoch: 200 [6272/50000]\tLoss: 0.0096\tLR: 0.000800\n",
            "Training Epoch: 200 [6400/50000]\tLoss: 0.0130\tLR: 0.000800\n",
            "Training Epoch: 200 [6528/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 200 [6656/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 200 [6784/50000]\tLoss: 0.0220\tLR: 0.000800\n",
            "Training Epoch: 200 [6912/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 200 [7040/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 200 [7168/50000]\tLoss: 0.0063\tLR: 0.000800\n",
            "Training Epoch: 200 [7296/50000]\tLoss: 0.0244\tLR: 0.000800\n",
            "Training Epoch: 200 [7424/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 200 [7552/50000]\tLoss: 0.0313\tLR: 0.000800\n",
            "Training Epoch: 200 [7680/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 200 [7808/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 200 [7936/50000]\tLoss: 0.0226\tLR: 0.000800\n",
            "Training Epoch: 200 [8064/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 200 [8192/50000]\tLoss: 0.0091\tLR: 0.000800\n",
            "Training Epoch: 200 [8320/50000]\tLoss: 0.0203\tLR: 0.000800\n",
            "Training Epoch: 200 [8448/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 200 [8576/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 200 [8704/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 200 [8832/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 200 [8960/50000]\tLoss: 0.0241\tLR: 0.000800\n",
            "Training Epoch: 200 [9088/50000]\tLoss: 0.0180\tLR: 0.000800\n",
            "Training Epoch: 200 [9216/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 200 [9344/50000]\tLoss: 0.0218\tLR: 0.000800\n",
            "Training Epoch: 200 [9472/50000]\tLoss: 0.0179\tLR: 0.000800\n",
            "Training Epoch: 200 [9600/50000]\tLoss: 0.0127\tLR: 0.000800\n",
            "Training Epoch: 200 [9728/50000]\tLoss: 0.0194\tLR: 0.000800\n",
            "Training Epoch: 200 [9856/50000]\tLoss: 0.0318\tLR: 0.000800\n",
            "Training Epoch: 200 [9984/50000]\tLoss: 0.0306\tLR: 0.000800\n",
            "Training Epoch: 200 [10112/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 200 [10240/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 200 [10368/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 200 [10496/50000]\tLoss: 0.0343\tLR: 0.000800\n",
            "Training Epoch: 200 [10624/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 200 [10752/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 200 [10880/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 200 [11008/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 200 [11136/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 200 [11264/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 200 [11392/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 200 [11520/50000]\tLoss: 0.0286\tLR: 0.000800\n",
            "Training Epoch: 200 [11648/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 200 [11776/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 200 [11904/50000]\tLoss: 0.0236\tLR: 0.000800\n",
            "Training Epoch: 200 [12032/50000]\tLoss: 0.0192\tLR: 0.000800\n",
            "Training Epoch: 200 [12160/50000]\tLoss: 0.0338\tLR: 0.000800\n",
            "Training Epoch: 200 [12288/50000]\tLoss: 0.0241\tLR: 0.000800\n",
            "Training Epoch: 200 [12416/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 200 [12544/50000]\tLoss: 0.0855\tLR: 0.000800\n",
            "Training Epoch: 200 [12672/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 200 [12800/50000]\tLoss: 0.0375\tLR: 0.000800\n",
            "Training Epoch: 200 [12928/50000]\tLoss: 0.0263\tLR: 0.000800\n",
            "Training Epoch: 200 [13056/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 200 [13184/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 200 [13312/50000]\tLoss: 0.0423\tLR: 0.000800\n",
            "Training Epoch: 200 [13440/50000]\tLoss: 0.0253\tLR: 0.000800\n",
            "Training Epoch: 200 [13568/50000]\tLoss: 0.0196\tLR: 0.000800\n",
            "Training Epoch: 200 [13696/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 200 [13824/50000]\tLoss: 0.0320\tLR: 0.000800\n",
            "Training Epoch: 200 [13952/50000]\tLoss: 0.0230\tLR: 0.000800\n",
            "Training Epoch: 200 [14080/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 200 [14208/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 200 [14336/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 200 [14464/50000]\tLoss: 0.0392\tLR: 0.000800\n",
            "Training Epoch: 200 [14592/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 200 [14720/50000]\tLoss: 0.0318\tLR: 0.000800\n",
            "Training Epoch: 200 [14848/50000]\tLoss: 0.0038\tLR: 0.000800\n",
            "Training Epoch: 200 [14976/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 200 [15104/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 200 [15232/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 200 [15360/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 200 [15488/50000]\tLoss: 0.0027\tLR: 0.000800\n",
            "Training Epoch: 200 [15616/50000]\tLoss: 0.0209\tLR: 0.000800\n",
            "Training Epoch: 200 [15744/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 200 [15872/50000]\tLoss: 0.0326\tLR: 0.000800\n",
            "Training Epoch: 200 [16000/50000]\tLoss: 0.0141\tLR: 0.000800\n",
            "Training Epoch: 200 [16128/50000]\tLoss: 0.0175\tLR: 0.000800\n",
            "Training Epoch: 200 [16256/50000]\tLoss: 0.0258\tLR: 0.000800\n",
            "Training Epoch: 200 [16384/50000]\tLoss: 0.0427\tLR: 0.000800\n",
            "Training Epoch: 200 [16512/50000]\tLoss: 0.0302\tLR: 0.000800\n",
            "Training Epoch: 200 [16640/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 200 [16768/50000]\tLoss: 0.0160\tLR: 0.000800\n",
            "Training Epoch: 200 [16896/50000]\tLoss: 0.0085\tLR: 0.000800\n",
            "Training Epoch: 200 [17024/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 200 [17152/50000]\tLoss: 0.0153\tLR: 0.000800\n",
            "Training Epoch: 200 [17280/50000]\tLoss: 0.0111\tLR: 0.000800\n",
            "Training Epoch: 200 [17408/50000]\tLoss: 0.0223\tLR: 0.000800\n",
            "Training Epoch: 200 [17536/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 200 [17664/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 200 [17792/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 200 [17920/50000]\tLoss: 0.0043\tLR: 0.000800\n",
            "Training Epoch: 200 [18048/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 200 [18176/50000]\tLoss: 0.0183\tLR: 0.000800\n",
            "Training Epoch: 200 [18304/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 200 [18432/50000]\tLoss: 0.0387\tLR: 0.000800\n",
            "Training Epoch: 200 [18560/50000]\tLoss: 0.0273\tLR: 0.000800\n",
            "Training Epoch: 200 [18688/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 200 [18816/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 200 [18944/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 200 [19072/50000]\tLoss: 0.1006\tLR: 0.000800\n",
            "Training Epoch: 200 [19200/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 200 [19328/50000]\tLoss: 0.0054\tLR: 0.000800\n",
            "Training Epoch: 200 [19456/50000]\tLoss: 0.0315\tLR: 0.000800\n",
            "Training Epoch: 200 [19584/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 200 [19712/50000]\tLoss: 0.0208\tLR: 0.000800\n",
            "Training Epoch: 200 [19840/50000]\tLoss: 0.0266\tLR: 0.000800\n",
            "Training Epoch: 200 [19968/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 200 [20096/50000]\tLoss: 0.0097\tLR: 0.000800\n",
            "Training Epoch: 200 [20224/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 200 [20352/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 200 [20480/50000]\tLoss: 0.0140\tLR: 0.000800\n",
            "Training Epoch: 200 [20608/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 200 [20736/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 200 [20864/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 200 [20992/50000]\tLoss: 0.0262\tLR: 0.000800\n",
            "Training Epoch: 200 [21120/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 200 [21248/50000]\tLoss: 0.0215\tLR: 0.000800\n",
            "Training Epoch: 200 [21376/50000]\tLoss: 0.0368\tLR: 0.000800\n",
            "Training Epoch: 200 [21504/50000]\tLoss: 0.0049\tLR: 0.000800\n",
            "Training Epoch: 200 [21632/50000]\tLoss: 0.0221\tLR: 0.000800\n",
            "Training Epoch: 200 [21760/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 200 [21888/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 200 [22016/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 200 [22144/50000]\tLoss: 0.0700\tLR: 0.000800\n",
            "Training Epoch: 200 [22272/50000]\tLoss: 0.0207\tLR: 0.000800\n",
            "Training Epoch: 200 [22400/50000]\tLoss: 0.0617\tLR: 0.000800\n",
            "Training Epoch: 200 [22528/50000]\tLoss: 0.0448\tLR: 0.000800\n",
            "Training Epoch: 200 [22656/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 200 [22784/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 200 [22912/50000]\tLoss: 0.0101\tLR: 0.000800\n",
            "Training Epoch: 200 [23040/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 200 [23168/50000]\tLoss: 0.0132\tLR: 0.000800\n",
            "Training Epoch: 200 [23296/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 200 [23424/50000]\tLoss: 0.0034\tLR: 0.000800\n",
            "Training Epoch: 200 [23552/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 200 [23680/50000]\tLoss: 0.0022\tLR: 0.000800\n",
            "Training Epoch: 200 [23808/50000]\tLoss: 0.0113\tLR: 0.000800\n",
            "Training Epoch: 200 [23936/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 200 [24064/50000]\tLoss: 0.0087\tLR: 0.000800\n",
            "Training Epoch: 200 [24192/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 200 [24320/50000]\tLoss: 0.0037\tLR: 0.000800\n",
            "Training Epoch: 200 [24448/50000]\tLoss: 0.0058\tLR: 0.000800\n",
            "Training Epoch: 200 [24576/50000]\tLoss: 0.0186\tLR: 0.000800\n",
            "Training Epoch: 200 [24704/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 200 [24832/50000]\tLoss: 0.0106\tLR: 0.000800\n",
            "Training Epoch: 200 [24960/50000]\tLoss: 0.0324\tLR: 0.000800\n",
            "Training Epoch: 200 [25088/50000]\tLoss: 0.0426\tLR: 0.000800\n",
            "Training Epoch: 200 [25216/50000]\tLoss: 0.0064\tLR: 0.000800\n",
            "Training Epoch: 200 [25344/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 200 [25472/50000]\tLoss: 0.0102\tLR: 0.000800\n",
            "Training Epoch: 200 [25600/50000]\tLoss: 0.0234\tLR: 0.000800\n",
            "Training Epoch: 200 [25728/50000]\tLoss: 0.0463\tLR: 0.000800\n",
            "Training Epoch: 200 [25856/50000]\tLoss: 0.0144\tLR: 0.000800\n",
            "Training Epoch: 200 [25984/50000]\tLoss: 0.0148\tLR: 0.000800\n",
            "Training Epoch: 200 [26112/50000]\tLoss: 0.0253\tLR: 0.000800\n",
            "Training Epoch: 200 [26240/50000]\tLoss: 0.0060\tLR: 0.000800\n",
            "Training Epoch: 200 [26368/50000]\tLoss: 0.0114\tLR: 0.000800\n",
            "Training Epoch: 200 [26496/50000]\tLoss: 0.0167\tLR: 0.000800\n",
            "Training Epoch: 200 [26624/50000]\tLoss: 0.0341\tLR: 0.000800\n",
            "Training Epoch: 200 [26752/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 200 [26880/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 200 [27008/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 200 [27136/50000]\tLoss: 0.0275\tLR: 0.000800\n",
            "Training Epoch: 200 [27264/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 200 [27392/50000]\tLoss: 0.0428\tLR: 0.000800\n",
            "Training Epoch: 200 [27520/50000]\tLoss: 0.0266\tLR: 0.000800\n",
            "Training Epoch: 200 [27648/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 200 [27776/50000]\tLoss: 0.0292\tLR: 0.000800\n",
            "Training Epoch: 200 [27904/50000]\tLoss: 0.0382\tLR: 0.000800\n",
            "Training Epoch: 200 [28032/50000]\tLoss: 0.0073\tLR: 0.000800\n",
            "Training Epoch: 200 [28160/50000]\tLoss: 0.0464\tLR: 0.000800\n",
            "Training Epoch: 200 [28288/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 200 [28416/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 200 [28544/50000]\tLoss: 0.0134\tLR: 0.000800\n",
            "Training Epoch: 200 [28672/50000]\tLoss: 0.0405\tLR: 0.000800\n",
            "Training Epoch: 200 [28800/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 200 [28928/50000]\tLoss: 0.0139\tLR: 0.000800\n",
            "Training Epoch: 200 [29056/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 200 [29184/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 200 [29312/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 200 [29440/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 200 [29568/50000]\tLoss: 0.0115\tLR: 0.000800\n",
            "Training Epoch: 200 [29696/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 200 [29824/50000]\tLoss: 0.0323\tLR: 0.000800\n",
            "Training Epoch: 200 [29952/50000]\tLoss: 0.0118\tLR: 0.000800\n",
            "Training Epoch: 200 [30080/50000]\tLoss: 0.0609\tLR: 0.000800\n",
            "Training Epoch: 200 [30208/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 200 [30336/50000]\tLoss: 0.0053\tLR: 0.000800\n",
            "Training Epoch: 200 [30464/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 200 [30592/50000]\tLoss: 0.0462\tLR: 0.000800\n",
            "Training Epoch: 200 [30720/50000]\tLoss: 0.0104\tLR: 0.000800\n",
            "Training Epoch: 200 [30848/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 200 [30976/50000]\tLoss: 0.0061\tLR: 0.000800\n",
            "Training Epoch: 200 [31104/50000]\tLoss: 0.0040\tLR: 0.000800\n",
            "Training Epoch: 200 [31232/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 200 [31360/50000]\tLoss: 0.0071\tLR: 0.000800\n",
            "Training Epoch: 200 [31488/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 200 [31616/50000]\tLoss: 0.0248\tLR: 0.000800\n",
            "Training Epoch: 200 [31744/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 200 [31872/50000]\tLoss: 0.0250\tLR: 0.000800\n",
            "Training Epoch: 200 [32000/50000]\tLoss: 0.0158\tLR: 0.000800\n",
            "Training Epoch: 200 [32128/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 200 [32256/50000]\tLoss: 0.0051\tLR: 0.000800\n",
            "Training Epoch: 200 [32384/50000]\tLoss: 0.0080\tLR: 0.000800\n",
            "Training Epoch: 200 [32512/50000]\tLoss: 0.0090\tLR: 0.000800\n",
            "Training Epoch: 200 [32640/50000]\tLoss: 0.0160\tLR: 0.000800\n",
            "Training Epoch: 200 [32768/50000]\tLoss: 0.0156\tLR: 0.000800\n",
            "Training Epoch: 200 [32896/50000]\tLoss: 0.0254\tLR: 0.000800\n",
            "Training Epoch: 200 [33024/50000]\tLoss: 0.0123\tLR: 0.000800\n",
            "Training Epoch: 200 [33152/50000]\tLoss: 0.0178\tLR: 0.000800\n",
            "Training Epoch: 200 [33280/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 200 [33408/50000]\tLoss: 0.0176\tLR: 0.000800\n",
            "Training Epoch: 200 [33536/50000]\tLoss: 0.0124\tLR: 0.000800\n",
            "Training Epoch: 200 [33664/50000]\tLoss: 0.0084\tLR: 0.000800\n",
            "Training Epoch: 200 [33792/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 200 [33920/50000]\tLoss: 0.0081\tLR: 0.000800\n",
            "Training Epoch: 200 [34048/50000]\tLoss: 0.0076\tLR: 0.000800\n",
            "Training Epoch: 200 [34176/50000]\tLoss: 0.0025\tLR: 0.000800\n",
            "Training Epoch: 200 [34304/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 200 [34432/50000]\tLoss: 0.0539\tLR: 0.000800\n",
            "Training Epoch: 200 [34560/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 200 [34688/50000]\tLoss: 0.0192\tLR: 0.000800\n",
            "Training Epoch: 200 [34816/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 200 [34944/50000]\tLoss: 0.0120\tLR: 0.000800\n",
            "Training Epoch: 200 [35072/50000]\tLoss: 0.0331\tLR: 0.000800\n",
            "Training Epoch: 200 [35200/50000]\tLoss: 0.0197\tLR: 0.000800\n",
            "Training Epoch: 200 [35328/50000]\tLoss: 0.0370\tLR: 0.000800\n",
            "Training Epoch: 200 [35456/50000]\tLoss: 0.0454\tLR: 0.000800\n",
            "Training Epoch: 200 [35584/50000]\tLoss: 0.0375\tLR: 0.000800\n",
            "Training Epoch: 200 [35712/50000]\tLoss: 0.0360\tLR: 0.000800\n",
            "Training Epoch: 200 [35840/50000]\tLoss: 0.0274\tLR: 0.000800\n",
            "Training Epoch: 200 [35968/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 200 [36096/50000]\tLoss: 0.0126\tLR: 0.000800\n",
            "Training Epoch: 200 [36224/50000]\tLoss: 0.0089\tLR: 0.000800\n",
            "Training Epoch: 200 [36352/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 200 [36480/50000]\tLoss: 0.0376\tLR: 0.000800\n",
            "Training Epoch: 200 [36608/50000]\tLoss: 0.0107\tLR: 0.000800\n",
            "Training Epoch: 200 [36736/50000]\tLoss: 0.0109\tLR: 0.000800\n",
            "Training Epoch: 200 [36864/50000]\tLoss: 0.0360\tLR: 0.000800\n",
            "Training Epoch: 200 [36992/50000]\tLoss: 0.0373\tLR: 0.000800\n",
            "Training Epoch: 200 [37120/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 200 [37248/50000]\tLoss: 0.0079\tLR: 0.000800\n",
            "Training Epoch: 200 [37376/50000]\tLoss: 0.0142\tLR: 0.000800\n",
            "Training Epoch: 200 [37504/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 200 [37632/50000]\tLoss: 0.0039\tLR: 0.000800\n",
            "Training Epoch: 200 [37760/50000]\tLoss: 0.0103\tLR: 0.000800\n",
            "Training Epoch: 200 [37888/50000]\tLoss: 0.0042\tLR: 0.000800\n",
            "Training Epoch: 200 [38016/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 200 [38144/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 200 [38272/50000]\tLoss: 0.0036\tLR: 0.000800\n",
            "Training Epoch: 200 [38400/50000]\tLoss: 0.0182\tLR: 0.000800\n",
            "Training Epoch: 200 [38528/50000]\tLoss: 0.0341\tLR: 0.000800\n",
            "Training Epoch: 200 [38656/50000]\tLoss: 0.0261\tLR: 0.000800\n",
            "Training Epoch: 200 [38784/50000]\tLoss: 0.0099\tLR: 0.000800\n",
            "Training Epoch: 200 [38912/50000]\tLoss: 0.0206\tLR: 0.000800\n",
            "Training Epoch: 200 [39040/50000]\tLoss: 0.0045\tLR: 0.000800\n",
            "Training Epoch: 200 [39168/50000]\tLoss: 0.0143\tLR: 0.000800\n",
            "Training Epoch: 200 [39296/50000]\tLoss: 0.0391\tLR: 0.000800\n",
            "Training Epoch: 200 [39424/50000]\tLoss: 0.0044\tLR: 0.000800\n",
            "Training Epoch: 200 [39552/50000]\tLoss: 0.0133\tLR: 0.000800\n",
            "Training Epoch: 200 [39680/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 200 [39808/50000]\tLoss: 0.0067\tLR: 0.000800\n",
            "Training Epoch: 200 [39936/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 200 [40064/50000]\tLoss: 0.0095\tLR: 0.000800\n",
            "Training Epoch: 200 [40192/50000]\tLoss: 0.0208\tLR: 0.000800\n",
            "Training Epoch: 200 [40320/50000]\tLoss: 0.0046\tLR: 0.000800\n",
            "Training Epoch: 200 [40448/50000]\tLoss: 0.0052\tLR: 0.000800\n",
            "Training Epoch: 200 [40576/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 200 [40704/50000]\tLoss: 0.0394\tLR: 0.000800\n",
            "Training Epoch: 200 [40832/50000]\tLoss: 0.0468\tLR: 0.000800\n",
            "Training Epoch: 200 [40960/50000]\tLoss: 0.0266\tLR: 0.000800\n",
            "Training Epoch: 200 [41088/50000]\tLoss: 0.0192\tLR: 0.000800\n",
            "Training Epoch: 200 [41216/50000]\tLoss: 0.0197\tLR: 0.000800\n",
            "Training Epoch: 200 [41344/50000]\tLoss: 0.0451\tLR: 0.000800\n",
            "Training Epoch: 200 [41472/50000]\tLoss: 0.0096\tLR: 0.000800\n",
            "Training Epoch: 200 [41600/50000]\tLoss: 0.0157\tLR: 0.000800\n",
            "Training Epoch: 200 [41728/50000]\tLoss: 0.0077\tLR: 0.000800\n",
            "Training Epoch: 200 [41856/50000]\tLoss: 0.0313\tLR: 0.000800\n",
            "Training Epoch: 200 [41984/50000]\tLoss: 0.0112\tLR: 0.000800\n",
            "Training Epoch: 200 [42112/50000]\tLoss: 0.0239\tLR: 0.000800\n",
            "Training Epoch: 200 [42240/50000]\tLoss: 0.0200\tLR: 0.000800\n",
            "Training Epoch: 200 [42368/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 200 [42496/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 200 [42624/50000]\tLoss: 0.0083\tLR: 0.000800\n",
            "Training Epoch: 200 [42752/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 200 [42880/50000]\tLoss: 0.0068\tLR: 0.000800\n",
            "Training Epoch: 200 [43008/50000]\tLoss: 0.0152\tLR: 0.000800\n",
            "Training Epoch: 200 [43136/50000]\tLoss: 0.0708\tLR: 0.000800\n",
            "Training Epoch: 200 [43264/50000]\tLoss: 0.0057\tLR: 0.000800\n",
            "Training Epoch: 200 [43392/50000]\tLoss: 0.0151\tLR: 0.000800\n",
            "Training Epoch: 200 [43520/50000]\tLoss: 0.0088\tLR: 0.000800\n",
            "Training Epoch: 200 [43648/50000]\tLoss: 0.0098\tLR: 0.000800\n",
            "Training Epoch: 200 [43776/50000]\tLoss: 0.0534\tLR: 0.000800\n",
            "Training Epoch: 200 [43904/50000]\tLoss: 0.0777\tLR: 0.000800\n",
            "Training Epoch: 200 [44032/50000]\tLoss: 0.0072\tLR: 0.000800\n",
            "Training Epoch: 200 [44160/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 200 [44288/50000]\tLoss: 0.0047\tLR: 0.000800\n",
            "Training Epoch: 200 [44416/50000]\tLoss: 0.0390\tLR: 0.000800\n",
            "Training Epoch: 200 [44544/50000]\tLoss: 0.0185\tLR: 0.000800\n",
            "Training Epoch: 200 [44672/50000]\tLoss: 0.0348\tLR: 0.000800\n",
            "Training Epoch: 200 [44800/50000]\tLoss: 0.0173\tLR: 0.000800\n",
            "Training Epoch: 200 [44928/50000]\tLoss: 0.0364\tLR: 0.000800\n",
            "Training Epoch: 200 [45056/50000]\tLoss: 0.0092\tLR: 0.000800\n",
            "Training Epoch: 200 [45184/50000]\tLoss: 0.0147\tLR: 0.000800\n",
            "Training Epoch: 200 [45312/50000]\tLoss: 0.0157\tLR: 0.000800\n",
            "Training Epoch: 200 [45440/50000]\tLoss: 0.0125\tLR: 0.000800\n",
            "Training Epoch: 200 [45568/50000]\tLoss: 0.0048\tLR: 0.000800\n",
            "Training Epoch: 200 [45696/50000]\tLoss: 0.0189\tLR: 0.000800\n",
            "Training Epoch: 200 [45824/50000]\tLoss: 0.0349\tLR: 0.000800\n",
            "Training Epoch: 200 [45952/50000]\tLoss: 0.0055\tLR: 0.000800\n",
            "Training Epoch: 200 [46080/50000]\tLoss: 0.0066\tLR: 0.000800\n",
            "Training Epoch: 200 [46208/50000]\tLoss: 0.0287\tLR: 0.000800\n",
            "Training Epoch: 200 [46336/50000]\tLoss: 0.0421\tLR: 0.000800\n",
            "Training Epoch: 200 [46464/50000]\tLoss: 0.0035\tLR: 0.000800\n",
            "Training Epoch: 200 [46592/50000]\tLoss: 0.0075\tLR: 0.000800\n",
            "Training Epoch: 200 [46720/50000]\tLoss: 0.0364\tLR: 0.000800\n",
            "Training Epoch: 200 [46848/50000]\tLoss: 0.0460\tLR: 0.000800\n",
            "Training Epoch: 200 [46976/50000]\tLoss: 0.0398\tLR: 0.000800\n",
            "Training Epoch: 200 [47104/50000]\tLoss: 0.0227\tLR: 0.000800\n",
            "Training Epoch: 200 [47232/50000]\tLoss: 0.0070\tLR: 0.000800\n",
            "Training Epoch: 200 [47360/50000]\tLoss: 0.0216\tLR: 0.000800\n",
            "Training Epoch: 200 [47488/50000]\tLoss: 0.0268\tLR: 0.000800\n",
            "Training Epoch: 200 [47616/50000]\tLoss: 0.0138\tLR: 0.000800\n",
            "Training Epoch: 200 [47744/50000]\tLoss: 0.0108\tLR: 0.000800\n",
            "Training Epoch: 200 [47872/50000]\tLoss: 0.0086\tLR: 0.000800\n",
            "Training Epoch: 200 [48000/50000]\tLoss: 0.0445\tLR: 0.000800\n",
            "Training Epoch: 200 [48128/50000]\tLoss: 0.0026\tLR: 0.000800\n",
            "Training Epoch: 200 [48256/50000]\tLoss: 0.0296\tLR: 0.000800\n",
            "Training Epoch: 200 [48384/50000]\tLoss: 0.0100\tLR: 0.000800\n",
            "Training Epoch: 200 [48512/50000]\tLoss: 0.0129\tLR: 0.000800\n",
            "Training Epoch: 200 [48640/50000]\tLoss: 0.0074\tLR: 0.000800\n",
            "Training Epoch: 200 [48768/50000]\tLoss: 0.0137\tLR: 0.000800\n",
            "Training Epoch: 200 [48896/50000]\tLoss: 0.0171\tLR: 0.000800\n",
            "Training Epoch: 200 [49024/50000]\tLoss: 0.0807\tLR: 0.000800\n",
            "Training Epoch: 200 [49152/50000]\tLoss: 0.0031\tLR: 0.000800\n",
            "Training Epoch: 200 [49280/50000]\tLoss: 0.0056\tLR: 0.000800\n",
            "Training Epoch: 200 [49408/50000]\tLoss: 0.0240\tLR: 0.000800\n",
            "Training Epoch: 200 [49536/50000]\tLoss: 0.0236\tLR: 0.000800\n",
            "Training Epoch: 200 [49664/50000]\tLoss: 0.0105\tLR: 0.000800\n",
            "Training Epoch: 200 [49792/50000]\tLoss: 0.0149\tLR: 0.000800\n",
            "Training Epoch: 200 [49920/50000]\tLoss: 0.0352\tLR: 0.000800\n",
            "Training Epoch: 200 [50000/50000]\tLoss: 0.0549\tLR: 0.000800\n",
            "epoch 200 training time consumed: 34.81s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 418462 KiB | 771528 KiB | 222826 GiB | 222826 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 220795 GiB | 220795 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   2030 GiB |   2030 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 418462 KiB | 771528 KiB | 222826 GiB | 222826 GiB |\n",
            "|       from large pool | 414912 KiB | 769152 KiB | 220795 GiB | 220795 GiB |\n",
            "|       from small pool |   3550 KiB |  12370 KiB |   2030 GiB |   2030 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 415501 KiB | 765946 KiB | 222138 GiB | 222138 GiB |\n",
            "|       from large pool | 411968 KiB | 763584 KiB | 220108 GiB | 220108 GiB |\n",
            "|       from small pool |   3533 KiB |  12356 KiB |   2029 GiB |   2029 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    980 MiB |    980 MiB |    980 MiB |      0 B   |\n",
            "|       from large pool |    966 MiB |    966 MiB |    966 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 251234 KiB | 401976 KiB | 197088 GiB | 197088 GiB |\n",
            "|       from large pool | 248640 KiB | 398464 KiB | 195057 GiB | 195057 GiB |\n",
            "|       from small pool |   2594 KiB |   4794 KiB |   2031 GiB |   2031 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     225    |     280    |   24408 K  |   24408 K  |\n",
            "|       from large pool |      38    |      61    |   10667 K  |   10667 K  |\n",
            "|       from small pool |     187    |     230    |   13741 K  |   13740 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     225    |     280    |   24408 K  |   24408 K  |\n",
            "|       from large pool |      38    |      61    |   10667 K  |   10667 K  |\n",
            "|       from small pool |     187    |     230    |   13741 K  |   13740 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
            "|       from large pool |      17    |      17    |      17    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      20    |      28    |   10122 K  |   10122 K  |\n",
            "|       from large pool |      13    |      15    |    4849 K  |    4849 K  |\n",
            "|       from small pool |       7    |      14    |    5273 K  |    5273 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 200, Average loss: 0.0129, Accuracy: 0.7209, Time consumed:3.01s\n",
            "\n",
            "saving weights file to checkpoint/vgg16/Saturday_04_May_2024_07h_16m_24s/vgg16-200-regular.pth\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/Knowledge-Distillation/train.py -net vgg16 -gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing with CPU"
      ],
      "metadata": {
        "id": "_UEMQaOxvzUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Knowledge-Distillation/test.py -net vgg16 -weights /content/drive/MyDrive/Knowledge-Distillation/runs/vgg16/vgg16-164-best.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZczzDz_v07g",
        "outputId": "196053c7-54e1-4ccb-88d0-fc96276e073a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n",
            "100% 169001437/169001437 [00:03<00:00, 43241464.46it/s]\n",
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "iteration: 1 \t total 625 iterations\n",
            "iteration: 2 \t total 625 iterations\n",
            "iteration: 3 \t total 625 iterations\n",
            "iteration: 4 \t total 625 iterations\n",
            "iteration: 5 \t total 625 iterations\n",
            "iteration: 6 \t total 625 iterations\n",
            "iteration: 7 \t total 625 iterations\n",
            "iteration: 8 \t total 625 iterations\n",
            "iteration: 9 \t total 625 iterations\n",
            "iteration: 10 \t total 625 iterations\n",
            "iteration: 11 \t total 625 iterations\n",
            "iteration: 12 \t total 625 iterations\n",
            "iteration: 13 \t total 625 iterations\n",
            "iteration: 14 \t total 625 iterations\n",
            "iteration: 15 \t total 625 iterations\n",
            "iteration: 16 \t total 625 iterations\n",
            "iteration: 17 \t total 625 iterations\n",
            "iteration: 18 \t total 625 iterations\n",
            "iteration: 19 \t total 625 iterations\n",
            "iteration: 20 \t total 625 iterations\n",
            "iteration: 21 \t total 625 iterations\n",
            "iteration: 22 \t total 625 iterations\n",
            "iteration: 23 \t total 625 iterations\n",
            "iteration: 24 \t total 625 iterations\n",
            "iteration: 25 \t total 625 iterations\n",
            "iteration: 26 \t total 625 iterations\n",
            "iteration: 27 \t total 625 iterations\n",
            "iteration: 28 \t total 625 iterations\n",
            "iteration: 29 \t total 625 iterations\n",
            "iteration: 30 \t total 625 iterations\n",
            "iteration: 31 \t total 625 iterations\n",
            "iteration: 32 \t total 625 iterations\n",
            "iteration: 33 \t total 625 iterations\n",
            "iteration: 34 \t total 625 iterations\n",
            "iteration: 35 \t total 625 iterations\n",
            "iteration: 36 \t total 625 iterations\n",
            "iteration: 37 \t total 625 iterations\n",
            "iteration: 38 \t total 625 iterations\n",
            "iteration: 39 \t total 625 iterations\n",
            "iteration: 40 \t total 625 iterations\n",
            "iteration: 41 \t total 625 iterations\n",
            "iteration: 42 \t total 625 iterations\n",
            "iteration: 43 \t total 625 iterations\n",
            "iteration: 44 \t total 625 iterations\n",
            "iteration: 45 \t total 625 iterations\n",
            "iteration: 46 \t total 625 iterations\n",
            "iteration: 47 \t total 625 iterations\n",
            "iteration: 48 \t total 625 iterations\n",
            "iteration: 49 \t total 625 iterations\n",
            "iteration: 50 \t total 625 iterations\n",
            "iteration: 51 \t total 625 iterations\n",
            "iteration: 52 \t total 625 iterations\n",
            "iteration: 53 \t total 625 iterations\n",
            "iteration: 54 \t total 625 iterations\n",
            "iteration: 55 \t total 625 iterations\n",
            "iteration: 56 \t total 625 iterations\n",
            "iteration: 57 \t total 625 iterations\n",
            "iteration: 58 \t total 625 iterations\n",
            "iteration: 59 \t total 625 iterations\n",
            "iteration: 60 \t total 625 iterations\n",
            "iteration: 61 \t total 625 iterations\n",
            "iteration: 62 \t total 625 iterations\n",
            "iteration: 63 \t total 625 iterations\n",
            "iteration: 64 \t total 625 iterations\n",
            "iteration: 65 \t total 625 iterations\n",
            "iteration: 66 \t total 625 iterations\n",
            "iteration: 67 \t total 625 iterations\n",
            "iteration: 68 \t total 625 iterations\n",
            "iteration: 69 \t total 625 iterations\n",
            "iteration: 70 \t total 625 iterations\n",
            "iteration: 71 \t total 625 iterations\n",
            "iteration: 72 \t total 625 iterations\n",
            "iteration: 73 \t total 625 iterations\n",
            "iteration: 74 \t total 625 iterations\n",
            "iteration: 75 \t total 625 iterations\n",
            "iteration: 76 \t total 625 iterations\n",
            "iteration: 77 \t total 625 iterations\n",
            "iteration: 78 \t total 625 iterations\n",
            "iteration: 79 \t total 625 iterations\n",
            "iteration: 80 \t total 625 iterations\n",
            "iteration: 81 \t total 625 iterations\n",
            "iteration: 82 \t total 625 iterations\n",
            "iteration: 83 \t total 625 iterations\n",
            "iteration: 84 \t total 625 iterations\n",
            "iteration: 85 \t total 625 iterations\n",
            "iteration: 86 \t total 625 iterations\n",
            "iteration: 87 \t total 625 iterations\n",
            "iteration: 88 \t total 625 iterations\n",
            "iteration: 89 \t total 625 iterations\n",
            "iteration: 90 \t total 625 iterations\n",
            "iteration: 91 \t total 625 iterations\n",
            "iteration: 92 \t total 625 iterations\n",
            "iteration: 93 \t total 625 iterations\n",
            "iteration: 94 \t total 625 iterations\n",
            "iteration: 95 \t total 625 iterations\n",
            "iteration: 96 \t total 625 iterations\n",
            "iteration: 97 \t total 625 iterations\n",
            "iteration: 98 \t total 625 iterations\n",
            "iteration: 99 \t total 625 iterations\n",
            "iteration: 100 \t total 625 iterations\n",
            "iteration: 101 \t total 625 iterations\n",
            "iteration: 102 \t total 625 iterations\n",
            "iteration: 103 \t total 625 iterations\n",
            "iteration: 104 \t total 625 iterations\n",
            "iteration: 105 \t total 625 iterations\n",
            "iteration: 106 \t total 625 iterations\n",
            "iteration: 107 \t total 625 iterations\n",
            "iteration: 108 \t total 625 iterations\n",
            "iteration: 109 \t total 625 iterations\n",
            "iteration: 110 \t total 625 iterations\n",
            "iteration: 111 \t total 625 iterations\n",
            "iteration: 112 \t total 625 iterations\n",
            "iteration: 113 \t total 625 iterations\n",
            "iteration: 114 \t total 625 iterations\n",
            "iteration: 115 \t total 625 iterations\n",
            "iteration: 116 \t total 625 iterations\n",
            "iteration: 117 \t total 625 iterations\n",
            "iteration: 118 \t total 625 iterations\n",
            "iteration: 119 \t total 625 iterations\n",
            "iteration: 120 \t total 625 iterations\n",
            "iteration: 121 \t total 625 iterations\n",
            "iteration: 122 \t total 625 iterations\n",
            "iteration: 123 \t total 625 iterations\n",
            "iteration: 124 \t total 625 iterations\n",
            "iteration: 125 \t total 625 iterations\n",
            "iteration: 126 \t total 625 iterations\n",
            "iteration: 127 \t total 625 iterations\n",
            "iteration: 128 \t total 625 iterations\n",
            "iteration: 129 \t total 625 iterations\n",
            "iteration: 130 \t total 625 iterations\n",
            "iteration: 131 \t total 625 iterations\n",
            "iteration: 132 \t total 625 iterations\n",
            "iteration: 133 \t total 625 iterations\n",
            "iteration: 134 \t total 625 iterations\n",
            "iteration: 135 \t total 625 iterations\n",
            "iteration: 136 \t total 625 iterations\n",
            "iteration: 137 \t total 625 iterations\n",
            "iteration: 138 \t total 625 iterations\n",
            "iteration: 139 \t total 625 iterations\n",
            "iteration: 140 \t total 625 iterations\n",
            "iteration: 141 \t total 625 iterations\n",
            "iteration: 142 \t total 625 iterations\n",
            "iteration: 143 \t total 625 iterations\n",
            "iteration: 144 \t total 625 iterations\n",
            "iteration: 145 \t total 625 iterations\n",
            "iteration: 146 \t total 625 iterations\n",
            "iteration: 147 \t total 625 iterations\n",
            "iteration: 148 \t total 625 iterations\n",
            "iteration: 149 \t total 625 iterations\n",
            "iteration: 150 \t total 625 iterations\n",
            "iteration: 151 \t total 625 iterations\n",
            "iteration: 152 \t total 625 iterations\n",
            "iteration: 153 \t total 625 iterations\n",
            "iteration: 154 \t total 625 iterations\n",
            "iteration: 155 \t total 625 iterations\n",
            "iteration: 156 \t total 625 iterations\n",
            "iteration: 157 \t total 625 iterations\n",
            "iteration: 158 \t total 625 iterations\n",
            "iteration: 159 \t total 625 iterations\n",
            "iteration: 160 \t total 625 iterations\n",
            "iteration: 161 \t total 625 iterations\n",
            "iteration: 162 \t total 625 iterations\n",
            "iteration: 163 \t total 625 iterations\n",
            "iteration: 164 \t total 625 iterations\n",
            "iteration: 165 \t total 625 iterations\n",
            "iteration: 166 \t total 625 iterations\n",
            "iteration: 167 \t total 625 iterations\n",
            "iteration: 168 \t total 625 iterations\n",
            "iteration: 169 \t total 625 iterations\n",
            "iteration: 170 \t total 625 iterations\n",
            "iteration: 171 \t total 625 iterations\n",
            "iteration: 172 \t total 625 iterations\n",
            "iteration: 173 \t total 625 iterations\n",
            "iteration: 174 \t total 625 iterations\n",
            "iteration: 175 \t total 625 iterations\n",
            "iteration: 176 \t total 625 iterations\n",
            "iteration: 177 \t total 625 iterations\n",
            "iteration: 178 \t total 625 iterations\n",
            "iteration: 179 \t total 625 iterations\n",
            "iteration: 180 \t total 625 iterations\n",
            "iteration: 181 \t total 625 iterations\n",
            "iteration: 182 \t total 625 iterations\n",
            "iteration: 183 \t total 625 iterations\n",
            "iteration: 184 \t total 625 iterations\n",
            "iteration: 185 \t total 625 iterations\n",
            "iteration: 186 \t total 625 iterations\n",
            "iteration: 187 \t total 625 iterations\n",
            "iteration: 188 \t total 625 iterations\n",
            "iteration: 189 \t total 625 iterations\n",
            "iteration: 190 \t total 625 iterations\n",
            "iteration: 191 \t total 625 iterations\n",
            "iteration: 192 \t total 625 iterations\n",
            "iteration: 193 \t total 625 iterations\n",
            "iteration: 194 \t total 625 iterations\n",
            "iteration: 195 \t total 625 iterations\n",
            "iteration: 196 \t total 625 iterations\n",
            "iteration: 197 \t total 625 iterations\n",
            "iteration: 198 \t total 625 iterations\n",
            "iteration: 199 \t total 625 iterations\n",
            "iteration: 200 \t total 625 iterations\n",
            "iteration: 201 \t total 625 iterations\n",
            "iteration: 202 \t total 625 iterations\n",
            "iteration: 203 \t total 625 iterations\n",
            "iteration: 204 \t total 625 iterations\n",
            "iteration: 205 \t total 625 iterations\n",
            "iteration: 206 \t total 625 iterations\n",
            "iteration: 207 \t total 625 iterations\n",
            "iteration: 208 \t total 625 iterations\n",
            "iteration: 209 \t total 625 iterations\n",
            "iteration: 210 \t total 625 iterations\n",
            "iteration: 211 \t total 625 iterations\n",
            "iteration: 212 \t total 625 iterations\n",
            "iteration: 213 \t total 625 iterations\n",
            "iteration: 214 \t total 625 iterations\n",
            "iteration: 215 \t total 625 iterations\n",
            "iteration: 216 \t total 625 iterations\n",
            "iteration: 217 \t total 625 iterations\n",
            "iteration: 218 \t total 625 iterations\n",
            "iteration: 219 \t total 625 iterations\n",
            "iteration: 220 \t total 625 iterations\n",
            "iteration: 221 \t total 625 iterations\n",
            "iteration: 222 \t total 625 iterations\n",
            "iteration: 223 \t total 625 iterations\n",
            "iteration: 224 \t total 625 iterations\n",
            "iteration: 225 \t total 625 iterations\n",
            "iteration: 226 \t total 625 iterations\n",
            "iteration: 227 \t total 625 iterations\n",
            "iteration: 228 \t total 625 iterations\n",
            "iteration: 229 \t total 625 iterations\n",
            "iteration: 230 \t total 625 iterations\n",
            "iteration: 231 \t total 625 iterations\n",
            "iteration: 232 \t total 625 iterations\n",
            "iteration: 233 \t total 625 iterations\n",
            "iteration: 234 \t total 625 iterations\n",
            "iteration: 235 \t total 625 iterations\n",
            "iteration: 236 \t total 625 iterations\n",
            "iteration: 237 \t total 625 iterations\n",
            "iteration: 238 \t total 625 iterations\n",
            "iteration: 239 \t total 625 iterations\n",
            "iteration: 240 \t total 625 iterations\n",
            "iteration: 241 \t total 625 iterations\n",
            "iteration: 242 \t total 625 iterations\n",
            "iteration: 243 \t total 625 iterations\n",
            "iteration: 244 \t total 625 iterations\n",
            "iteration: 245 \t total 625 iterations\n",
            "iteration: 246 \t total 625 iterations\n",
            "iteration: 247 \t total 625 iterations\n",
            "iteration: 248 \t total 625 iterations\n",
            "iteration: 249 \t total 625 iterations\n",
            "iteration: 250 \t total 625 iterations\n",
            "iteration: 251 \t total 625 iterations\n",
            "iteration: 252 \t total 625 iterations\n",
            "iteration: 253 \t total 625 iterations\n",
            "iteration: 254 \t total 625 iterations\n",
            "iteration: 255 \t total 625 iterations\n",
            "iteration: 256 \t total 625 iterations\n",
            "iteration: 257 \t total 625 iterations\n",
            "iteration: 258 \t total 625 iterations\n",
            "iteration: 259 \t total 625 iterations\n",
            "iteration: 260 \t total 625 iterations\n",
            "iteration: 261 \t total 625 iterations\n",
            "iteration: 262 \t total 625 iterations\n",
            "iteration: 263 \t total 625 iterations\n",
            "iteration: 264 \t total 625 iterations\n",
            "iteration: 265 \t total 625 iterations\n",
            "iteration: 266 \t total 625 iterations\n",
            "iteration: 267 \t total 625 iterations\n",
            "iteration: 268 \t total 625 iterations\n",
            "iteration: 269 \t total 625 iterations\n",
            "iteration: 270 \t total 625 iterations\n",
            "iteration: 271 \t total 625 iterations\n",
            "iteration: 272 \t total 625 iterations\n",
            "iteration: 273 \t total 625 iterations\n",
            "iteration: 274 \t total 625 iterations\n",
            "iteration: 275 \t total 625 iterations\n",
            "iteration: 276 \t total 625 iterations\n",
            "iteration: 277 \t total 625 iterations\n",
            "iteration: 278 \t total 625 iterations\n",
            "iteration: 279 \t total 625 iterations\n",
            "iteration: 280 \t total 625 iterations\n",
            "iteration: 281 \t total 625 iterations\n",
            "iteration: 282 \t total 625 iterations\n",
            "iteration: 283 \t total 625 iterations\n",
            "iteration: 284 \t total 625 iterations\n",
            "iteration: 285 \t total 625 iterations\n",
            "iteration: 286 \t total 625 iterations\n",
            "iteration: 287 \t total 625 iterations\n",
            "iteration: 288 \t total 625 iterations\n",
            "iteration: 289 \t total 625 iterations\n",
            "iteration: 290 \t total 625 iterations\n",
            "iteration: 291 \t total 625 iterations\n",
            "iteration: 292 \t total 625 iterations\n",
            "iteration: 293 \t total 625 iterations\n",
            "iteration: 294 \t total 625 iterations\n",
            "iteration: 295 \t total 625 iterations\n",
            "iteration: 296 \t total 625 iterations\n",
            "iteration: 297 \t total 625 iterations\n",
            "iteration: 298 \t total 625 iterations\n",
            "iteration: 299 \t total 625 iterations\n",
            "iteration: 300 \t total 625 iterations\n",
            "iteration: 301 \t total 625 iterations\n",
            "iteration: 302 \t total 625 iterations\n",
            "iteration: 303 \t total 625 iterations\n",
            "iteration: 304 \t total 625 iterations\n",
            "iteration: 305 \t total 625 iterations\n",
            "iteration: 306 \t total 625 iterations\n",
            "iteration: 307 \t total 625 iterations\n",
            "iteration: 308 \t total 625 iterations\n",
            "iteration: 309 \t total 625 iterations\n",
            "iteration: 310 \t total 625 iterations\n",
            "iteration: 311 \t total 625 iterations\n",
            "iteration: 312 \t total 625 iterations\n",
            "iteration: 313 \t total 625 iterations\n",
            "iteration: 314 \t total 625 iterations\n",
            "iteration: 315 \t total 625 iterations\n",
            "iteration: 316 \t total 625 iterations\n",
            "iteration: 317 \t total 625 iterations\n",
            "iteration: 318 \t total 625 iterations\n",
            "iteration: 319 \t total 625 iterations\n",
            "iteration: 320 \t total 625 iterations\n",
            "iteration: 321 \t total 625 iterations\n",
            "iteration: 322 \t total 625 iterations\n",
            "iteration: 323 \t total 625 iterations\n",
            "iteration: 324 \t total 625 iterations\n",
            "iteration: 325 \t total 625 iterations\n",
            "iteration: 326 \t total 625 iterations\n",
            "iteration: 327 \t total 625 iterations\n",
            "iteration: 328 \t total 625 iterations\n",
            "iteration: 329 \t total 625 iterations\n",
            "iteration: 330 \t total 625 iterations\n",
            "iteration: 331 \t total 625 iterations\n",
            "iteration: 332 \t total 625 iterations\n",
            "iteration: 333 \t total 625 iterations\n",
            "iteration: 334 \t total 625 iterations\n",
            "iteration: 335 \t total 625 iterations\n",
            "iteration: 336 \t total 625 iterations\n",
            "iteration: 337 \t total 625 iterations\n",
            "iteration: 338 \t total 625 iterations\n",
            "iteration: 339 \t total 625 iterations\n",
            "iteration: 340 \t total 625 iterations\n",
            "iteration: 341 \t total 625 iterations\n",
            "iteration: 342 \t total 625 iterations\n",
            "iteration: 343 \t total 625 iterations\n",
            "iteration: 344 \t total 625 iterations\n",
            "iteration: 345 \t total 625 iterations\n",
            "iteration: 346 \t total 625 iterations\n",
            "iteration: 347 \t total 625 iterations\n",
            "iteration: 348 \t total 625 iterations\n",
            "iteration: 349 \t total 625 iterations\n",
            "iteration: 350 \t total 625 iterations\n",
            "iteration: 351 \t total 625 iterations\n",
            "iteration: 352 \t total 625 iterations\n",
            "iteration: 353 \t total 625 iterations\n",
            "iteration: 354 \t total 625 iterations\n",
            "iteration: 355 \t total 625 iterations\n",
            "iteration: 356 \t total 625 iterations\n",
            "iteration: 357 \t total 625 iterations\n",
            "iteration: 358 \t total 625 iterations\n",
            "iteration: 359 \t total 625 iterations\n",
            "iteration: 360 \t total 625 iterations\n",
            "iteration: 361 \t total 625 iterations\n",
            "iteration: 362 \t total 625 iterations\n",
            "iteration: 363 \t total 625 iterations\n",
            "iteration: 364 \t total 625 iterations\n",
            "iteration: 365 \t total 625 iterations\n",
            "iteration: 366 \t total 625 iterations\n",
            "iteration: 367 \t total 625 iterations\n",
            "iteration: 368 \t total 625 iterations\n",
            "iteration: 369 \t total 625 iterations\n",
            "iteration: 370 \t total 625 iterations\n",
            "iteration: 371 \t total 625 iterations\n",
            "iteration: 372 \t total 625 iterations\n",
            "iteration: 373 \t total 625 iterations\n",
            "iteration: 374 \t total 625 iterations\n",
            "iteration: 375 \t total 625 iterations\n",
            "iteration: 376 \t total 625 iterations\n",
            "iteration: 377 \t total 625 iterations\n",
            "iteration: 378 \t total 625 iterations\n",
            "iteration: 379 \t total 625 iterations\n",
            "iteration: 380 \t total 625 iterations\n",
            "iteration: 381 \t total 625 iterations\n",
            "iteration: 382 \t total 625 iterations\n",
            "iteration: 383 \t total 625 iterations\n",
            "iteration: 384 \t total 625 iterations\n",
            "iteration: 385 \t total 625 iterations\n",
            "iteration: 386 \t total 625 iterations\n",
            "iteration: 387 \t total 625 iterations\n",
            "iteration: 388 \t total 625 iterations\n",
            "iteration: 389 \t total 625 iterations\n",
            "iteration: 390 \t total 625 iterations\n",
            "iteration: 391 \t total 625 iterations\n",
            "iteration: 392 \t total 625 iterations\n",
            "iteration: 393 \t total 625 iterations\n",
            "iteration: 394 \t total 625 iterations\n",
            "iteration: 395 \t total 625 iterations\n",
            "iteration: 396 \t total 625 iterations\n",
            "iteration: 397 \t total 625 iterations\n",
            "iteration: 398 \t total 625 iterations\n",
            "iteration: 399 \t total 625 iterations\n",
            "iteration: 400 \t total 625 iterations\n",
            "iteration: 401 \t total 625 iterations\n",
            "iteration: 402 \t total 625 iterations\n",
            "iteration: 403 \t total 625 iterations\n",
            "iteration: 404 \t total 625 iterations\n",
            "iteration: 405 \t total 625 iterations\n",
            "iteration: 406 \t total 625 iterations\n",
            "iteration: 407 \t total 625 iterations\n",
            "iteration: 408 \t total 625 iterations\n",
            "iteration: 409 \t total 625 iterations\n",
            "iteration: 410 \t total 625 iterations\n",
            "iteration: 411 \t total 625 iterations\n",
            "iteration: 412 \t total 625 iterations\n",
            "iteration: 413 \t total 625 iterations\n",
            "iteration: 414 \t total 625 iterations\n",
            "iteration: 415 \t total 625 iterations\n",
            "iteration: 416 \t total 625 iterations\n",
            "iteration: 417 \t total 625 iterations\n",
            "iteration: 418 \t total 625 iterations\n",
            "iteration: 419 \t total 625 iterations\n",
            "iteration: 420 \t total 625 iterations\n",
            "iteration: 421 \t total 625 iterations\n",
            "iteration: 422 \t total 625 iterations\n",
            "iteration: 423 \t total 625 iterations\n",
            "iteration: 424 \t total 625 iterations\n",
            "iteration: 425 \t total 625 iterations\n",
            "iteration: 426 \t total 625 iterations\n",
            "iteration: 427 \t total 625 iterations\n",
            "iteration: 428 \t total 625 iterations\n",
            "iteration: 429 \t total 625 iterations\n",
            "iteration: 430 \t total 625 iterations\n",
            "iteration: 431 \t total 625 iterations\n",
            "iteration: 432 \t total 625 iterations\n",
            "iteration: 433 \t total 625 iterations\n",
            "iteration: 434 \t total 625 iterations\n",
            "iteration: 435 \t total 625 iterations\n",
            "iteration: 436 \t total 625 iterations\n",
            "iteration: 437 \t total 625 iterations\n",
            "iteration: 438 \t total 625 iterations\n",
            "iteration: 439 \t total 625 iterations\n",
            "iteration: 440 \t total 625 iterations\n",
            "iteration: 441 \t total 625 iterations\n",
            "iteration: 442 \t total 625 iterations\n",
            "iteration: 443 \t total 625 iterations\n",
            "iteration: 444 \t total 625 iterations\n",
            "iteration: 445 \t total 625 iterations\n",
            "iteration: 446 \t total 625 iterations\n",
            "iteration: 447 \t total 625 iterations\n",
            "iteration: 448 \t total 625 iterations\n",
            "iteration: 449 \t total 625 iterations\n",
            "iteration: 450 \t total 625 iterations\n",
            "iteration: 451 \t total 625 iterations\n",
            "iteration: 452 \t total 625 iterations\n",
            "iteration: 453 \t total 625 iterations\n",
            "iteration: 454 \t total 625 iterations\n",
            "iteration: 455 \t total 625 iterations\n",
            "iteration: 456 \t total 625 iterations\n",
            "iteration: 457 \t total 625 iterations\n",
            "iteration: 458 \t total 625 iterations\n",
            "iteration: 459 \t total 625 iterations\n",
            "iteration: 460 \t total 625 iterations\n",
            "iteration: 461 \t total 625 iterations\n",
            "iteration: 462 \t total 625 iterations\n",
            "iteration: 463 \t total 625 iterations\n",
            "iteration: 464 \t total 625 iterations\n",
            "iteration: 465 \t total 625 iterations\n",
            "iteration: 466 \t total 625 iterations\n",
            "iteration: 467 \t total 625 iterations\n",
            "iteration: 468 \t total 625 iterations\n",
            "iteration: 469 \t total 625 iterations\n",
            "iteration: 470 \t total 625 iterations\n",
            "iteration: 471 \t total 625 iterations\n",
            "iteration: 472 \t total 625 iterations\n",
            "iteration: 473 \t total 625 iterations\n",
            "iteration: 474 \t total 625 iterations\n",
            "iteration: 475 \t total 625 iterations\n",
            "iteration: 476 \t total 625 iterations\n",
            "iteration: 477 \t total 625 iterations\n",
            "iteration: 478 \t total 625 iterations\n",
            "iteration: 479 \t total 625 iterations\n",
            "iteration: 480 \t total 625 iterations\n",
            "iteration: 481 \t total 625 iterations\n",
            "iteration: 482 \t total 625 iterations\n",
            "iteration: 483 \t total 625 iterations\n",
            "iteration: 484 \t total 625 iterations\n",
            "iteration: 485 \t total 625 iterations\n",
            "iteration: 486 \t total 625 iterations\n",
            "iteration: 487 \t total 625 iterations\n",
            "iteration: 488 \t total 625 iterations\n",
            "iteration: 489 \t total 625 iterations\n",
            "iteration: 490 \t total 625 iterations\n",
            "iteration: 491 \t total 625 iterations\n",
            "iteration: 492 \t total 625 iterations\n",
            "iteration: 493 \t total 625 iterations\n",
            "iteration: 494 \t total 625 iterations\n",
            "iteration: 495 \t total 625 iterations\n",
            "iteration: 496 \t total 625 iterations\n",
            "iteration: 497 \t total 625 iterations\n",
            "iteration: 498 \t total 625 iterations\n",
            "iteration: 499 \t total 625 iterations\n",
            "iteration: 500 \t total 625 iterations\n",
            "iteration: 501 \t total 625 iterations\n",
            "iteration: 502 \t total 625 iterations\n",
            "iteration: 503 \t total 625 iterations\n",
            "iteration: 504 \t total 625 iterations\n",
            "iteration: 505 \t total 625 iterations\n",
            "iteration: 506 \t total 625 iterations\n",
            "iteration: 507 \t total 625 iterations\n",
            "iteration: 508 \t total 625 iterations\n",
            "iteration: 509 \t total 625 iterations\n",
            "iteration: 510 \t total 625 iterations\n",
            "iteration: 511 \t total 625 iterations\n",
            "iteration: 512 \t total 625 iterations\n",
            "iteration: 513 \t total 625 iterations\n",
            "iteration: 514 \t total 625 iterations\n",
            "iteration: 515 \t total 625 iterations\n",
            "iteration: 516 \t total 625 iterations\n",
            "iteration: 517 \t total 625 iterations\n",
            "iteration: 518 \t total 625 iterations\n",
            "iteration: 519 \t total 625 iterations\n",
            "iteration: 520 \t total 625 iterations\n",
            "iteration: 521 \t total 625 iterations\n",
            "iteration: 522 \t total 625 iterations\n",
            "iteration: 523 \t total 625 iterations\n",
            "iteration: 524 \t total 625 iterations\n",
            "iteration: 525 \t total 625 iterations\n",
            "iteration: 526 \t total 625 iterations\n",
            "iteration: 527 \t total 625 iterations\n",
            "iteration: 528 \t total 625 iterations\n",
            "iteration: 529 \t total 625 iterations\n",
            "iteration: 530 \t total 625 iterations\n",
            "iteration: 531 \t total 625 iterations\n",
            "iteration: 532 \t total 625 iterations\n",
            "iteration: 533 \t total 625 iterations\n",
            "iteration: 534 \t total 625 iterations\n",
            "iteration: 535 \t total 625 iterations\n",
            "iteration: 536 \t total 625 iterations\n",
            "iteration: 537 \t total 625 iterations\n",
            "iteration: 538 \t total 625 iterations\n",
            "iteration: 539 \t total 625 iterations\n",
            "iteration: 540 \t total 625 iterations\n",
            "iteration: 541 \t total 625 iterations\n",
            "iteration: 542 \t total 625 iterations\n",
            "iteration: 543 \t total 625 iterations\n",
            "iteration: 544 \t total 625 iterations\n",
            "iteration: 545 \t total 625 iterations\n",
            "iteration: 546 \t total 625 iterations\n",
            "iteration: 547 \t total 625 iterations\n",
            "iteration: 548 \t total 625 iterations\n",
            "iteration: 549 \t total 625 iterations\n",
            "iteration: 550 \t total 625 iterations\n",
            "iteration: 551 \t total 625 iterations\n",
            "iteration: 552 \t total 625 iterations\n",
            "iteration: 553 \t total 625 iterations\n",
            "iteration: 554 \t total 625 iterations\n",
            "iteration: 555 \t total 625 iterations\n",
            "iteration: 556 \t total 625 iterations\n",
            "iteration: 557 \t total 625 iterations\n",
            "iteration: 558 \t total 625 iterations\n",
            "iteration: 559 \t total 625 iterations\n",
            "iteration: 560 \t total 625 iterations\n",
            "iteration: 561 \t total 625 iterations\n",
            "iteration: 562 \t total 625 iterations\n",
            "iteration: 563 \t total 625 iterations\n",
            "iteration: 564 \t total 625 iterations\n",
            "iteration: 565 \t total 625 iterations\n",
            "iteration: 566 \t total 625 iterations\n",
            "iteration: 567 \t total 625 iterations\n",
            "iteration: 568 \t total 625 iterations\n",
            "iteration: 569 \t total 625 iterations\n",
            "iteration: 570 \t total 625 iterations\n",
            "iteration: 571 \t total 625 iterations\n",
            "iteration: 572 \t total 625 iterations\n",
            "iteration: 573 \t total 625 iterations\n",
            "iteration: 574 \t total 625 iterations\n",
            "iteration: 575 \t total 625 iterations\n",
            "iteration: 576 \t total 625 iterations\n",
            "iteration: 577 \t total 625 iterations\n",
            "iteration: 578 \t total 625 iterations\n",
            "iteration: 579 \t total 625 iterations\n",
            "iteration: 580 \t total 625 iterations\n",
            "iteration: 581 \t total 625 iterations\n",
            "iteration: 582 \t total 625 iterations\n",
            "iteration: 583 \t total 625 iterations\n",
            "iteration: 584 \t total 625 iterations\n",
            "iteration: 585 \t total 625 iterations\n",
            "iteration: 586 \t total 625 iterations\n",
            "iteration: 587 \t total 625 iterations\n",
            "iteration: 588 \t total 625 iterations\n",
            "iteration: 589 \t total 625 iterations\n",
            "iteration: 590 \t total 625 iterations\n",
            "iteration: 591 \t total 625 iterations\n",
            "iteration: 592 \t total 625 iterations\n",
            "iteration: 593 \t total 625 iterations\n",
            "iteration: 594 \t total 625 iterations\n",
            "iteration: 595 \t total 625 iterations\n",
            "iteration: 596 \t total 625 iterations\n",
            "iteration: 597 \t total 625 iterations\n",
            "iteration: 598 \t total 625 iterations\n",
            "iteration: 599 \t total 625 iterations\n",
            "iteration: 600 \t total 625 iterations\n",
            "iteration: 601 \t total 625 iterations\n",
            "iteration: 602 \t total 625 iterations\n",
            "iteration: 603 \t total 625 iterations\n",
            "iteration: 604 \t total 625 iterations\n",
            "iteration: 605 \t total 625 iterations\n",
            "iteration: 606 \t total 625 iterations\n",
            "iteration: 607 \t total 625 iterations\n",
            "iteration: 608 \t total 625 iterations\n",
            "iteration: 609 \t total 625 iterations\n",
            "iteration: 610 \t total 625 iterations\n",
            "iteration: 611 \t total 625 iterations\n",
            "iteration: 612 \t total 625 iterations\n",
            "iteration: 613 \t total 625 iterations\n",
            "iteration: 614 \t total 625 iterations\n",
            "iteration: 615 \t total 625 iterations\n",
            "iteration: 616 \t total 625 iterations\n",
            "iteration: 617 \t total 625 iterations\n",
            "iteration: 618 \t total 625 iterations\n",
            "iteration: 619 \t total 625 iterations\n",
            "iteration: 620 \t total 625 iterations\n",
            "iteration: 621 \t total 625 iterations\n",
            "iteration: 622 \t total 625 iterations\n",
            "iteration: 623 \t total 625 iterations\n",
            "iteration: 624 \t total 625 iterations\n",
            "iteration: 625 \t total 625 iterations\n",
            "\n",
            "Parameter numbers: 34015396\n",
            "VGG(\n",
            "  34.02 M, 100.000% Params, 333.73 MMac, 99.877% MACs, \n",
            "  (features): Sequential(\n",
            "    14.72 M, 43.284% Params, 314.43 MMac, 94.101% MACs, \n",
            "    (0): Conv2d(1.79 k, 0.005% Params, 1.84 MMac, 0.549% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, 0.000% Params, 131.07 KMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(0, 0.000% Params, 65.54 KMac, 0.020% MACs, inplace=True)\n",
            "    (3): Conv2d(36.93 k, 0.109% Params, 37.81 MMac, 11.317% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(128, 0.000% Params, 131.07 KMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(0, 0.000% Params, 65.54 KMac, 0.020% MACs, inplace=True)\n",
            "    (6): MaxPool2d(0, 0.000% Params, 65.54 KMac, 0.020% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(73.86 k, 0.217% Params, 18.91 MMac, 5.658% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(256, 0.001% Params, 65.54 KMac, 0.020% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(0, 0.000% Params, 32.77 KMac, 0.010% MACs, inplace=True)\n",
            "    (10): Conv2d(147.58 k, 0.434% Params, 37.78 MMac, 11.307% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(256, 0.001% Params, 65.54 KMac, 0.020% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(0, 0.000% Params, 32.77 KMac, 0.010% MACs, inplace=True)\n",
            "    (13): MaxPool2d(0, 0.000% Params, 32.77 KMac, 0.010% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(295.17 k, 0.868% Params, 18.89 MMac, 5.654% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(512, 0.002% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (17): Conv2d(590.08 k, 1.735% Params, 37.77 MMac, 11.302% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(512, 0.002% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (20): Conv2d(590.08 k, 1.735% Params, 37.77 MMac, 11.302% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(512, 0.002% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (23): MaxPool2d(0, 0.000% Params, 16.38 KMac, 0.005% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(1.18 M, 3.469% Params, 18.88 MMac, 5.651% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(1.02 k, 0.003% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(0, 0.000% Params, 8.19 KMac, 0.002% MACs, inplace=True)\n",
            "    (27): Conv2d(2.36 M, 6.937% Params, 37.76 MMac, 11.300% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(1.02 k, 0.003% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(0, 0.000% Params, 8.19 KMac, 0.002% MACs, inplace=True)\n",
            "    (30): Conv2d(2.36 M, 6.937% Params, 37.76 MMac, 11.300% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(1.02 k, 0.003% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(0, 0.000% Params, 8.19 KMac, 0.002% MACs, inplace=True)\n",
            "    (33): MaxPool2d(0, 0.000% Params, 8.19 KMac, 0.002% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(2.36 M, 6.937% Params, 9.44 MMac, 2.825% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(1.02 k, 0.003% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (37): Conv2d(2.36 M, 6.937% Params, 9.44 MMac, 2.825% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(1.02 k, 0.003% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (40): Conv2d(2.36 M, 6.937% Params, 9.44 MMac, 2.825% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(1.02 k, 0.003% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (43): MaxPool2d(0, 0.000% Params, 2.05 KMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    19.29 M, 56.716% Params, 19.3 MMac, 5.776% MACs, \n",
            "    (0): Linear(2.1 M, 6.177% Params, 2.1 MMac, 0.629% MACs, in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, inplace=True)\n",
            "    (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
            "    (3): Linear(16.78 M, 49.334% Params, 16.78 MMac, 5.022% MACs, in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, inplace=True)\n",
            "    (5): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
            "    (6): Linear(409.7 k, 1.204% Params, 409.7 KMac, 0.123% MACs, in_features=4096, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "Top 1 err:  tensor(0.2777)\n",
            "Top 5 err:  tensor(0.1012)\n",
            "FLOPs: 334.14 MMac\n",
            "Params: 34.02 M\n",
            "Inference time: 110.7865 seconds\n",
            "Time per inference step: 177.2584 milliseconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing with GPU"
      ],
      "metadata": {
        "id": "IOttZjCkvpG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/Knowledge-Distillation/test.py -net vgg16 -gpu -weights /content/drive/MyDrive/Knowledge-Distillation/runs/vgg16/vgg16-164-best.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot_Ir_c927zZ",
        "outputId": "857768a1-2c05-45ca-80cd-391f6698433b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "iteration: 1 \t total 625 iterations\n",
            "iteration: 2 \t total 625 iterations\n",
            "iteration: 3 \t total 625 iterations\n",
            "iteration: 4 \t total 625 iterations\n",
            "iteration: 5 \t total 625 iterations\n",
            "iteration: 6 \t total 625 iterations\n",
            "iteration: 7 \t total 625 iterations\n",
            "iteration: 8 \t total 625 iterations\n",
            "iteration: 9 \t total 625 iterations\n",
            "iteration: 10 \t total 625 iterations\n",
            "iteration: 11 \t total 625 iterations\n",
            "iteration: 12 \t total 625 iterations\n",
            "iteration: 13 \t total 625 iterations\n",
            "iteration: 14 \t total 625 iterations\n",
            "iteration: 15 \t total 625 iterations\n",
            "iteration: 16 \t total 625 iterations\n",
            "iteration: 17 \t total 625 iterations\n",
            "iteration: 18 \t total 625 iterations\n",
            "iteration: 19 \t total 625 iterations\n",
            "iteration: 20 \t total 625 iterations\n",
            "iteration: 21 \t total 625 iterations\n",
            "iteration: 22 \t total 625 iterations\n",
            "iteration: 23 \t total 625 iterations\n",
            "iteration: 24 \t total 625 iterations\n",
            "iteration: 25 \t total 625 iterations\n",
            "iteration: 26 \t total 625 iterations\n",
            "iteration: 27 \t total 625 iterations\n",
            "iteration: 28 \t total 625 iterations\n",
            "iteration: 29 \t total 625 iterations\n",
            "iteration: 30 \t total 625 iterations\n",
            "iteration: 31 \t total 625 iterations\n",
            "iteration: 32 \t total 625 iterations\n",
            "iteration: 33 \t total 625 iterations\n",
            "iteration: 34 \t total 625 iterations\n",
            "iteration: 35 \t total 625 iterations\n",
            "iteration: 36 \t total 625 iterations\n",
            "iteration: 37 \t total 625 iterations\n",
            "iteration: 38 \t total 625 iterations\n",
            "iteration: 39 \t total 625 iterations\n",
            "iteration: 40 \t total 625 iterations\n",
            "iteration: 41 \t total 625 iterations\n",
            "iteration: 42 \t total 625 iterations\n",
            "iteration: 43 \t total 625 iterations\n",
            "iteration: 44 \t total 625 iterations\n",
            "iteration: 45 \t total 625 iterations\n",
            "iteration: 46 \t total 625 iterations\n",
            "iteration: 47 \t total 625 iterations\n",
            "iteration: 48 \t total 625 iterations\n",
            "iteration: 49 \t total 625 iterations\n",
            "iteration: 50 \t total 625 iterations\n",
            "iteration: 51 \t total 625 iterations\n",
            "iteration: 52 \t total 625 iterations\n",
            "iteration: 53 \t total 625 iterations\n",
            "iteration: 54 \t total 625 iterations\n",
            "iteration: 55 \t total 625 iterations\n",
            "iteration: 56 \t total 625 iterations\n",
            "iteration: 57 \t total 625 iterations\n",
            "iteration: 58 \t total 625 iterations\n",
            "iteration: 59 \t total 625 iterations\n",
            "iteration: 60 \t total 625 iterations\n",
            "iteration: 61 \t total 625 iterations\n",
            "iteration: 62 \t total 625 iterations\n",
            "iteration: 63 \t total 625 iterations\n",
            "iteration: 64 \t total 625 iterations\n",
            "iteration: 65 \t total 625 iterations\n",
            "iteration: 66 \t total 625 iterations\n",
            "iteration: 67 \t total 625 iterations\n",
            "iteration: 68 \t total 625 iterations\n",
            "iteration: 69 \t total 625 iterations\n",
            "iteration: 70 \t total 625 iterations\n",
            "iteration: 71 \t total 625 iterations\n",
            "iteration: 72 \t total 625 iterations\n",
            "iteration: 73 \t total 625 iterations\n",
            "iteration: 74 \t total 625 iterations\n",
            "iteration: 75 \t total 625 iterations\n",
            "iteration: 76 \t total 625 iterations\n",
            "iteration: 77 \t total 625 iterations\n",
            "iteration: 78 \t total 625 iterations\n",
            "iteration: 79 \t total 625 iterations\n",
            "iteration: 80 \t total 625 iterations\n",
            "iteration: 81 \t total 625 iterations\n",
            "iteration: 82 \t total 625 iterations\n",
            "iteration: 83 \t total 625 iterations\n",
            "iteration: 84 \t total 625 iterations\n",
            "iteration: 85 \t total 625 iterations\n",
            "iteration: 86 \t total 625 iterations\n",
            "iteration: 87 \t total 625 iterations\n",
            "iteration: 88 \t total 625 iterations\n",
            "iteration: 89 \t total 625 iterations\n",
            "iteration: 90 \t total 625 iterations\n",
            "iteration: 91 \t total 625 iterations\n",
            "iteration: 92 \t total 625 iterations\n",
            "iteration: 93 \t total 625 iterations\n",
            "iteration: 94 \t total 625 iterations\n",
            "iteration: 95 \t total 625 iterations\n",
            "iteration: 96 \t total 625 iterations\n",
            "iteration: 97 \t total 625 iterations\n",
            "iteration: 98 \t total 625 iterations\n",
            "iteration: 99 \t total 625 iterations\n",
            "iteration: 100 \t total 625 iterations\n",
            "iteration: 101 \t total 625 iterations\n",
            "iteration: 102 \t total 625 iterations\n",
            "iteration: 103 \t total 625 iterations\n",
            "iteration: 104 \t total 625 iterations\n",
            "iteration: 105 \t total 625 iterations\n",
            "iteration: 106 \t total 625 iterations\n",
            "iteration: 107 \t total 625 iterations\n",
            "iteration: 108 \t total 625 iterations\n",
            "iteration: 109 \t total 625 iterations\n",
            "iteration: 110 \t total 625 iterations\n",
            "iteration: 111 \t total 625 iterations\n",
            "iteration: 112 \t total 625 iterations\n",
            "iteration: 113 \t total 625 iterations\n",
            "iteration: 114 \t total 625 iterations\n",
            "iteration: 115 \t total 625 iterations\n",
            "iteration: 116 \t total 625 iterations\n",
            "iteration: 117 \t total 625 iterations\n",
            "iteration: 118 \t total 625 iterations\n",
            "iteration: 119 \t total 625 iterations\n",
            "iteration: 120 \t total 625 iterations\n",
            "iteration: 121 \t total 625 iterations\n",
            "iteration: 122 \t total 625 iterations\n",
            "iteration: 123 \t total 625 iterations\n",
            "iteration: 124 \t total 625 iterations\n",
            "iteration: 125 \t total 625 iterations\n",
            "iteration: 126 \t total 625 iterations\n",
            "iteration: 127 \t total 625 iterations\n",
            "iteration: 128 \t total 625 iterations\n",
            "iteration: 129 \t total 625 iterations\n",
            "iteration: 130 \t total 625 iterations\n",
            "iteration: 131 \t total 625 iterations\n",
            "iteration: 132 \t total 625 iterations\n",
            "iteration: 133 \t total 625 iterations\n",
            "iteration: 134 \t total 625 iterations\n",
            "iteration: 135 \t total 625 iterations\n",
            "iteration: 136 \t total 625 iterations\n",
            "iteration: 137 \t total 625 iterations\n",
            "iteration: 138 \t total 625 iterations\n",
            "iteration: 139 \t total 625 iterations\n",
            "iteration: 140 \t total 625 iterations\n",
            "iteration: 141 \t total 625 iterations\n",
            "iteration: 142 \t total 625 iterations\n",
            "iteration: 143 \t total 625 iterations\n",
            "iteration: 144 \t total 625 iterations\n",
            "iteration: 145 \t total 625 iterations\n",
            "iteration: 146 \t total 625 iterations\n",
            "iteration: 147 \t total 625 iterations\n",
            "iteration: 148 \t total 625 iterations\n",
            "iteration: 149 \t total 625 iterations\n",
            "iteration: 150 \t total 625 iterations\n",
            "iteration: 151 \t total 625 iterations\n",
            "iteration: 152 \t total 625 iterations\n",
            "iteration: 153 \t total 625 iterations\n",
            "iteration: 154 \t total 625 iterations\n",
            "iteration: 155 \t total 625 iterations\n",
            "iteration: 156 \t total 625 iterations\n",
            "iteration: 157 \t total 625 iterations\n",
            "iteration: 158 \t total 625 iterations\n",
            "iteration: 159 \t total 625 iterations\n",
            "iteration: 160 \t total 625 iterations\n",
            "iteration: 161 \t total 625 iterations\n",
            "iteration: 162 \t total 625 iterations\n",
            "iteration: 163 \t total 625 iterations\n",
            "iteration: 164 \t total 625 iterations\n",
            "iteration: 165 \t total 625 iterations\n",
            "iteration: 166 \t total 625 iterations\n",
            "iteration: 167 \t total 625 iterations\n",
            "iteration: 168 \t total 625 iterations\n",
            "iteration: 169 \t total 625 iterations\n",
            "iteration: 170 \t total 625 iterations\n",
            "iteration: 171 \t total 625 iterations\n",
            "iteration: 172 \t total 625 iterations\n",
            "iteration: 173 \t total 625 iterations\n",
            "iteration: 174 \t total 625 iterations\n",
            "iteration: 175 \t total 625 iterations\n",
            "iteration: 176 \t total 625 iterations\n",
            "iteration: 177 \t total 625 iterations\n",
            "iteration: 178 \t total 625 iterations\n",
            "iteration: 179 \t total 625 iterations\n",
            "iteration: 180 \t total 625 iterations\n",
            "iteration: 181 \t total 625 iterations\n",
            "iteration: 182 \t total 625 iterations\n",
            "iteration: 183 \t total 625 iterations\n",
            "iteration: 184 \t total 625 iterations\n",
            "iteration: 185 \t total 625 iterations\n",
            "iteration: 186 \t total 625 iterations\n",
            "iteration: 187 \t total 625 iterations\n",
            "iteration: 188 \t total 625 iterations\n",
            "iteration: 189 \t total 625 iterations\n",
            "iteration: 190 \t total 625 iterations\n",
            "iteration: 191 \t total 625 iterations\n",
            "iteration: 192 \t total 625 iterations\n",
            "iteration: 193 \t total 625 iterations\n",
            "iteration: 194 \t total 625 iterations\n",
            "iteration: 195 \t total 625 iterations\n",
            "iteration: 196 \t total 625 iterations\n",
            "iteration: 197 \t total 625 iterations\n",
            "iteration: 198 \t total 625 iterations\n",
            "iteration: 199 \t total 625 iterations\n",
            "iteration: 200 \t total 625 iterations\n",
            "iteration: 201 \t total 625 iterations\n",
            "iteration: 202 \t total 625 iterations\n",
            "iteration: 203 \t total 625 iterations\n",
            "iteration: 204 \t total 625 iterations\n",
            "iteration: 205 \t total 625 iterations\n",
            "iteration: 206 \t total 625 iterations\n",
            "iteration: 207 \t total 625 iterations\n",
            "iteration: 208 \t total 625 iterations\n",
            "iteration: 209 \t total 625 iterations\n",
            "iteration: 210 \t total 625 iterations\n",
            "iteration: 211 \t total 625 iterations\n",
            "iteration: 212 \t total 625 iterations\n",
            "iteration: 213 \t total 625 iterations\n",
            "iteration: 214 \t total 625 iterations\n",
            "iteration: 215 \t total 625 iterations\n",
            "iteration: 216 \t total 625 iterations\n",
            "iteration: 217 \t total 625 iterations\n",
            "iteration: 218 \t total 625 iterations\n",
            "iteration: 219 \t total 625 iterations\n",
            "iteration: 220 \t total 625 iterations\n",
            "iteration: 221 \t total 625 iterations\n",
            "iteration: 222 \t total 625 iterations\n",
            "iteration: 223 \t total 625 iterations\n",
            "iteration: 224 \t total 625 iterations\n",
            "iteration: 225 \t total 625 iterations\n",
            "iteration: 226 \t total 625 iterations\n",
            "iteration: 227 \t total 625 iterations\n",
            "iteration: 228 \t total 625 iterations\n",
            "iteration: 229 \t total 625 iterations\n",
            "iteration: 230 \t total 625 iterations\n",
            "iteration: 231 \t total 625 iterations\n",
            "iteration: 232 \t total 625 iterations\n",
            "iteration: 233 \t total 625 iterations\n",
            "iteration: 234 \t total 625 iterations\n",
            "iteration: 235 \t total 625 iterations\n",
            "iteration: 236 \t total 625 iterations\n",
            "iteration: 237 \t total 625 iterations\n",
            "iteration: 238 \t total 625 iterations\n",
            "iteration: 239 \t total 625 iterations\n",
            "iteration: 240 \t total 625 iterations\n",
            "iteration: 241 \t total 625 iterations\n",
            "iteration: 242 \t total 625 iterations\n",
            "iteration: 243 \t total 625 iterations\n",
            "iteration: 244 \t total 625 iterations\n",
            "iteration: 245 \t total 625 iterations\n",
            "iteration: 246 \t total 625 iterations\n",
            "iteration: 247 \t total 625 iterations\n",
            "iteration: 248 \t total 625 iterations\n",
            "iteration: 249 \t total 625 iterations\n",
            "iteration: 250 \t total 625 iterations\n",
            "iteration: 251 \t total 625 iterations\n",
            "iteration: 252 \t total 625 iterations\n",
            "iteration: 253 \t total 625 iterations\n",
            "iteration: 254 \t total 625 iterations\n",
            "iteration: 255 \t total 625 iterations\n",
            "iteration: 256 \t total 625 iterations\n",
            "iteration: 257 \t total 625 iterations\n",
            "iteration: 258 \t total 625 iterations\n",
            "iteration: 259 \t total 625 iterations\n",
            "iteration: 260 \t total 625 iterations\n",
            "iteration: 261 \t total 625 iterations\n",
            "iteration: 262 \t total 625 iterations\n",
            "iteration: 263 \t total 625 iterations\n",
            "iteration: 264 \t total 625 iterations\n",
            "iteration: 265 \t total 625 iterations\n",
            "iteration: 266 \t total 625 iterations\n",
            "iteration: 267 \t total 625 iterations\n",
            "iteration: 268 \t total 625 iterations\n",
            "iteration: 269 \t total 625 iterations\n",
            "iteration: 270 \t total 625 iterations\n",
            "iteration: 271 \t total 625 iterations\n",
            "iteration: 272 \t total 625 iterations\n",
            "iteration: 273 \t total 625 iterations\n",
            "iteration: 274 \t total 625 iterations\n",
            "iteration: 275 \t total 625 iterations\n",
            "iteration: 276 \t total 625 iterations\n",
            "iteration: 277 \t total 625 iterations\n",
            "iteration: 278 \t total 625 iterations\n",
            "iteration: 279 \t total 625 iterations\n",
            "iteration: 280 \t total 625 iterations\n",
            "iteration: 281 \t total 625 iterations\n",
            "iteration: 282 \t total 625 iterations\n",
            "iteration: 283 \t total 625 iterations\n",
            "iteration: 284 \t total 625 iterations\n",
            "iteration: 285 \t total 625 iterations\n",
            "iteration: 286 \t total 625 iterations\n",
            "iteration: 287 \t total 625 iterations\n",
            "iteration: 288 \t total 625 iterations\n",
            "iteration: 289 \t total 625 iterations\n",
            "iteration: 290 \t total 625 iterations\n",
            "iteration: 291 \t total 625 iterations\n",
            "iteration: 292 \t total 625 iterations\n",
            "iteration: 293 \t total 625 iterations\n",
            "iteration: 294 \t total 625 iterations\n",
            "iteration: 295 \t total 625 iterations\n",
            "iteration: 296 \t total 625 iterations\n",
            "iteration: 297 \t total 625 iterations\n",
            "iteration: 298 \t total 625 iterations\n",
            "iteration: 299 \t total 625 iterations\n",
            "iteration: 300 \t total 625 iterations\n",
            "iteration: 301 \t total 625 iterations\n",
            "iteration: 302 \t total 625 iterations\n",
            "iteration: 303 \t total 625 iterations\n",
            "iteration: 304 \t total 625 iterations\n",
            "iteration: 305 \t total 625 iterations\n",
            "iteration: 306 \t total 625 iterations\n",
            "iteration: 307 \t total 625 iterations\n",
            "iteration: 308 \t total 625 iterations\n",
            "iteration: 309 \t total 625 iterations\n",
            "iteration: 310 \t total 625 iterations\n",
            "iteration: 311 \t total 625 iterations\n",
            "iteration: 312 \t total 625 iterations\n",
            "iteration: 313 \t total 625 iterations\n",
            "iteration: 314 \t total 625 iterations\n",
            "iteration: 315 \t total 625 iterations\n",
            "iteration: 316 \t total 625 iterations\n",
            "iteration: 317 \t total 625 iterations\n",
            "iteration: 318 \t total 625 iterations\n",
            "iteration: 319 \t total 625 iterations\n",
            "iteration: 320 \t total 625 iterations\n",
            "iteration: 321 \t total 625 iterations\n",
            "iteration: 322 \t total 625 iterations\n",
            "iteration: 323 \t total 625 iterations\n",
            "iteration: 324 \t total 625 iterations\n",
            "iteration: 325 \t total 625 iterations\n",
            "iteration: 326 \t total 625 iterations\n",
            "iteration: 327 \t total 625 iterations\n",
            "iteration: 328 \t total 625 iterations\n",
            "iteration: 329 \t total 625 iterations\n",
            "iteration: 330 \t total 625 iterations\n",
            "iteration: 331 \t total 625 iterations\n",
            "iteration: 332 \t total 625 iterations\n",
            "iteration: 333 \t total 625 iterations\n",
            "iteration: 334 \t total 625 iterations\n",
            "iteration: 335 \t total 625 iterations\n",
            "iteration: 336 \t total 625 iterations\n",
            "iteration: 337 \t total 625 iterations\n",
            "iteration: 338 \t total 625 iterations\n",
            "iteration: 339 \t total 625 iterations\n",
            "iteration: 340 \t total 625 iterations\n",
            "iteration: 341 \t total 625 iterations\n",
            "iteration: 342 \t total 625 iterations\n",
            "iteration: 343 \t total 625 iterations\n",
            "iteration: 344 \t total 625 iterations\n",
            "iteration: 345 \t total 625 iterations\n",
            "iteration: 346 \t total 625 iterations\n",
            "iteration: 347 \t total 625 iterations\n",
            "iteration: 348 \t total 625 iterations\n",
            "iteration: 349 \t total 625 iterations\n",
            "iteration: 350 \t total 625 iterations\n",
            "iteration: 351 \t total 625 iterations\n",
            "iteration: 352 \t total 625 iterations\n",
            "iteration: 353 \t total 625 iterations\n",
            "iteration: 354 \t total 625 iterations\n",
            "iteration: 355 \t total 625 iterations\n",
            "iteration: 356 \t total 625 iterations\n",
            "iteration: 357 \t total 625 iterations\n",
            "iteration: 358 \t total 625 iterations\n",
            "iteration: 359 \t total 625 iterations\n",
            "iteration: 360 \t total 625 iterations\n",
            "iteration: 361 \t total 625 iterations\n",
            "iteration: 362 \t total 625 iterations\n",
            "iteration: 363 \t total 625 iterations\n",
            "iteration: 364 \t total 625 iterations\n",
            "iteration: 365 \t total 625 iterations\n",
            "iteration: 366 \t total 625 iterations\n",
            "iteration: 367 \t total 625 iterations\n",
            "iteration: 368 \t total 625 iterations\n",
            "iteration: 369 \t total 625 iterations\n",
            "iteration: 370 \t total 625 iterations\n",
            "iteration: 371 \t total 625 iterations\n",
            "iteration: 372 \t total 625 iterations\n",
            "iteration: 373 \t total 625 iterations\n",
            "iteration: 374 \t total 625 iterations\n",
            "iteration: 375 \t total 625 iterations\n",
            "iteration: 376 \t total 625 iterations\n",
            "iteration: 377 \t total 625 iterations\n",
            "iteration: 378 \t total 625 iterations\n",
            "iteration: 379 \t total 625 iterations\n",
            "iteration: 380 \t total 625 iterations\n",
            "iteration: 381 \t total 625 iterations\n",
            "iteration: 382 \t total 625 iterations\n",
            "iteration: 383 \t total 625 iterations\n",
            "iteration: 384 \t total 625 iterations\n",
            "iteration: 385 \t total 625 iterations\n",
            "iteration: 386 \t total 625 iterations\n",
            "iteration: 387 \t total 625 iterations\n",
            "iteration: 388 \t total 625 iterations\n",
            "iteration: 389 \t total 625 iterations\n",
            "iteration: 390 \t total 625 iterations\n",
            "iteration: 391 \t total 625 iterations\n",
            "iteration: 392 \t total 625 iterations\n",
            "iteration: 393 \t total 625 iterations\n",
            "iteration: 394 \t total 625 iterations\n",
            "iteration: 395 \t total 625 iterations\n",
            "iteration: 396 \t total 625 iterations\n",
            "iteration: 397 \t total 625 iterations\n",
            "iteration: 398 \t total 625 iterations\n",
            "iteration: 399 \t total 625 iterations\n",
            "iteration: 400 \t total 625 iterations\n",
            "iteration: 401 \t total 625 iterations\n",
            "iteration: 402 \t total 625 iterations\n",
            "iteration: 403 \t total 625 iterations\n",
            "iteration: 404 \t total 625 iterations\n",
            "iteration: 405 \t total 625 iterations\n",
            "iteration: 406 \t total 625 iterations\n",
            "iteration: 407 \t total 625 iterations\n",
            "iteration: 408 \t total 625 iterations\n",
            "iteration: 409 \t total 625 iterations\n",
            "iteration: 410 \t total 625 iterations\n",
            "iteration: 411 \t total 625 iterations\n",
            "iteration: 412 \t total 625 iterations\n",
            "iteration: 413 \t total 625 iterations\n",
            "iteration: 414 \t total 625 iterations\n",
            "iteration: 415 \t total 625 iterations\n",
            "iteration: 416 \t total 625 iterations\n",
            "iteration: 417 \t total 625 iterations\n",
            "iteration: 418 \t total 625 iterations\n",
            "iteration: 419 \t total 625 iterations\n",
            "iteration: 420 \t total 625 iterations\n",
            "iteration: 421 \t total 625 iterations\n",
            "iteration: 422 \t total 625 iterations\n",
            "iteration: 423 \t total 625 iterations\n",
            "iteration: 424 \t total 625 iterations\n",
            "iteration: 425 \t total 625 iterations\n",
            "iteration: 426 \t total 625 iterations\n",
            "iteration: 427 \t total 625 iterations\n",
            "iteration: 428 \t total 625 iterations\n",
            "iteration: 429 \t total 625 iterations\n",
            "iteration: 430 \t total 625 iterations\n",
            "iteration: 431 \t total 625 iterations\n",
            "iteration: 432 \t total 625 iterations\n",
            "iteration: 433 \t total 625 iterations\n",
            "iteration: 434 \t total 625 iterations\n",
            "iteration: 435 \t total 625 iterations\n",
            "iteration: 436 \t total 625 iterations\n",
            "iteration: 437 \t total 625 iterations\n",
            "iteration: 438 \t total 625 iterations\n",
            "iteration: 439 \t total 625 iterations\n",
            "iteration: 440 \t total 625 iterations\n",
            "iteration: 441 \t total 625 iterations\n",
            "iteration: 442 \t total 625 iterations\n",
            "iteration: 443 \t total 625 iterations\n",
            "iteration: 444 \t total 625 iterations\n",
            "iteration: 445 \t total 625 iterations\n",
            "iteration: 446 \t total 625 iterations\n",
            "iteration: 447 \t total 625 iterations\n",
            "iteration: 448 \t total 625 iterations\n",
            "iteration: 449 \t total 625 iterations\n",
            "iteration: 450 \t total 625 iterations\n",
            "iteration: 451 \t total 625 iterations\n",
            "iteration: 452 \t total 625 iterations\n",
            "iteration: 453 \t total 625 iterations\n",
            "iteration: 454 \t total 625 iterations\n",
            "iteration: 455 \t total 625 iterations\n",
            "iteration: 456 \t total 625 iterations\n",
            "iteration: 457 \t total 625 iterations\n",
            "iteration: 458 \t total 625 iterations\n",
            "iteration: 459 \t total 625 iterations\n",
            "iteration: 460 \t total 625 iterations\n",
            "iteration: 461 \t total 625 iterations\n",
            "iteration: 462 \t total 625 iterations\n",
            "iteration: 463 \t total 625 iterations\n",
            "iteration: 464 \t total 625 iterations\n",
            "iteration: 465 \t total 625 iterations\n",
            "iteration: 466 \t total 625 iterations\n",
            "iteration: 467 \t total 625 iterations\n",
            "iteration: 468 \t total 625 iterations\n",
            "iteration: 469 \t total 625 iterations\n",
            "iteration: 470 \t total 625 iterations\n",
            "iteration: 471 \t total 625 iterations\n",
            "iteration: 472 \t total 625 iterations\n",
            "iteration: 473 \t total 625 iterations\n",
            "iteration: 474 \t total 625 iterations\n",
            "iteration: 475 \t total 625 iterations\n",
            "iteration: 476 \t total 625 iterations\n",
            "iteration: 477 \t total 625 iterations\n",
            "iteration: 478 \t total 625 iterations\n",
            "iteration: 479 \t total 625 iterations\n",
            "iteration: 480 \t total 625 iterations\n",
            "iteration: 481 \t total 625 iterations\n",
            "iteration: 482 \t total 625 iterations\n",
            "iteration: 483 \t total 625 iterations\n",
            "iteration: 484 \t total 625 iterations\n",
            "iteration: 485 \t total 625 iterations\n",
            "iteration: 486 \t total 625 iterations\n",
            "iteration: 487 \t total 625 iterations\n",
            "iteration: 488 \t total 625 iterations\n",
            "iteration: 489 \t total 625 iterations\n",
            "iteration: 490 \t total 625 iterations\n",
            "iteration: 491 \t total 625 iterations\n",
            "iteration: 492 \t total 625 iterations\n",
            "iteration: 493 \t total 625 iterations\n",
            "iteration: 494 \t total 625 iterations\n",
            "iteration: 495 \t total 625 iterations\n",
            "iteration: 496 \t total 625 iterations\n",
            "iteration: 497 \t total 625 iterations\n",
            "iteration: 498 \t total 625 iterations\n",
            "iteration: 499 \t total 625 iterations\n",
            "iteration: 500 \t total 625 iterations\n",
            "iteration: 501 \t total 625 iterations\n",
            "iteration: 502 \t total 625 iterations\n",
            "iteration: 503 \t total 625 iterations\n",
            "iteration: 504 \t total 625 iterations\n",
            "iteration: 505 \t total 625 iterations\n",
            "iteration: 506 \t total 625 iterations\n",
            "iteration: 507 \t total 625 iterations\n",
            "iteration: 508 \t total 625 iterations\n",
            "iteration: 509 \t total 625 iterations\n",
            "iteration: 510 \t total 625 iterations\n",
            "iteration: 511 \t total 625 iterations\n",
            "iteration: 512 \t total 625 iterations\n",
            "iteration: 513 \t total 625 iterations\n",
            "iteration: 514 \t total 625 iterations\n",
            "iteration: 515 \t total 625 iterations\n",
            "iteration: 516 \t total 625 iterations\n",
            "iteration: 517 \t total 625 iterations\n",
            "iteration: 518 \t total 625 iterations\n",
            "iteration: 519 \t total 625 iterations\n",
            "iteration: 520 \t total 625 iterations\n",
            "iteration: 521 \t total 625 iterations\n",
            "iteration: 522 \t total 625 iterations\n",
            "iteration: 523 \t total 625 iterations\n",
            "iteration: 524 \t total 625 iterations\n",
            "iteration: 525 \t total 625 iterations\n",
            "iteration: 526 \t total 625 iterations\n",
            "iteration: 527 \t total 625 iterations\n",
            "iteration: 528 \t total 625 iterations\n",
            "iteration: 529 \t total 625 iterations\n",
            "iteration: 530 \t total 625 iterations\n",
            "iteration: 531 \t total 625 iterations\n",
            "iteration: 532 \t total 625 iterations\n",
            "iteration: 533 \t total 625 iterations\n",
            "iteration: 534 \t total 625 iterations\n",
            "iteration: 535 \t total 625 iterations\n",
            "iteration: 536 \t total 625 iterations\n",
            "iteration: 537 \t total 625 iterations\n",
            "iteration: 538 \t total 625 iterations\n",
            "iteration: 539 \t total 625 iterations\n",
            "iteration: 540 \t total 625 iterations\n",
            "iteration: 541 \t total 625 iterations\n",
            "iteration: 542 \t total 625 iterations\n",
            "iteration: 543 \t total 625 iterations\n",
            "iteration: 544 \t total 625 iterations\n",
            "iteration: 545 \t total 625 iterations\n",
            "iteration: 546 \t total 625 iterations\n",
            "iteration: 547 \t total 625 iterations\n",
            "iteration: 548 \t total 625 iterations\n",
            "iteration: 549 \t total 625 iterations\n",
            "iteration: 550 \t total 625 iterations\n",
            "iteration: 551 \t total 625 iterations\n",
            "iteration: 552 \t total 625 iterations\n",
            "iteration: 553 \t total 625 iterations\n",
            "iteration: 554 \t total 625 iterations\n",
            "iteration: 555 \t total 625 iterations\n",
            "iteration: 556 \t total 625 iterations\n",
            "iteration: 557 \t total 625 iterations\n",
            "iteration: 558 \t total 625 iterations\n",
            "iteration: 559 \t total 625 iterations\n",
            "iteration: 560 \t total 625 iterations\n",
            "iteration: 561 \t total 625 iterations\n",
            "iteration: 562 \t total 625 iterations\n",
            "iteration: 563 \t total 625 iterations\n",
            "iteration: 564 \t total 625 iterations\n",
            "iteration: 565 \t total 625 iterations\n",
            "iteration: 566 \t total 625 iterations\n",
            "iteration: 567 \t total 625 iterations\n",
            "iteration: 568 \t total 625 iterations\n",
            "iteration: 569 \t total 625 iterations\n",
            "iteration: 570 \t total 625 iterations\n",
            "iteration: 571 \t total 625 iterations\n",
            "iteration: 572 \t total 625 iterations\n",
            "iteration: 573 \t total 625 iterations\n",
            "iteration: 574 \t total 625 iterations\n",
            "iteration: 575 \t total 625 iterations\n",
            "iteration: 576 \t total 625 iterations\n",
            "iteration: 577 \t total 625 iterations\n",
            "iteration: 578 \t total 625 iterations\n",
            "iteration: 579 \t total 625 iterations\n",
            "iteration: 580 \t total 625 iterations\n",
            "iteration: 581 \t total 625 iterations\n",
            "iteration: 582 \t total 625 iterations\n",
            "iteration: 583 \t total 625 iterations\n",
            "iteration: 584 \t total 625 iterations\n",
            "iteration: 585 \t total 625 iterations\n",
            "iteration: 586 \t total 625 iterations\n",
            "iteration: 587 \t total 625 iterations\n",
            "iteration: 588 \t total 625 iterations\n",
            "iteration: 589 \t total 625 iterations\n",
            "iteration: 590 \t total 625 iterations\n",
            "iteration: 591 \t total 625 iterations\n",
            "iteration: 592 \t total 625 iterations\n",
            "iteration: 593 \t total 625 iterations\n",
            "iteration: 594 \t total 625 iterations\n",
            "iteration: 595 \t total 625 iterations\n",
            "iteration: 596 \t total 625 iterations\n",
            "iteration: 597 \t total 625 iterations\n",
            "iteration: 598 \t total 625 iterations\n",
            "iteration: 599 \t total 625 iterations\n",
            "iteration: 600 \t total 625 iterations\n",
            "iteration: 601 \t total 625 iterations\n",
            "iteration: 602 \t total 625 iterations\n",
            "iteration: 603 \t total 625 iterations\n",
            "iteration: 604 \t total 625 iterations\n",
            "iteration: 605 \t total 625 iterations\n",
            "iteration: 606 \t total 625 iterations\n",
            "iteration: 607 \t total 625 iterations\n",
            "iteration: 608 \t total 625 iterations\n",
            "iteration: 609 \t total 625 iterations\n",
            "iteration: 610 \t total 625 iterations\n",
            "iteration: 611 \t total 625 iterations\n",
            "iteration: 612 \t total 625 iterations\n",
            "iteration: 613 \t total 625 iterations\n",
            "iteration: 614 \t total 625 iterations\n",
            "iteration: 615 \t total 625 iterations\n",
            "iteration: 616 \t total 625 iterations\n",
            "iteration: 617 \t total 625 iterations\n",
            "iteration: 618 \t total 625 iterations\n",
            "iteration: 619 \t total 625 iterations\n",
            "iteration: 620 \t total 625 iterations\n",
            "iteration: 621 \t total 625 iterations\n",
            "iteration: 622 \t total 625 iterations\n",
            "iteration: 623 \t total 625 iterations\n",
            "iteration: 624 \t total 625 iterations\n",
            "iteration: 625 \t total 625 iterations\n",
            "\n",
            "Parameter numbers: 34015396\n",
            "VGG(\n",
            "  34.02 M, 100.000% Params, 333.73 MMac, 99.877% MACs, \n",
            "  (features): Sequential(\n",
            "    14.72 M, 43.284% Params, 314.43 MMac, 94.101% MACs, \n",
            "    (0): Conv2d(1.79 k, 0.005% Params, 1.84 MMac, 0.549% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, 0.000% Params, 131.07 KMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(0, 0.000% Params, 65.54 KMac, 0.020% MACs, inplace=True)\n",
            "    (3): Conv2d(36.93 k, 0.109% Params, 37.81 MMac, 11.317% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(128, 0.000% Params, 131.07 KMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(0, 0.000% Params, 65.54 KMac, 0.020% MACs, inplace=True)\n",
            "    (6): MaxPool2d(0, 0.000% Params, 65.54 KMac, 0.020% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(73.86 k, 0.217% Params, 18.91 MMac, 5.658% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(256, 0.001% Params, 65.54 KMac, 0.020% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(0, 0.000% Params, 32.77 KMac, 0.010% MACs, inplace=True)\n",
            "    (10): Conv2d(147.58 k, 0.434% Params, 37.78 MMac, 11.307% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(256, 0.001% Params, 65.54 KMac, 0.020% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(0, 0.000% Params, 32.77 KMac, 0.010% MACs, inplace=True)\n",
            "    (13): MaxPool2d(0, 0.000% Params, 32.77 KMac, 0.010% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(295.17 k, 0.868% Params, 18.89 MMac, 5.654% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(512, 0.002% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (17): Conv2d(590.08 k, 1.735% Params, 37.77 MMac, 11.302% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(512, 0.002% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (20): Conv2d(590.08 k, 1.735% Params, 37.77 MMac, 11.302% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(512, 0.002% Params, 32.77 KMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(0, 0.000% Params, 16.38 KMac, 0.005% MACs, inplace=True)\n",
            "    (23): MaxPool2d(0, 0.000% Params, 16.38 KMac, 0.005% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(1.18 M, 3.469% Params, 18.88 MMac, 5.651% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(1.02 k, 0.003% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(0, 0.000% Params, 8.19 KMac, 0.002% MACs, inplace=True)\n",
            "    (27): Conv2d(2.36 M, 6.937% Params, 37.76 MMac, 11.300% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(1.02 k, 0.003% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(0, 0.000% Params, 8.19 KMac, 0.002% MACs, inplace=True)\n",
            "    (30): Conv2d(2.36 M, 6.937% Params, 37.76 MMac, 11.300% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(1.02 k, 0.003% Params, 16.38 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(0, 0.000% Params, 8.19 KMac, 0.002% MACs, inplace=True)\n",
            "    (33): MaxPool2d(0, 0.000% Params, 8.19 KMac, 0.002% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(2.36 M, 6.937% Params, 9.44 MMac, 2.825% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(1.02 k, 0.003% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (37): Conv2d(2.36 M, 6.937% Params, 9.44 MMac, 2.825% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(1.02 k, 0.003% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (40): Conv2d(2.36 M, 6.937% Params, 9.44 MMac, 2.825% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(1.02 k, 0.003% Params, 4.1 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(0, 0.000% Params, 2.05 KMac, 0.001% MACs, inplace=True)\n",
            "    (43): MaxPool2d(0, 0.000% Params, 2.05 KMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    19.29 M, 56.716% Params, 19.3 MMac, 5.776% MACs, \n",
            "    (0): Linear(2.1 M, 6.177% Params, 2.1 MMac, 0.629% MACs, in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, inplace=True)\n",
            "    (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
            "    (3): Linear(16.78 M, 49.334% Params, 16.78 MMac, 5.022% MACs, in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, inplace=True)\n",
            "    (5): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
            "    (6): Linear(409.7 k, 1.204% Params, 409.7 KMac, 0.123% MACs, in_features=4096, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
            "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
            "             ReLU-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
            "             ReLU-20            [-1, 256, 8, 8]               0\n",
            "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
            "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
            "             ReLU-23            [-1, 256, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-30            [-1, 512, 4, 4]               0\n",
            "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-33            [-1, 512, 4, 4]               0\n",
            "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
            "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-37            [-1, 512, 2, 2]               0\n",
            "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-40            [-1, 512, 2, 2]               0\n",
            "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-43            [-1, 512, 2, 2]               0\n",
            "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
            "           Linear-45                 [-1, 4096]       2,101,248\n",
            "             ReLU-46                 [-1, 4096]               0\n",
            "          Dropout-47                 [-1, 4096]               0\n",
            "           Linear-48                 [-1, 4096]      16,781,312\n",
            "             ReLU-49                 [-1, 4096]               0\n",
            "          Dropout-50                 [-1, 4096]               0\n",
            "           Linear-51                  [-1, 100]         409,700\n",
            "================================================================\n",
            "Total params: 34,015,396\n",
            "Trainable params: 34,015,396\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 6.75\n",
            "Params size (MB): 129.76\n",
            "Estimated Total Size (MB): 136.52\n",
            "----------------------------------------------------------------\n",
            "Top 1 err:  tensor(0.2777, device='cuda:0')\n",
            "Top 5 err:  tensor(0.1012, device='cuda:0')\n",
            "FLOPs: 334.14 MMac\n",
            "Params: 34.02 M\n",
            "Inference time: 6.7243 seconds\n",
            "Time per inference step: 10.7589 milliseconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}