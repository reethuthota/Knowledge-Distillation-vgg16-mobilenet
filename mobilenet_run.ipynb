{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTOzos77yjSL",
        "outputId": "1b88d17c-5ec5-4fe8-f4ad-5d9169f8a52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hysnL6UQyj1Y"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSpMt8_YmjKp",
        "outputId": "dbdc29b9-62ef-4cc1-d52e-0582c073f7b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Epoch: 189 [40832/50000]\tLoss: 0.1513\tLR: 0.000800\n",
            "Training Epoch: 189 [40960/50000]\tLoss: 0.1388\tLR: 0.000800\n",
            "Training Epoch: 189 [41088/50000]\tLoss: 0.1388\tLR: 0.000800\n",
            "Training Epoch: 189 [41216/50000]\tLoss: 0.0968\tLR: 0.000800\n",
            "Training Epoch: 189 [41344/50000]\tLoss: 0.2144\tLR: 0.000800\n",
            "Training Epoch: 189 [41472/50000]\tLoss: 0.1788\tLR: 0.000800\n",
            "Training Epoch: 189 [41600/50000]\tLoss: 0.1427\tLR: 0.000800\n",
            "Training Epoch: 189 [41728/50000]\tLoss: 0.1503\tLR: 0.000800\n",
            "Training Epoch: 189 [41856/50000]\tLoss: 0.1428\tLR: 0.000800\n",
            "Training Epoch: 189 [41984/50000]\tLoss: 0.1298\tLR: 0.000800\n",
            "Training Epoch: 189 [42112/50000]\tLoss: 0.1587\tLR: 0.000800\n",
            "Training Epoch: 189 [42240/50000]\tLoss: 0.1484\tLR: 0.000800\n",
            "Training Epoch: 189 [42368/50000]\tLoss: 0.1964\tLR: 0.000800\n",
            "Training Epoch: 189 [42496/50000]\tLoss: 0.1235\tLR: 0.000800\n",
            "Training Epoch: 189 [42624/50000]\tLoss: 0.1899\tLR: 0.000800\n",
            "Training Epoch: 189 [42752/50000]\tLoss: 0.1238\tLR: 0.000800\n",
            "Training Epoch: 189 [42880/50000]\tLoss: 0.1652\tLR: 0.000800\n",
            "Training Epoch: 189 [43008/50000]\tLoss: 0.1930\tLR: 0.000800\n",
            "Training Epoch: 189 [43136/50000]\tLoss: 0.1369\tLR: 0.000800\n",
            "Training Epoch: 189 [43264/50000]\tLoss: 0.0927\tLR: 0.000800\n",
            "Training Epoch: 189 [43392/50000]\tLoss: 0.1363\tLR: 0.000800\n",
            "Training Epoch: 189 [43520/50000]\tLoss: 0.1988\tLR: 0.000800\n",
            "Training Epoch: 189 [43648/50000]\tLoss: 0.1338\tLR: 0.000800\n",
            "Training Epoch: 189 [43776/50000]\tLoss: 0.1712\tLR: 0.000800\n",
            "Training Epoch: 189 [43904/50000]\tLoss: 0.1024\tLR: 0.000800\n",
            "Training Epoch: 189 [44032/50000]\tLoss: 0.1743\tLR: 0.000800\n",
            "Training Epoch: 189 [44160/50000]\tLoss: 0.1642\tLR: 0.000800\n",
            "Training Epoch: 189 [44288/50000]\tLoss: 0.0939\tLR: 0.000800\n",
            "Training Epoch: 189 [44416/50000]\tLoss: 0.1926\tLR: 0.000800\n",
            "Training Epoch: 189 [44544/50000]\tLoss: 0.1423\tLR: 0.000800\n",
            "Training Epoch: 189 [44672/50000]\tLoss: 0.1319\tLR: 0.000800\n",
            "Training Epoch: 189 [44800/50000]\tLoss: 0.1483\tLR: 0.000800\n",
            "Training Epoch: 189 [44928/50000]\tLoss: 0.1390\tLR: 0.000800\n",
            "Training Epoch: 189 [45056/50000]\tLoss: 0.1651\tLR: 0.000800\n",
            "Training Epoch: 189 [45184/50000]\tLoss: 0.1913\tLR: 0.000800\n",
            "Training Epoch: 189 [45312/50000]\tLoss: 0.1644\tLR: 0.000800\n",
            "Training Epoch: 189 [45440/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 189 [45568/50000]\tLoss: 0.1295\tLR: 0.000800\n",
            "Training Epoch: 189 [45696/50000]\tLoss: 0.1202\tLR: 0.000800\n",
            "Training Epoch: 189 [45824/50000]\tLoss: 0.2009\tLR: 0.000800\n",
            "Training Epoch: 189 [45952/50000]\tLoss: 0.1712\tLR: 0.000800\n",
            "Training Epoch: 189 [46080/50000]\tLoss: 0.1052\tLR: 0.000800\n",
            "Training Epoch: 189 [46208/50000]\tLoss: 0.1612\tLR: 0.000800\n",
            "Training Epoch: 189 [46336/50000]\tLoss: 0.1940\tLR: 0.000800\n",
            "Training Epoch: 189 [46464/50000]\tLoss: 0.1027\tLR: 0.000800\n",
            "Training Epoch: 189 [46592/50000]\tLoss: 0.2001\tLR: 0.000800\n",
            "Training Epoch: 189 [46720/50000]\tLoss: 0.1498\tLR: 0.000800\n",
            "Training Epoch: 189 [46848/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 189 [46976/50000]\tLoss: 0.0885\tLR: 0.000800\n",
            "Training Epoch: 189 [47104/50000]\tLoss: 0.1866\tLR: 0.000800\n",
            "Training Epoch: 189 [47232/50000]\tLoss: 0.0885\tLR: 0.000800\n",
            "Training Epoch: 189 [47360/50000]\tLoss: 0.1387\tLR: 0.000800\n",
            "Training Epoch: 189 [47488/50000]\tLoss: 0.1839\tLR: 0.000800\n",
            "Training Epoch: 189 [47616/50000]\tLoss: 0.1420\tLR: 0.000800\n",
            "Training Epoch: 189 [47744/50000]\tLoss: 0.2009\tLR: 0.000800\n",
            "Training Epoch: 189 [47872/50000]\tLoss: 0.1279\tLR: 0.000800\n",
            "Training Epoch: 189 [48000/50000]\tLoss: 0.1513\tLR: 0.000800\n",
            "Training Epoch: 189 [48128/50000]\tLoss: 0.1842\tLR: 0.000800\n",
            "Training Epoch: 189 [48256/50000]\tLoss: 0.1058\tLR: 0.000800\n",
            "Training Epoch: 189 [48384/50000]\tLoss: 0.1727\tLR: 0.000800\n",
            "Training Epoch: 189 [48512/50000]\tLoss: 0.1646\tLR: 0.000800\n",
            "Training Epoch: 189 [48640/50000]\tLoss: 0.1101\tLR: 0.000800\n",
            "Training Epoch: 189 [48768/50000]\tLoss: 0.1402\tLR: 0.000800\n",
            "Training Epoch: 189 [48896/50000]\tLoss: 0.1316\tLR: 0.000800\n",
            "Training Epoch: 189 [49024/50000]\tLoss: 0.1802\tLR: 0.000800\n",
            "Training Epoch: 189 [49152/50000]\tLoss: 0.1586\tLR: 0.000800\n",
            "Training Epoch: 189 [49280/50000]\tLoss: 0.1705\tLR: 0.000800\n",
            "Training Epoch: 189 [49408/50000]\tLoss: 0.2244\tLR: 0.000800\n",
            "Training Epoch: 189 [49536/50000]\tLoss: 0.1837\tLR: 0.000800\n",
            "Training Epoch: 189 [49664/50000]\tLoss: 0.1602\tLR: 0.000800\n",
            "Training Epoch: 189 [49792/50000]\tLoss: 0.1784\tLR: 0.000800\n",
            "Training Epoch: 189 [49920/50000]\tLoss: 0.1379\tLR: 0.000800\n",
            "Training Epoch: 189 [50000/50000]\tLoss: 0.2931\tLR: 0.000800\n",
            "epoch 189 training time consumed: 29.54s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  85054 GiB |  85054 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  83435 GiB |  83435 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1619 GiB |   1619 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  85054 GiB |  85054 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  83435 GiB |  83435 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1619 GiB |   1619 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  84801 GiB |  84801 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  83183 GiB |  83183 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1617 GiB |   1617 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  52037 GiB |  52037 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  50364 GiB |  50364 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1673 GiB |   1673 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   32956 K  |   32956 K  |\n",
            "|       from large pool |       8    |      61    |   11056 K  |   11056 K  |\n",
            "|       from small pool |     373    |     464    |   21900 K  |   21900 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   32956 K  |   32956 K  |\n",
            "|       from large pool |       8    |      61    |   11056 K  |   11056 K  |\n",
            "|       from small pool |     373    |     464    |   21900 K  |   21900 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   10835 K  |   10835 K  |\n",
            "|       from large pool |       5    |      14    |    5271 K  |    5271 K  |\n",
            "|       from small pool |      10    |      16    |    5564 K  |    5564 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 189, Average loss: 0.0113, Accuracy: 0.6653, Time consumed:3.78s\n",
            "\n",
            "Training Epoch: 190 [128/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 190 [256/50000]\tLoss: 0.1345\tLR: 0.000800\n",
            "Training Epoch: 190 [384/50000]\tLoss: 0.1502\tLR: 0.000800\n",
            "Training Epoch: 190 [512/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 190 [640/50000]\tLoss: 0.1213\tLR: 0.000800\n",
            "Training Epoch: 190 [768/50000]\tLoss: 0.0904\tLR: 0.000800\n",
            "Training Epoch: 190 [896/50000]\tLoss: 0.1357\tLR: 0.000800\n",
            "Training Epoch: 190 [1024/50000]\tLoss: 0.1159\tLR: 0.000800\n",
            "Training Epoch: 190 [1152/50000]\tLoss: 0.1356\tLR: 0.000800\n",
            "Training Epoch: 190 [1280/50000]\tLoss: 0.1756\tLR: 0.000800\n",
            "Training Epoch: 190 [1408/50000]\tLoss: 0.1830\tLR: 0.000800\n",
            "Training Epoch: 190 [1536/50000]\tLoss: 0.1009\tLR: 0.000800\n",
            "Training Epoch: 190 [1664/50000]\tLoss: 0.1129\tLR: 0.000800\n",
            "Training Epoch: 190 [1792/50000]\tLoss: 0.1339\tLR: 0.000800\n",
            "Training Epoch: 190 [1920/50000]\tLoss: 0.1454\tLR: 0.000800\n",
            "Training Epoch: 190 [2048/50000]\tLoss: 0.1905\tLR: 0.000800\n",
            "Training Epoch: 190 [2176/50000]\tLoss: 0.1986\tLR: 0.000800\n",
            "Training Epoch: 190 [2304/50000]\tLoss: 0.1717\tLR: 0.000800\n",
            "Training Epoch: 190 [2432/50000]\tLoss: 0.1282\tLR: 0.000800\n",
            "Training Epoch: 190 [2560/50000]\tLoss: 0.1532\tLR: 0.000800\n",
            "Training Epoch: 190 [2688/50000]\tLoss: 0.1570\tLR: 0.000800\n",
            "Training Epoch: 190 [2816/50000]\tLoss: 0.1221\tLR: 0.000800\n",
            "Training Epoch: 190 [2944/50000]\tLoss: 0.1023\tLR: 0.000800\n",
            "Training Epoch: 190 [3072/50000]\tLoss: 0.0998\tLR: 0.000800\n",
            "Training Epoch: 190 [3200/50000]\tLoss: 0.2061\tLR: 0.000800\n",
            "Training Epoch: 190 [3328/50000]\tLoss: 0.0927\tLR: 0.000800\n",
            "Training Epoch: 190 [3456/50000]\tLoss: 0.1583\tLR: 0.000800\n",
            "Training Epoch: 190 [3584/50000]\tLoss: 0.0868\tLR: 0.000800\n",
            "Training Epoch: 190 [3712/50000]\tLoss: 0.1655\tLR: 0.000800\n",
            "Training Epoch: 190 [3840/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 190 [3968/50000]\tLoss: 0.1567\tLR: 0.000800\n",
            "Training Epoch: 190 [4096/50000]\tLoss: 0.1259\tLR: 0.000800\n",
            "Training Epoch: 190 [4224/50000]\tLoss: 0.1164\tLR: 0.000800\n",
            "Training Epoch: 190 [4352/50000]\tLoss: 0.1627\tLR: 0.000800\n",
            "Training Epoch: 190 [4480/50000]\tLoss: 0.0995\tLR: 0.000800\n",
            "Training Epoch: 190 [4608/50000]\tLoss: 0.1630\tLR: 0.000800\n",
            "Training Epoch: 190 [4736/50000]\tLoss: 0.1393\tLR: 0.000800\n",
            "Training Epoch: 190 [4864/50000]\tLoss: 0.1529\tLR: 0.000800\n",
            "Training Epoch: 190 [4992/50000]\tLoss: 0.1257\tLR: 0.000800\n",
            "Training Epoch: 190 [5120/50000]\tLoss: 0.1300\tLR: 0.000800\n",
            "Training Epoch: 190 [5248/50000]\tLoss: 0.1293\tLR: 0.000800\n",
            "Training Epoch: 190 [5376/50000]\tLoss: 0.1451\tLR: 0.000800\n",
            "Training Epoch: 190 [5504/50000]\tLoss: 0.1444\tLR: 0.000800\n",
            "Training Epoch: 190 [5632/50000]\tLoss: 0.1351\tLR: 0.000800\n",
            "Training Epoch: 190 [5760/50000]\tLoss: 0.1483\tLR: 0.000800\n",
            "Training Epoch: 190 [5888/50000]\tLoss: 0.1121\tLR: 0.000800\n",
            "Training Epoch: 190 [6016/50000]\tLoss: 0.1654\tLR: 0.000800\n",
            "Training Epoch: 190 [6144/50000]\tLoss: 0.1136\tLR: 0.000800\n",
            "Training Epoch: 190 [6272/50000]\tLoss: 0.1162\tLR: 0.000800\n",
            "Training Epoch: 190 [6400/50000]\tLoss: 0.0798\tLR: 0.000800\n",
            "Training Epoch: 190 [6528/50000]\tLoss: 0.0987\tLR: 0.000800\n",
            "Training Epoch: 190 [6656/50000]\tLoss: 0.1982\tLR: 0.000800\n",
            "Training Epoch: 190 [6784/50000]\tLoss: 0.1388\tLR: 0.000800\n",
            "Training Epoch: 190 [6912/50000]\tLoss: 0.1100\tLR: 0.000800\n",
            "Training Epoch: 190 [7040/50000]\tLoss: 0.1611\tLR: 0.000800\n",
            "Training Epoch: 190 [7168/50000]\tLoss: 0.1803\tLR: 0.000800\n",
            "Training Epoch: 190 [7296/50000]\tLoss: 0.1719\tLR: 0.000800\n",
            "Training Epoch: 190 [7424/50000]\tLoss: 0.1882\tLR: 0.000800\n",
            "Training Epoch: 190 [7552/50000]\tLoss: 0.1718\tLR: 0.000800\n",
            "Training Epoch: 190 [7680/50000]\tLoss: 0.1311\tLR: 0.000800\n",
            "Training Epoch: 190 [7808/50000]\tLoss: 0.1754\tLR: 0.000800\n",
            "Training Epoch: 190 [7936/50000]\tLoss: 0.1370\tLR: 0.000800\n",
            "Training Epoch: 190 [8064/50000]\tLoss: 0.1335\tLR: 0.000800\n",
            "Training Epoch: 190 [8192/50000]\tLoss: 0.1081\tLR: 0.000800\n",
            "Training Epoch: 190 [8320/50000]\tLoss: 0.1863\tLR: 0.000800\n",
            "Training Epoch: 190 [8448/50000]\tLoss: 0.1818\tLR: 0.000800\n",
            "Training Epoch: 190 [8576/50000]\tLoss: 0.1406\tLR: 0.000800\n",
            "Training Epoch: 190 [8704/50000]\tLoss: 0.0770\tLR: 0.000800\n",
            "Training Epoch: 190 [8832/50000]\tLoss: 0.1514\tLR: 0.000800\n",
            "Training Epoch: 190 [8960/50000]\tLoss: 0.0939\tLR: 0.000800\n",
            "Training Epoch: 190 [9088/50000]\tLoss: 0.1195\tLR: 0.000800\n",
            "Training Epoch: 190 [9216/50000]\tLoss: 0.1535\tLR: 0.000800\n",
            "Training Epoch: 190 [9344/50000]\tLoss: 0.2099\tLR: 0.000800\n",
            "Training Epoch: 190 [9472/50000]\tLoss: 0.1781\tLR: 0.000800\n",
            "Training Epoch: 190 [9600/50000]\tLoss: 0.1198\tLR: 0.000800\n",
            "Training Epoch: 190 [9728/50000]\tLoss: 0.1836\tLR: 0.000800\n",
            "Training Epoch: 190 [9856/50000]\tLoss: 0.2097\tLR: 0.000800\n",
            "Training Epoch: 190 [9984/50000]\tLoss: 0.1596\tLR: 0.000800\n",
            "Training Epoch: 190 [10112/50000]\tLoss: 0.1092\tLR: 0.000800\n",
            "Training Epoch: 190 [10240/50000]\tLoss: 0.1874\tLR: 0.000800\n",
            "Training Epoch: 190 [10368/50000]\tLoss: 0.1694\tLR: 0.000800\n",
            "Training Epoch: 190 [10496/50000]\tLoss: 0.1355\tLR: 0.000800\n",
            "Training Epoch: 190 [10624/50000]\tLoss: 0.1195\tLR: 0.000800\n",
            "Training Epoch: 190 [10752/50000]\tLoss: 0.1514\tLR: 0.000800\n",
            "Training Epoch: 190 [10880/50000]\tLoss: 0.1529\tLR: 0.000800\n",
            "Training Epoch: 190 [11008/50000]\tLoss: 0.0794\tLR: 0.000800\n",
            "Training Epoch: 190 [11136/50000]\tLoss: 0.1865\tLR: 0.000800\n",
            "Training Epoch: 190 [11264/50000]\tLoss: 0.1580\tLR: 0.000800\n",
            "Training Epoch: 190 [11392/50000]\tLoss: 0.1583\tLR: 0.000800\n",
            "Training Epoch: 190 [11520/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 190 [11648/50000]\tLoss: 0.1657\tLR: 0.000800\n",
            "Training Epoch: 190 [11776/50000]\tLoss: 0.1601\tLR: 0.000800\n",
            "Training Epoch: 190 [11904/50000]\tLoss: 0.1125\tLR: 0.000800\n",
            "Training Epoch: 190 [12032/50000]\tLoss: 0.1808\tLR: 0.000800\n",
            "Training Epoch: 190 [12160/50000]\tLoss: 0.1145\tLR: 0.000800\n",
            "Training Epoch: 190 [12288/50000]\tLoss: 0.1392\tLR: 0.000800\n",
            "Training Epoch: 190 [12416/50000]\tLoss: 0.1241\tLR: 0.000800\n",
            "Training Epoch: 190 [12544/50000]\tLoss: 0.2371\tLR: 0.000800\n",
            "Training Epoch: 190 [12672/50000]\tLoss: 0.1817\tLR: 0.000800\n",
            "Training Epoch: 190 [12800/50000]\tLoss: 0.1299\tLR: 0.000800\n",
            "Training Epoch: 190 [12928/50000]\tLoss: 0.1336\tLR: 0.000800\n",
            "Training Epoch: 190 [13056/50000]\tLoss: 0.1382\tLR: 0.000800\n",
            "Training Epoch: 190 [13184/50000]\tLoss: 0.2410\tLR: 0.000800\n",
            "Training Epoch: 190 [13312/50000]\tLoss: 0.2139\tLR: 0.000800\n",
            "Training Epoch: 190 [13440/50000]\tLoss: 0.2135\tLR: 0.000800\n",
            "Training Epoch: 190 [13568/50000]\tLoss: 0.1602\tLR: 0.000800\n",
            "Training Epoch: 190 [13696/50000]\tLoss: 0.1506\tLR: 0.000800\n",
            "Training Epoch: 190 [13824/50000]\tLoss: 0.2524\tLR: 0.000800\n",
            "Training Epoch: 190 [13952/50000]\tLoss: 0.1462\tLR: 0.000800\n",
            "Training Epoch: 190 [14080/50000]\tLoss: 0.1374\tLR: 0.000800\n",
            "Training Epoch: 190 [14208/50000]\tLoss: 0.1573\tLR: 0.000800\n",
            "Training Epoch: 190 [14336/50000]\tLoss: 0.1441\tLR: 0.000800\n",
            "Training Epoch: 190 [14464/50000]\tLoss: 0.2208\tLR: 0.000800\n",
            "Training Epoch: 190 [14592/50000]\tLoss: 0.1609\tLR: 0.000800\n",
            "Training Epoch: 190 [14720/50000]\tLoss: 0.1349\tLR: 0.000800\n",
            "Training Epoch: 190 [14848/50000]\tLoss: 0.1126\tLR: 0.000800\n",
            "Training Epoch: 190 [14976/50000]\tLoss: 0.1125\tLR: 0.000800\n",
            "Training Epoch: 190 [15104/50000]\tLoss: 0.2029\tLR: 0.000800\n",
            "Training Epoch: 190 [15232/50000]\tLoss: 0.2035\tLR: 0.000800\n",
            "Training Epoch: 190 [15360/50000]\tLoss: 0.0697\tLR: 0.000800\n",
            "Training Epoch: 190 [15488/50000]\tLoss: 0.1237\tLR: 0.000800\n",
            "Training Epoch: 190 [15616/50000]\tLoss: 0.1150\tLR: 0.000800\n",
            "Training Epoch: 190 [15744/50000]\tLoss: 0.1213\tLR: 0.000800\n",
            "Training Epoch: 190 [15872/50000]\tLoss: 0.0882\tLR: 0.000800\n",
            "Training Epoch: 190 [16000/50000]\tLoss: 0.2112\tLR: 0.000800\n",
            "Training Epoch: 190 [16128/50000]\tLoss: 0.1043\tLR: 0.000800\n",
            "Training Epoch: 190 [16256/50000]\tLoss: 0.0929\tLR: 0.000800\n",
            "Training Epoch: 190 [16384/50000]\tLoss: 0.1523\tLR: 0.000800\n",
            "Training Epoch: 190 [16512/50000]\tLoss: 0.2308\tLR: 0.000800\n",
            "Training Epoch: 190 [16640/50000]\tLoss: 0.1166\tLR: 0.000800\n",
            "Training Epoch: 190 [16768/50000]\tLoss: 0.1238\tLR: 0.000800\n",
            "Training Epoch: 190 [16896/50000]\tLoss: 0.1779\tLR: 0.000800\n",
            "Training Epoch: 190 [17024/50000]\tLoss: 0.1256\tLR: 0.000800\n",
            "Training Epoch: 190 [17152/50000]\tLoss: 0.1995\tLR: 0.000800\n",
            "Training Epoch: 190 [17280/50000]\tLoss: 0.1867\tLR: 0.000800\n",
            "Training Epoch: 190 [17408/50000]\tLoss: 0.1957\tLR: 0.000800\n",
            "Training Epoch: 190 [17536/50000]\tLoss: 0.1107\tLR: 0.000800\n",
            "Training Epoch: 190 [17664/50000]\tLoss: 0.1194\tLR: 0.000800\n",
            "Training Epoch: 190 [17792/50000]\tLoss: 0.1927\tLR: 0.000800\n",
            "Training Epoch: 190 [17920/50000]\tLoss: 0.1625\tLR: 0.000800\n",
            "Training Epoch: 190 [18048/50000]\tLoss: 0.1285\tLR: 0.000800\n",
            "Training Epoch: 190 [18176/50000]\tLoss: 0.0684\tLR: 0.000800\n",
            "Training Epoch: 190 [18304/50000]\tLoss: 0.1221\tLR: 0.000800\n",
            "Training Epoch: 190 [18432/50000]\tLoss: 0.1682\tLR: 0.000800\n",
            "Training Epoch: 190 [18560/50000]\tLoss: 0.1232\tLR: 0.000800\n",
            "Training Epoch: 190 [18688/50000]\tLoss: 0.1621\tLR: 0.000800\n",
            "Training Epoch: 190 [18816/50000]\tLoss: 0.1316\tLR: 0.000800\n",
            "Training Epoch: 190 [18944/50000]\tLoss: 0.1078\tLR: 0.000800\n",
            "Training Epoch: 190 [19072/50000]\tLoss: 0.1089\tLR: 0.000800\n",
            "Training Epoch: 190 [19200/50000]\tLoss: 0.1296\tLR: 0.000800\n",
            "Training Epoch: 190 [19328/50000]\tLoss: 0.1048\tLR: 0.000800\n",
            "Training Epoch: 190 [19456/50000]\tLoss: 0.1158\tLR: 0.000800\n",
            "Training Epoch: 190 [19584/50000]\tLoss: 0.1741\tLR: 0.000800\n",
            "Training Epoch: 190 [19712/50000]\tLoss: 0.1256\tLR: 0.000800\n",
            "Training Epoch: 190 [19840/50000]\tLoss: 0.1163\tLR: 0.000800\n",
            "Training Epoch: 190 [19968/50000]\tLoss: 0.1658\tLR: 0.000800\n",
            "Training Epoch: 190 [20096/50000]\tLoss: 0.1822\tLR: 0.000800\n",
            "Training Epoch: 190 [20224/50000]\tLoss: 0.1447\tLR: 0.000800\n",
            "Training Epoch: 190 [20352/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 190 [20480/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 190 [20608/50000]\tLoss: 0.1033\tLR: 0.000800\n",
            "Training Epoch: 190 [20736/50000]\tLoss: 0.1945\tLR: 0.000800\n",
            "Training Epoch: 190 [20864/50000]\tLoss: 0.0968\tLR: 0.000800\n",
            "Training Epoch: 190 [20992/50000]\tLoss: 0.0982\tLR: 0.000800\n",
            "Training Epoch: 190 [21120/50000]\tLoss: 0.1540\tLR: 0.000800\n",
            "Training Epoch: 190 [21248/50000]\tLoss: 0.1930\tLR: 0.000800\n",
            "Training Epoch: 190 [21376/50000]\tLoss: 0.1878\tLR: 0.000800\n",
            "Training Epoch: 190 [21504/50000]\tLoss: 0.2065\tLR: 0.000800\n",
            "Training Epoch: 190 [21632/50000]\tLoss: 0.1508\tLR: 0.000800\n",
            "Training Epoch: 190 [21760/50000]\tLoss: 0.1752\tLR: 0.000800\n",
            "Training Epoch: 190 [21888/50000]\tLoss: 0.1175\tLR: 0.000800\n",
            "Training Epoch: 190 [22016/50000]\tLoss: 0.2055\tLR: 0.000800\n",
            "Training Epoch: 190 [22144/50000]\tLoss: 0.1633\tLR: 0.000800\n",
            "Training Epoch: 190 [22272/50000]\tLoss: 0.1536\tLR: 0.000800\n",
            "Training Epoch: 190 [22400/50000]\tLoss: 0.1898\tLR: 0.000800\n",
            "Training Epoch: 190 [22528/50000]\tLoss: 0.1815\tLR: 0.000800\n",
            "Training Epoch: 190 [22656/50000]\tLoss: 0.1433\tLR: 0.000800\n",
            "Training Epoch: 190 [22784/50000]\tLoss: 0.1308\tLR: 0.000800\n",
            "Training Epoch: 190 [22912/50000]\tLoss: 0.1327\tLR: 0.000800\n",
            "Training Epoch: 190 [23040/50000]\tLoss: 0.1311\tLR: 0.000800\n",
            "Training Epoch: 190 [23168/50000]\tLoss: 0.1643\tLR: 0.000800\n",
            "Training Epoch: 190 [23296/50000]\tLoss: 0.1190\tLR: 0.000800\n",
            "Training Epoch: 190 [23424/50000]\tLoss: 0.1946\tLR: 0.000800\n",
            "Training Epoch: 190 [23552/50000]\tLoss: 0.1993\tLR: 0.000800\n",
            "Training Epoch: 190 [23680/50000]\tLoss: 0.1425\tLR: 0.000800\n",
            "Training Epoch: 190 [23808/50000]\tLoss: 0.2237\tLR: 0.000800\n",
            "Training Epoch: 190 [23936/50000]\tLoss: 0.2190\tLR: 0.000800\n",
            "Training Epoch: 190 [24064/50000]\tLoss: 0.1335\tLR: 0.000800\n",
            "Training Epoch: 190 [24192/50000]\tLoss: 0.1513\tLR: 0.000800\n",
            "Training Epoch: 190 [24320/50000]\tLoss: 0.1263\tLR: 0.000800\n",
            "Training Epoch: 190 [24448/50000]\tLoss: 0.1710\tLR: 0.000800\n",
            "Training Epoch: 190 [24576/50000]\tLoss: 0.1656\tLR: 0.000800\n",
            "Training Epoch: 190 [24704/50000]\tLoss: 0.1439\tLR: 0.000800\n",
            "Training Epoch: 190 [24832/50000]\tLoss: 0.1468\tLR: 0.000800\n",
            "Training Epoch: 190 [24960/50000]\tLoss: 0.1595\tLR: 0.000800\n",
            "Training Epoch: 190 [25088/50000]\tLoss: 0.1719\tLR: 0.000800\n",
            "Training Epoch: 190 [25216/50000]\tLoss: 0.1894\tLR: 0.000800\n",
            "Training Epoch: 190 [25344/50000]\tLoss: 0.1576\tLR: 0.000800\n",
            "Training Epoch: 190 [25472/50000]\tLoss: 0.1110\tLR: 0.000800\n",
            "Training Epoch: 190 [25600/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 190 [25728/50000]\tLoss: 0.1524\tLR: 0.000800\n",
            "Training Epoch: 190 [25856/50000]\tLoss: 0.1079\tLR: 0.000800\n",
            "Training Epoch: 190 [25984/50000]\tLoss: 0.1766\tLR: 0.000800\n",
            "Training Epoch: 190 [26112/50000]\tLoss: 0.2249\tLR: 0.000800\n",
            "Training Epoch: 190 [26240/50000]\tLoss: 0.0953\tLR: 0.000800\n",
            "Training Epoch: 190 [26368/50000]\tLoss: 0.1739\tLR: 0.000800\n",
            "Training Epoch: 190 [26496/50000]\tLoss: 0.1995\tLR: 0.000800\n",
            "Training Epoch: 190 [26624/50000]\tLoss: 0.1121\tLR: 0.000800\n",
            "Training Epoch: 190 [26752/50000]\tLoss: 0.1296\tLR: 0.000800\n",
            "Training Epoch: 190 [26880/50000]\tLoss: 0.1904\tLR: 0.000800\n",
            "Training Epoch: 190 [27008/50000]\tLoss: 0.1226\tLR: 0.000800\n",
            "Training Epoch: 190 [27136/50000]\tLoss: 0.1486\tLR: 0.000800\n",
            "Training Epoch: 190 [27264/50000]\tLoss: 0.0948\tLR: 0.000800\n",
            "Training Epoch: 190 [27392/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 190 [27520/50000]\tLoss: 0.1304\tLR: 0.000800\n",
            "Training Epoch: 190 [27648/50000]\tLoss: 0.1284\tLR: 0.000800\n",
            "Training Epoch: 190 [27776/50000]\tLoss: 0.1237\tLR: 0.000800\n",
            "Training Epoch: 190 [27904/50000]\tLoss: 0.0692\tLR: 0.000800\n",
            "Training Epoch: 190 [28032/50000]\tLoss: 0.0889\tLR: 0.000800\n",
            "Training Epoch: 190 [28160/50000]\tLoss: 0.1895\tLR: 0.000800\n",
            "Training Epoch: 190 [28288/50000]\tLoss: 0.1534\tLR: 0.000800\n",
            "Training Epoch: 190 [28416/50000]\tLoss: 0.1560\tLR: 0.000800\n",
            "Training Epoch: 190 [28544/50000]\tLoss: 0.1171\tLR: 0.000800\n",
            "Training Epoch: 190 [28672/50000]\tLoss: 0.2192\tLR: 0.000800\n",
            "Training Epoch: 190 [28800/50000]\tLoss: 0.2555\tLR: 0.000800\n",
            "Training Epoch: 190 [28928/50000]\tLoss: 0.1137\tLR: 0.000800\n",
            "Training Epoch: 190 [29056/50000]\tLoss: 0.1484\tLR: 0.000800\n",
            "Training Epoch: 190 [29184/50000]\tLoss: 0.1139\tLR: 0.000800\n",
            "Training Epoch: 190 [29312/50000]\tLoss: 0.1548\tLR: 0.000800\n",
            "Training Epoch: 190 [29440/50000]\tLoss: 0.1584\tLR: 0.000800\n",
            "Training Epoch: 190 [29568/50000]\tLoss: 0.1809\tLR: 0.000800\n",
            "Training Epoch: 190 [29696/50000]\tLoss: 0.1566\tLR: 0.000800\n",
            "Training Epoch: 190 [29824/50000]\tLoss: 0.1331\tLR: 0.000800\n",
            "Training Epoch: 190 [29952/50000]\tLoss: 0.1566\tLR: 0.000800\n",
            "Training Epoch: 190 [30080/50000]\tLoss: 0.1069\tLR: 0.000800\n",
            "Training Epoch: 190 [30208/50000]\tLoss: 0.1337\tLR: 0.000800\n",
            "Training Epoch: 190 [30336/50000]\tLoss: 0.1479\tLR: 0.000800\n",
            "Training Epoch: 190 [30464/50000]\tLoss: 0.1308\tLR: 0.000800\n",
            "Training Epoch: 190 [30592/50000]\tLoss: 0.1888\tLR: 0.000800\n",
            "Training Epoch: 190 [30720/50000]\tLoss: 0.1575\tLR: 0.000800\n",
            "Training Epoch: 190 [30848/50000]\tLoss: 0.1456\tLR: 0.000800\n",
            "Training Epoch: 190 [30976/50000]\tLoss: 0.1583\tLR: 0.000800\n",
            "Training Epoch: 190 [31104/50000]\tLoss: 0.1084\tLR: 0.000800\n",
            "Training Epoch: 190 [31232/50000]\tLoss: 0.1749\tLR: 0.000800\n",
            "Training Epoch: 190 [31360/50000]\tLoss: 0.1488\tLR: 0.000800\n",
            "Training Epoch: 190 [31488/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 190 [31616/50000]\tLoss: 0.1807\tLR: 0.000800\n",
            "Training Epoch: 190 [31744/50000]\tLoss: 0.1424\tLR: 0.000800\n",
            "Training Epoch: 190 [31872/50000]\tLoss: 0.2030\tLR: 0.000800\n",
            "Training Epoch: 190 [32000/50000]\tLoss: 0.2243\tLR: 0.000800\n",
            "Training Epoch: 190 [32128/50000]\tLoss: 0.1768\tLR: 0.000800\n",
            "Training Epoch: 190 [32256/50000]\tLoss: 0.1839\tLR: 0.000800\n",
            "Training Epoch: 190 [32384/50000]\tLoss: 0.1619\tLR: 0.000800\n",
            "Training Epoch: 190 [32512/50000]\tLoss: 0.1815\tLR: 0.000800\n",
            "Training Epoch: 190 [32640/50000]\tLoss: 0.1199\tLR: 0.000800\n",
            "Training Epoch: 190 [32768/50000]\tLoss: 0.0685\tLR: 0.000800\n",
            "Training Epoch: 190 [32896/50000]\tLoss: 0.1723\tLR: 0.000800\n",
            "Training Epoch: 190 [33024/50000]\tLoss: 0.1258\tLR: 0.000800\n",
            "Training Epoch: 190 [33152/50000]\tLoss: 0.2682\tLR: 0.000800\n",
            "Training Epoch: 190 [33280/50000]\tLoss: 0.1404\tLR: 0.000800\n",
            "Training Epoch: 190 [33408/50000]\tLoss: 0.0957\tLR: 0.000800\n",
            "Training Epoch: 190 [33536/50000]\tLoss: 0.0935\tLR: 0.000800\n",
            "Training Epoch: 190 [33664/50000]\tLoss: 0.1032\tLR: 0.000800\n",
            "Training Epoch: 190 [33792/50000]\tLoss: 0.1396\tLR: 0.000800\n",
            "Training Epoch: 190 [33920/50000]\tLoss: 0.1613\tLR: 0.000800\n",
            "Training Epoch: 190 [34048/50000]\tLoss: 0.1807\tLR: 0.000800\n",
            "Training Epoch: 190 [34176/50000]\tLoss: 0.1436\tLR: 0.000800\n",
            "Training Epoch: 190 [34304/50000]\tLoss: 0.1104\tLR: 0.000800\n",
            "Training Epoch: 190 [34432/50000]\tLoss: 0.1525\tLR: 0.000800\n",
            "Training Epoch: 190 [34560/50000]\tLoss: 0.1142\tLR: 0.000800\n",
            "Training Epoch: 190 [34688/50000]\tLoss: 0.1398\tLR: 0.000800\n",
            "Training Epoch: 190 [34816/50000]\tLoss: 0.2010\tLR: 0.000800\n",
            "Training Epoch: 190 [34944/50000]\tLoss: 0.0928\tLR: 0.000800\n",
            "Training Epoch: 190 [35072/50000]\tLoss: 0.1109\tLR: 0.000800\n",
            "Training Epoch: 190 [35200/50000]\tLoss: 0.1207\tLR: 0.000800\n",
            "Training Epoch: 190 [35328/50000]\tLoss: 0.1228\tLR: 0.000800\n",
            "Training Epoch: 190 [35456/50000]\tLoss: 0.1191\tLR: 0.000800\n",
            "Training Epoch: 190 [35584/50000]\tLoss: 0.1898\tLR: 0.000800\n",
            "Training Epoch: 190 [35712/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 190 [35840/50000]\tLoss: 0.2126\tLR: 0.000800\n",
            "Training Epoch: 190 [35968/50000]\tLoss: 0.1260\tLR: 0.000800\n",
            "Training Epoch: 190 [36096/50000]\tLoss: 0.1582\tLR: 0.000800\n",
            "Training Epoch: 190 [36224/50000]\tLoss: 0.1662\tLR: 0.000800\n",
            "Training Epoch: 190 [36352/50000]\tLoss: 0.1441\tLR: 0.000800\n",
            "Training Epoch: 190 [36480/50000]\tLoss: 0.1040\tLR: 0.000800\n",
            "Training Epoch: 190 [36608/50000]\tLoss: 0.1554\tLR: 0.000800\n",
            "Training Epoch: 190 [36736/50000]\tLoss: 0.1228\tLR: 0.000800\n",
            "Training Epoch: 190 [36864/50000]\tLoss: 0.1299\tLR: 0.000800\n",
            "Training Epoch: 190 [36992/50000]\tLoss: 0.1259\tLR: 0.000800\n",
            "Training Epoch: 190 [37120/50000]\tLoss: 0.1705\tLR: 0.000800\n",
            "Training Epoch: 190 [37248/50000]\tLoss: 0.0982\tLR: 0.000800\n",
            "Training Epoch: 190 [37376/50000]\tLoss: 0.1353\tLR: 0.000800\n",
            "Training Epoch: 190 [37504/50000]\tLoss: 0.2133\tLR: 0.000800\n",
            "Training Epoch: 190 [37632/50000]\tLoss: 0.1230\tLR: 0.000800\n",
            "Training Epoch: 190 [37760/50000]\tLoss: 0.2665\tLR: 0.000800\n",
            "Training Epoch: 190 [37888/50000]\tLoss: 0.0907\tLR: 0.000800\n",
            "Training Epoch: 190 [38016/50000]\tLoss: 0.1559\tLR: 0.000800\n",
            "Training Epoch: 190 [38144/50000]\tLoss: 0.1808\tLR: 0.000800\n",
            "Training Epoch: 190 [38272/50000]\tLoss: 0.1415\tLR: 0.000800\n",
            "Training Epoch: 190 [38400/50000]\tLoss: 0.1620\tLR: 0.000800\n",
            "Training Epoch: 190 [38528/50000]\tLoss: 0.1445\tLR: 0.000800\n",
            "Training Epoch: 190 [38656/50000]\tLoss: 0.1623\tLR: 0.000800\n",
            "Training Epoch: 190 [38784/50000]\tLoss: 0.1822\tLR: 0.000800\n",
            "Training Epoch: 190 [38912/50000]\tLoss: 0.1442\tLR: 0.000800\n",
            "Training Epoch: 190 [39040/50000]\tLoss: 0.2255\tLR: 0.000800\n",
            "Training Epoch: 190 [39168/50000]\tLoss: 0.1924\tLR: 0.000800\n",
            "Training Epoch: 190 [39296/50000]\tLoss: 0.1168\tLR: 0.000800\n",
            "Training Epoch: 190 [39424/50000]\tLoss: 0.1628\tLR: 0.000800\n",
            "Training Epoch: 190 [39552/50000]\tLoss: 0.1474\tLR: 0.000800\n",
            "Training Epoch: 190 [39680/50000]\tLoss: 0.1373\tLR: 0.000800\n",
            "Training Epoch: 190 [39808/50000]\tLoss: 0.1639\tLR: 0.000800\n",
            "Training Epoch: 190 [39936/50000]\tLoss: 0.1641\tLR: 0.000800\n",
            "Training Epoch: 190 [40064/50000]\tLoss: 0.1651\tLR: 0.000800\n",
            "Training Epoch: 190 [40192/50000]\tLoss: 0.1016\tLR: 0.000800\n",
            "Training Epoch: 190 [40320/50000]\tLoss: 0.0981\tLR: 0.000800\n",
            "Training Epoch: 190 [40448/50000]\tLoss: 0.2077\tLR: 0.000800\n",
            "Training Epoch: 190 [40576/50000]\tLoss: 0.2104\tLR: 0.000800\n",
            "Training Epoch: 190 [40704/50000]\tLoss: 0.1502\tLR: 0.000800\n",
            "Training Epoch: 190 [40832/50000]\tLoss: 0.1360\tLR: 0.000800\n",
            "Training Epoch: 190 [40960/50000]\tLoss: 0.1523\tLR: 0.000800\n",
            "Training Epoch: 190 [41088/50000]\tLoss: 0.1288\tLR: 0.000800\n",
            "Training Epoch: 190 [41216/50000]\tLoss: 0.1582\tLR: 0.000800\n",
            "Training Epoch: 190 [41344/50000]\tLoss: 0.1603\tLR: 0.000800\n",
            "Training Epoch: 190 [41472/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 190 [41600/50000]\tLoss: 0.1280\tLR: 0.000800\n",
            "Training Epoch: 190 [41728/50000]\tLoss: 0.2017\tLR: 0.000800\n",
            "Training Epoch: 190 [41856/50000]\tLoss: 0.1130\tLR: 0.000800\n",
            "Training Epoch: 190 [41984/50000]\tLoss: 0.1345\tLR: 0.000800\n",
            "Training Epoch: 190 [42112/50000]\tLoss: 0.1331\tLR: 0.000800\n",
            "Training Epoch: 190 [42240/50000]\tLoss: 0.1419\tLR: 0.000800\n",
            "Training Epoch: 190 [42368/50000]\tLoss: 0.1830\tLR: 0.000800\n",
            "Training Epoch: 190 [42496/50000]\tLoss: 0.1422\tLR: 0.000800\n",
            "Training Epoch: 190 [42624/50000]\tLoss: 0.1513\tLR: 0.000800\n",
            "Training Epoch: 190 [42752/50000]\tLoss: 0.1612\tLR: 0.000800\n",
            "Training Epoch: 190 [42880/50000]\tLoss: 0.1119\tLR: 0.000800\n",
            "Training Epoch: 190 [43008/50000]\tLoss: 0.1214\tLR: 0.000800\n",
            "Training Epoch: 190 [43136/50000]\tLoss: 0.1470\tLR: 0.000800\n",
            "Training Epoch: 190 [43264/50000]\tLoss: 0.1452\tLR: 0.000800\n",
            "Training Epoch: 190 [43392/50000]\tLoss: 0.1789\tLR: 0.000800\n",
            "Training Epoch: 190 [43520/50000]\tLoss: 0.1507\tLR: 0.000800\n",
            "Training Epoch: 190 [43648/50000]\tLoss: 0.1701\tLR: 0.000800\n",
            "Training Epoch: 190 [43776/50000]\tLoss: 0.1982\tLR: 0.000800\n",
            "Training Epoch: 190 [43904/50000]\tLoss: 0.2026\tLR: 0.000800\n",
            "Training Epoch: 190 [44032/50000]\tLoss: 0.1968\tLR: 0.000800\n",
            "Training Epoch: 190 [44160/50000]\tLoss: 0.1214\tLR: 0.000800\n",
            "Training Epoch: 190 [44288/50000]\tLoss: 0.1589\tLR: 0.000800\n",
            "Training Epoch: 190 [44416/50000]\tLoss: 0.1473\tLR: 0.000800\n",
            "Training Epoch: 190 [44544/50000]\tLoss: 0.1178\tLR: 0.000800\n",
            "Training Epoch: 190 [44672/50000]\tLoss: 0.1751\tLR: 0.000800\n",
            "Training Epoch: 190 [44800/50000]\tLoss: 0.0901\tLR: 0.000800\n",
            "Training Epoch: 190 [44928/50000]\tLoss: 0.1205\tLR: 0.000800\n",
            "Training Epoch: 190 [45056/50000]\tLoss: 0.1684\tLR: 0.000800\n",
            "Training Epoch: 190 [45184/50000]\tLoss: 0.1320\tLR: 0.000800\n",
            "Training Epoch: 190 [45312/50000]\tLoss: 0.1920\tLR: 0.000800\n",
            "Training Epoch: 190 [45440/50000]\tLoss: 0.1859\tLR: 0.000800\n",
            "Training Epoch: 190 [45568/50000]\tLoss: 0.1271\tLR: 0.000800\n",
            "Training Epoch: 190 [45696/50000]\tLoss: 0.1640\tLR: 0.000800\n",
            "Training Epoch: 190 [45824/50000]\tLoss: 0.1507\tLR: 0.000800\n",
            "Training Epoch: 190 [45952/50000]\tLoss: 0.1254\tLR: 0.000800\n",
            "Training Epoch: 190 [46080/50000]\tLoss: 0.1350\tLR: 0.000800\n",
            "Training Epoch: 190 [46208/50000]\tLoss: 0.1466\tLR: 0.000800\n",
            "Training Epoch: 190 [46336/50000]\tLoss: 0.1378\tLR: 0.000800\n",
            "Training Epoch: 190 [46464/50000]\tLoss: 0.1803\tLR: 0.000800\n",
            "Training Epoch: 190 [46592/50000]\tLoss: 0.1953\tLR: 0.000800\n",
            "Training Epoch: 190 [46720/50000]\tLoss: 0.1442\tLR: 0.000800\n",
            "Training Epoch: 190 [46848/50000]\tLoss: 0.2491\tLR: 0.000800\n",
            "Training Epoch: 190 [46976/50000]\tLoss: 0.1997\tLR: 0.000800\n",
            "Training Epoch: 190 [47104/50000]\tLoss: 0.1150\tLR: 0.000800\n",
            "Training Epoch: 190 [47232/50000]\tLoss: 0.1629\tLR: 0.000800\n",
            "Training Epoch: 190 [47360/50000]\tLoss: 0.1111\tLR: 0.000800\n",
            "Training Epoch: 190 [47488/50000]\tLoss: 0.1472\tLR: 0.000800\n",
            "Training Epoch: 190 [47616/50000]\tLoss: 0.0936\tLR: 0.000800\n",
            "Training Epoch: 190 [47744/50000]\tLoss: 0.2294\tLR: 0.000800\n",
            "Training Epoch: 190 [47872/50000]\tLoss: 0.1113\tLR: 0.000800\n",
            "Training Epoch: 190 [48000/50000]\tLoss: 0.1277\tLR: 0.000800\n",
            "Training Epoch: 190 [48128/50000]\tLoss: 0.1098\tLR: 0.000800\n",
            "Training Epoch: 190 [48256/50000]\tLoss: 0.1225\tLR: 0.000800\n",
            "Training Epoch: 190 [48384/50000]\tLoss: 0.1257\tLR: 0.000800\n",
            "Training Epoch: 190 [48512/50000]\tLoss: 0.1508\tLR: 0.000800\n",
            "Training Epoch: 190 [48640/50000]\tLoss: 0.2896\tLR: 0.000800\n",
            "Training Epoch: 190 [48768/50000]\tLoss: 0.1649\tLR: 0.000800\n",
            "Training Epoch: 190 [48896/50000]\tLoss: 0.1149\tLR: 0.000800\n",
            "Training Epoch: 190 [49024/50000]\tLoss: 0.1367\tLR: 0.000800\n",
            "Training Epoch: 190 [49152/50000]\tLoss: 0.1398\tLR: 0.000800\n",
            "Training Epoch: 190 [49280/50000]\tLoss: 0.1863\tLR: 0.000800\n",
            "Training Epoch: 190 [49408/50000]\tLoss: 0.1592\tLR: 0.000800\n",
            "Training Epoch: 190 [49536/50000]\tLoss: 0.2099\tLR: 0.000800\n",
            "Training Epoch: 190 [49664/50000]\tLoss: 0.0901\tLR: 0.000800\n",
            "Training Epoch: 190 [49792/50000]\tLoss: 0.0920\tLR: 0.000800\n",
            "Training Epoch: 190 [49920/50000]\tLoss: 0.1266\tLR: 0.000800\n",
            "Training Epoch: 190 [50000/50000]\tLoss: 0.1714\tLR: 0.000800\n",
            "epoch 190 training time consumed: 28.72s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  85504 GiB |  85504 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  83876 GiB |  83876 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1627 GiB |   1627 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  85504 GiB |  85504 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  83876 GiB |  83876 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1627 GiB |   1627 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  85250 GiB |  85250 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  83624 GiB |  83624 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1626 GiB |   1626 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  52312 GiB |  52312 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  50630 GiB |  50630 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1682 GiB |   1682 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   33131 K  |   33130 K  |\n",
            "|       from large pool |       8    |      61    |   11114 K  |   11114 K  |\n",
            "|       from small pool |     373    |     464    |   22016 K  |   22016 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   33131 K  |   33130 K  |\n",
            "|       from large pool |       8    |      61    |   11114 K  |   11114 K  |\n",
            "|       from small pool |     373    |     464    |   22016 K  |   22016 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   10893 K  |   10893 K  |\n",
            "|       from large pool |       5    |      14    |    5299 K  |    5299 K  |\n",
            "|       from small pool |      10    |      16    |    5594 K  |    5594 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 190, Average loss: 0.0112, Accuracy: 0.6627, Time consumed:2.76s\n",
            "\n",
            "saving weights file to checkpoint/mobilenet/Wednesday_01_May_2024_19h_23m_52s/mobilenet-190-regular.pth\n",
            "Training Epoch: 191 [128/50000]\tLoss: 0.1056\tLR: 0.000800\n",
            "Training Epoch: 191 [256/50000]\tLoss: 0.1489\tLR: 0.000800\n",
            "Training Epoch: 191 [384/50000]\tLoss: 0.1596\tLR: 0.000800\n",
            "Training Epoch: 191 [512/50000]\tLoss: 0.0823\tLR: 0.000800\n",
            "Training Epoch: 191 [640/50000]\tLoss: 0.2129\tLR: 0.000800\n",
            "Training Epoch: 191 [768/50000]\tLoss: 0.1645\tLR: 0.000800\n",
            "Training Epoch: 191 [896/50000]\tLoss: 0.1786\tLR: 0.000800\n",
            "Training Epoch: 191 [1024/50000]\tLoss: 0.0767\tLR: 0.000800\n",
            "Training Epoch: 191 [1152/50000]\tLoss: 0.1584\tLR: 0.000800\n",
            "Training Epoch: 191 [1280/50000]\tLoss: 0.1628\tLR: 0.000800\n",
            "Training Epoch: 191 [1408/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 191 [1536/50000]\tLoss: 0.1304\tLR: 0.000800\n",
            "Training Epoch: 191 [1664/50000]\tLoss: 0.1559\tLR: 0.000800\n",
            "Training Epoch: 191 [1792/50000]\tLoss: 0.1125\tLR: 0.000800\n",
            "Training Epoch: 191 [1920/50000]\tLoss: 0.1487\tLR: 0.000800\n",
            "Training Epoch: 191 [2048/50000]\tLoss: 0.1652\tLR: 0.000800\n",
            "Training Epoch: 191 [2176/50000]\tLoss: 0.1140\tLR: 0.000800\n",
            "Training Epoch: 191 [2304/50000]\tLoss: 0.1632\tLR: 0.000800\n",
            "Training Epoch: 191 [2432/50000]\tLoss: 0.1898\tLR: 0.000800\n",
            "Training Epoch: 191 [2560/50000]\tLoss: 0.1630\tLR: 0.000800\n",
            "Training Epoch: 191 [2688/50000]\tLoss: 0.1063\tLR: 0.000800\n",
            "Training Epoch: 191 [2816/50000]\tLoss: 0.1417\tLR: 0.000800\n",
            "Training Epoch: 191 [2944/50000]\tLoss: 0.1693\tLR: 0.000800\n",
            "Training Epoch: 191 [3072/50000]\tLoss: 0.1326\tLR: 0.000800\n",
            "Training Epoch: 191 [3200/50000]\tLoss: 0.1393\tLR: 0.000800\n",
            "Training Epoch: 191 [3328/50000]\tLoss: 0.1332\tLR: 0.000800\n",
            "Training Epoch: 191 [3456/50000]\tLoss: 0.2091\tLR: 0.000800\n",
            "Training Epoch: 191 [3584/50000]\tLoss: 0.0932\tLR: 0.000800\n",
            "Training Epoch: 191 [3712/50000]\tLoss: 0.1233\tLR: 0.000800\n",
            "Training Epoch: 191 [3840/50000]\tLoss: 0.1515\tLR: 0.000800\n",
            "Training Epoch: 191 [3968/50000]\tLoss: 0.1299\tLR: 0.000800\n",
            "Training Epoch: 191 [4096/50000]\tLoss: 0.1083\tLR: 0.000800\n",
            "Training Epoch: 191 [4224/50000]\tLoss: 0.1266\tLR: 0.000800\n",
            "Training Epoch: 191 [4352/50000]\tLoss: 0.1349\tLR: 0.000800\n",
            "Training Epoch: 191 [4480/50000]\tLoss: 0.1409\tLR: 0.000800\n",
            "Training Epoch: 191 [4608/50000]\tLoss: 0.1364\tLR: 0.000800\n",
            "Training Epoch: 191 [4736/50000]\tLoss: 0.1063\tLR: 0.000800\n",
            "Training Epoch: 191 [4864/50000]\tLoss: 0.1353\tLR: 0.000800\n",
            "Training Epoch: 191 [4992/50000]\tLoss: 0.1855\tLR: 0.000800\n",
            "Training Epoch: 191 [5120/50000]\tLoss: 0.1728\tLR: 0.000800\n",
            "Training Epoch: 191 [5248/50000]\tLoss: 0.1185\tLR: 0.000800\n",
            "Training Epoch: 191 [5376/50000]\tLoss: 0.1062\tLR: 0.000800\n",
            "Training Epoch: 191 [5504/50000]\tLoss: 0.1499\tLR: 0.000800\n",
            "Training Epoch: 191 [5632/50000]\tLoss: 0.1539\tLR: 0.000800\n",
            "Training Epoch: 191 [5760/50000]\tLoss: 0.1547\tLR: 0.000800\n",
            "Training Epoch: 191 [5888/50000]\tLoss: 0.1403\tLR: 0.000800\n",
            "Training Epoch: 191 [6016/50000]\tLoss: 0.1747\tLR: 0.000800\n",
            "Training Epoch: 191 [6144/50000]\tLoss: 0.1331\tLR: 0.000800\n",
            "Training Epoch: 191 [6272/50000]\tLoss: 0.2354\tLR: 0.000800\n",
            "Training Epoch: 191 [6400/50000]\tLoss: 0.1607\tLR: 0.000800\n",
            "Training Epoch: 191 [6528/50000]\tLoss: 0.1793\tLR: 0.000800\n",
            "Training Epoch: 191 [6656/50000]\tLoss: 0.0903\tLR: 0.000800\n",
            "Training Epoch: 191 [6784/50000]\tLoss: 0.1661\tLR: 0.000800\n",
            "Training Epoch: 191 [6912/50000]\tLoss: 0.1120\tLR: 0.000800\n",
            "Training Epoch: 191 [7040/50000]\tLoss: 0.1345\tLR: 0.000800\n",
            "Training Epoch: 191 [7168/50000]\tLoss: 0.2485\tLR: 0.000800\n",
            "Training Epoch: 191 [7296/50000]\tLoss: 0.1536\tLR: 0.000800\n",
            "Training Epoch: 191 [7424/50000]\tLoss: 0.1412\tLR: 0.000800\n",
            "Training Epoch: 191 [7552/50000]\tLoss: 0.1121\tLR: 0.000800\n",
            "Training Epoch: 191 [7680/50000]\tLoss: 0.0812\tLR: 0.000800\n",
            "Training Epoch: 191 [7808/50000]\tLoss: 0.1415\tLR: 0.000800\n",
            "Training Epoch: 191 [7936/50000]\tLoss: 0.1497\tLR: 0.000800\n",
            "Training Epoch: 191 [8064/50000]\tLoss: 0.1083\tLR: 0.000800\n",
            "Training Epoch: 191 [8192/50000]\tLoss: 0.1760\tLR: 0.000800\n",
            "Training Epoch: 191 [8320/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 191 [8448/50000]\tLoss: 0.2050\tLR: 0.000800\n",
            "Training Epoch: 191 [8576/50000]\tLoss: 0.1519\tLR: 0.000800\n",
            "Training Epoch: 191 [8704/50000]\tLoss: 0.1388\tLR: 0.000800\n",
            "Training Epoch: 191 [8832/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 191 [8960/50000]\tLoss: 0.1891\tLR: 0.000800\n",
            "Training Epoch: 191 [9088/50000]\tLoss: 0.2341\tLR: 0.000800\n",
            "Training Epoch: 191 [9216/50000]\tLoss: 0.0845\tLR: 0.000800\n",
            "Training Epoch: 191 [9344/50000]\tLoss: 0.1602\tLR: 0.000800\n",
            "Training Epoch: 191 [9472/50000]\tLoss: 0.1403\tLR: 0.000800\n",
            "Training Epoch: 191 [9600/50000]\tLoss: 0.1700\tLR: 0.000800\n",
            "Training Epoch: 191 [9728/50000]\tLoss: 0.1838\tLR: 0.000800\n",
            "Training Epoch: 191 [9856/50000]\tLoss: 0.1293\tLR: 0.000800\n",
            "Training Epoch: 191 [9984/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 191 [10112/50000]\tLoss: 0.1156\tLR: 0.000800\n",
            "Training Epoch: 191 [10240/50000]\tLoss: 0.1637\tLR: 0.000800\n",
            "Training Epoch: 191 [10368/50000]\tLoss: 0.1223\tLR: 0.000800\n",
            "Training Epoch: 191 [10496/50000]\tLoss: 0.1758\tLR: 0.000800\n",
            "Training Epoch: 191 [10624/50000]\tLoss: 0.1408\tLR: 0.000800\n",
            "Training Epoch: 191 [10752/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 191 [10880/50000]\tLoss: 0.1309\tLR: 0.000800\n",
            "Training Epoch: 191 [11008/50000]\tLoss: 0.1856\tLR: 0.000800\n",
            "Training Epoch: 191 [11136/50000]\tLoss: 0.1476\tLR: 0.000800\n",
            "Training Epoch: 191 [11264/50000]\tLoss: 0.1625\tLR: 0.000800\n",
            "Training Epoch: 191 [11392/50000]\tLoss: 0.1725\tLR: 0.000800\n",
            "Training Epoch: 191 [11520/50000]\tLoss: 0.1189\tLR: 0.000800\n",
            "Training Epoch: 191 [11648/50000]\tLoss: 0.1313\tLR: 0.000800\n",
            "Training Epoch: 191 [11776/50000]\tLoss: 0.1804\tLR: 0.000800\n",
            "Training Epoch: 191 [11904/50000]\tLoss: 0.1580\tLR: 0.000800\n",
            "Training Epoch: 191 [12032/50000]\tLoss: 0.1373\tLR: 0.000800\n",
            "Training Epoch: 191 [12160/50000]\tLoss: 0.1413\tLR: 0.000800\n",
            "Training Epoch: 191 [12288/50000]\tLoss: 0.1237\tLR: 0.000800\n",
            "Training Epoch: 191 [12416/50000]\tLoss: 0.1310\tLR: 0.000800\n",
            "Training Epoch: 191 [12544/50000]\tLoss: 0.1205\tLR: 0.000800\n",
            "Training Epoch: 191 [12672/50000]\tLoss: 0.1148\tLR: 0.000800\n",
            "Training Epoch: 191 [12800/50000]\tLoss: 0.1220\tLR: 0.000800\n",
            "Training Epoch: 191 [12928/50000]\tLoss: 0.1431\tLR: 0.000800\n",
            "Training Epoch: 191 [13056/50000]\tLoss: 0.1228\tLR: 0.000800\n",
            "Training Epoch: 191 [13184/50000]\tLoss: 0.0907\tLR: 0.000800\n",
            "Training Epoch: 191 [13312/50000]\tLoss: 0.1905\tLR: 0.000800\n",
            "Training Epoch: 191 [13440/50000]\tLoss: 0.1822\tLR: 0.000800\n",
            "Training Epoch: 191 [13568/50000]\tLoss: 0.1731\tLR: 0.000800\n",
            "Training Epoch: 191 [13696/50000]\tLoss: 0.1035\tLR: 0.000800\n",
            "Training Epoch: 191 [13824/50000]\tLoss: 0.2261\tLR: 0.000800\n",
            "Training Epoch: 191 [13952/50000]\tLoss: 0.1330\tLR: 0.000800\n",
            "Training Epoch: 191 [14080/50000]\tLoss: 0.1406\tLR: 0.000800\n",
            "Training Epoch: 191 [14208/50000]\tLoss: 0.1927\tLR: 0.000800\n",
            "Training Epoch: 191 [14336/50000]\tLoss: 0.1663\tLR: 0.000800\n",
            "Training Epoch: 191 [14464/50000]\tLoss: 0.1093\tLR: 0.000800\n",
            "Training Epoch: 191 [14592/50000]\tLoss: 0.1763\tLR: 0.000800\n",
            "Training Epoch: 191 [14720/50000]\tLoss: 0.1518\tLR: 0.000800\n",
            "Training Epoch: 191 [14848/50000]\tLoss: 0.1440\tLR: 0.000800\n",
            "Training Epoch: 191 [14976/50000]\tLoss: 0.1298\tLR: 0.000800\n",
            "Training Epoch: 191 [15104/50000]\tLoss: 0.1094\tLR: 0.000800\n",
            "Training Epoch: 191 [15232/50000]\tLoss: 0.1242\tLR: 0.000800\n",
            "Training Epoch: 191 [15360/50000]\tLoss: 0.1366\tLR: 0.000800\n",
            "Training Epoch: 191 [15488/50000]\tLoss: 0.0960\tLR: 0.000800\n",
            "Training Epoch: 191 [15616/50000]\tLoss: 0.1528\tLR: 0.000800\n",
            "Training Epoch: 191 [15744/50000]\tLoss: 0.0912\tLR: 0.000800\n",
            "Training Epoch: 191 [15872/50000]\tLoss: 0.1409\tLR: 0.000800\n",
            "Training Epoch: 191 [16000/50000]\tLoss: 0.1458\tLR: 0.000800\n",
            "Training Epoch: 191 [16128/50000]\tLoss: 0.1465\tLR: 0.000800\n",
            "Training Epoch: 191 [16256/50000]\tLoss: 0.1213\tLR: 0.000800\n",
            "Training Epoch: 191 [16384/50000]\tLoss: 0.1230\tLR: 0.000800\n",
            "Training Epoch: 191 [16512/50000]\tLoss: 0.1327\tLR: 0.000800\n",
            "Training Epoch: 191 [16640/50000]\tLoss: 0.1510\tLR: 0.000800\n",
            "Training Epoch: 191 [16768/50000]\tLoss: 0.1577\tLR: 0.000800\n",
            "Training Epoch: 191 [16896/50000]\tLoss: 0.2162\tLR: 0.000800\n",
            "Training Epoch: 191 [17024/50000]\tLoss: 0.2187\tLR: 0.000800\n",
            "Training Epoch: 191 [17152/50000]\tLoss: 0.1042\tLR: 0.000800\n",
            "Training Epoch: 191 [17280/50000]\tLoss: 0.1425\tLR: 0.000800\n",
            "Training Epoch: 191 [17408/50000]\tLoss: 0.1566\tLR: 0.000800\n",
            "Training Epoch: 191 [17536/50000]\tLoss: 0.1246\tLR: 0.000800\n",
            "Training Epoch: 191 [17664/50000]\tLoss: 0.1728\tLR: 0.000800\n",
            "Training Epoch: 191 [17792/50000]\tLoss: 0.1298\tLR: 0.000800\n",
            "Training Epoch: 191 [17920/50000]\tLoss: 0.1281\tLR: 0.000800\n",
            "Training Epoch: 191 [18048/50000]\tLoss: 0.1358\tLR: 0.000800\n",
            "Training Epoch: 191 [18176/50000]\tLoss: 0.1558\tLR: 0.000800\n",
            "Training Epoch: 191 [18304/50000]\tLoss: 0.1542\tLR: 0.000800\n",
            "Training Epoch: 191 [18432/50000]\tLoss: 0.1298\tLR: 0.000800\n",
            "Training Epoch: 191 [18560/50000]\tLoss: 0.2318\tLR: 0.000800\n",
            "Training Epoch: 191 [18688/50000]\tLoss: 0.2688\tLR: 0.000800\n",
            "Training Epoch: 191 [18816/50000]\tLoss: 0.1228\tLR: 0.000800\n",
            "Training Epoch: 191 [18944/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 191 [19072/50000]\tLoss: 0.0955\tLR: 0.000800\n",
            "Training Epoch: 191 [19200/50000]\tLoss: 0.1058\tLR: 0.000800\n",
            "Training Epoch: 191 [19328/50000]\tLoss: 0.1479\tLR: 0.000800\n",
            "Training Epoch: 191 [19456/50000]\tLoss: 0.2055\tLR: 0.000800\n",
            "Training Epoch: 191 [19584/50000]\tLoss: 0.1742\tLR: 0.000800\n",
            "Training Epoch: 191 [19712/50000]\tLoss: 0.1713\tLR: 0.000800\n",
            "Training Epoch: 191 [19840/50000]\tLoss: 0.1668\tLR: 0.000800\n",
            "Training Epoch: 191 [19968/50000]\tLoss: 0.0918\tLR: 0.000800\n",
            "Training Epoch: 191 [20096/50000]\tLoss: 0.1080\tLR: 0.000800\n",
            "Training Epoch: 191 [20224/50000]\tLoss: 0.1876\tLR: 0.000800\n",
            "Training Epoch: 191 [20352/50000]\tLoss: 0.1141\tLR: 0.000800\n",
            "Training Epoch: 191 [20480/50000]\tLoss: 0.1698\tLR: 0.000800\n",
            "Training Epoch: 191 [20608/50000]\tLoss: 0.1537\tLR: 0.000800\n",
            "Training Epoch: 191 [20736/50000]\tLoss: 0.2762\tLR: 0.000800\n",
            "Training Epoch: 191 [20864/50000]\tLoss: 0.1431\tLR: 0.000800\n",
            "Training Epoch: 191 [20992/50000]\tLoss: 0.1485\tLR: 0.000800\n",
            "Training Epoch: 191 [21120/50000]\tLoss: 0.1835\tLR: 0.000800\n",
            "Training Epoch: 191 [21248/50000]\tLoss: 0.1557\tLR: 0.000800\n",
            "Training Epoch: 191 [21376/50000]\tLoss: 0.0954\tLR: 0.000800\n",
            "Training Epoch: 191 [21504/50000]\tLoss: 0.1928\tLR: 0.000800\n",
            "Training Epoch: 191 [21632/50000]\tLoss: 0.1371\tLR: 0.000800\n",
            "Training Epoch: 191 [21760/50000]\tLoss: 0.1462\tLR: 0.000800\n",
            "Training Epoch: 191 [21888/50000]\tLoss: 0.1878\tLR: 0.000800\n",
            "Training Epoch: 191 [22016/50000]\tLoss: 0.1379\tLR: 0.000800\n",
            "Training Epoch: 191 [22144/50000]\tLoss: 0.1285\tLR: 0.000800\n",
            "Training Epoch: 191 [22272/50000]\tLoss: 0.1060\tLR: 0.000800\n",
            "Training Epoch: 191 [22400/50000]\tLoss: 0.1451\tLR: 0.000800\n",
            "Training Epoch: 191 [22528/50000]\tLoss: 0.1112\tLR: 0.000800\n",
            "Training Epoch: 191 [22656/50000]\tLoss: 0.1006\tLR: 0.000800\n",
            "Training Epoch: 191 [22784/50000]\tLoss: 0.1495\tLR: 0.000800\n",
            "Training Epoch: 191 [22912/50000]\tLoss: 0.1490\tLR: 0.000800\n",
            "Training Epoch: 191 [23040/50000]\tLoss: 0.1439\tLR: 0.000800\n",
            "Training Epoch: 191 [23168/50000]\tLoss: 0.1778\tLR: 0.000800\n",
            "Training Epoch: 191 [23296/50000]\tLoss: 0.1266\tLR: 0.000800\n",
            "Training Epoch: 191 [23424/50000]\tLoss: 0.1776\tLR: 0.000800\n",
            "Training Epoch: 191 [23552/50000]\tLoss: 0.1286\tLR: 0.000800\n",
            "Training Epoch: 191 [23680/50000]\tLoss: 0.1061\tLR: 0.000800\n",
            "Training Epoch: 191 [23808/50000]\tLoss: 0.1284\tLR: 0.000800\n",
            "Training Epoch: 191 [23936/50000]\tLoss: 0.1666\tLR: 0.000800\n",
            "Training Epoch: 191 [24064/50000]\tLoss: 0.1641\tLR: 0.000800\n",
            "Training Epoch: 191 [24192/50000]\tLoss: 0.1027\tLR: 0.000800\n",
            "Training Epoch: 191 [24320/50000]\tLoss: 0.1487\tLR: 0.000800\n",
            "Training Epoch: 191 [24448/50000]\tLoss: 0.1673\tLR: 0.000800\n",
            "Training Epoch: 191 [24576/50000]\tLoss: 0.1656\tLR: 0.000800\n",
            "Training Epoch: 191 [24704/50000]\tLoss: 0.1181\tLR: 0.000800\n",
            "Training Epoch: 191 [24832/50000]\tLoss: 0.1056\tLR: 0.000800\n",
            "Training Epoch: 191 [24960/50000]\tLoss: 0.2447\tLR: 0.000800\n",
            "Training Epoch: 191 [25088/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 191 [25216/50000]\tLoss: 0.1131\tLR: 0.000800\n",
            "Training Epoch: 191 [25344/50000]\tLoss: 0.2158\tLR: 0.000800\n",
            "Training Epoch: 191 [25472/50000]\tLoss: 0.1256\tLR: 0.000800\n",
            "Training Epoch: 191 [25600/50000]\tLoss: 0.2660\tLR: 0.000800\n",
            "Training Epoch: 191 [25728/50000]\tLoss: 0.1147\tLR: 0.000800\n",
            "Training Epoch: 191 [25856/50000]\tLoss: 0.1496\tLR: 0.000800\n",
            "Training Epoch: 191 [25984/50000]\tLoss: 0.2035\tLR: 0.000800\n",
            "Training Epoch: 191 [26112/50000]\tLoss: 0.1226\tLR: 0.000800\n",
            "Training Epoch: 191 [26240/50000]\tLoss: 0.1568\tLR: 0.000800\n",
            "Training Epoch: 191 [26368/50000]\tLoss: 0.1353\tLR: 0.000800\n",
            "Training Epoch: 191 [26496/50000]\tLoss: 0.1467\tLR: 0.000800\n",
            "Training Epoch: 191 [26624/50000]\tLoss: 0.1194\tLR: 0.000800\n",
            "Training Epoch: 191 [26752/50000]\tLoss: 0.1701\tLR: 0.000800\n",
            "Training Epoch: 191 [26880/50000]\tLoss: 0.1192\tLR: 0.000800\n",
            "Training Epoch: 191 [27008/50000]\tLoss: 0.1466\tLR: 0.000800\n",
            "Training Epoch: 191 [27136/50000]\tLoss: 0.1387\tLR: 0.000800\n",
            "Training Epoch: 191 [27264/50000]\tLoss: 0.1799\tLR: 0.000800\n",
            "Training Epoch: 191 [27392/50000]\tLoss: 0.1553\tLR: 0.000800\n",
            "Training Epoch: 191 [27520/50000]\tLoss: 0.1200\tLR: 0.000800\n",
            "Training Epoch: 191 [27648/50000]\tLoss: 0.1235\tLR: 0.000800\n",
            "Training Epoch: 191 [27776/50000]\tLoss: 0.1279\tLR: 0.000800\n",
            "Training Epoch: 191 [27904/50000]\tLoss: 0.2125\tLR: 0.000800\n",
            "Training Epoch: 191 [28032/50000]\tLoss: 0.1443\tLR: 0.000800\n",
            "Training Epoch: 191 [28160/50000]\tLoss: 0.1108\tLR: 0.000800\n",
            "Training Epoch: 191 [28288/50000]\tLoss: 0.1094\tLR: 0.000800\n",
            "Training Epoch: 191 [28416/50000]\tLoss: 0.1369\tLR: 0.000800\n",
            "Training Epoch: 191 [28544/50000]\tLoss: 0.1734\tLR: 0.000800\n",
            "Training Epoch: 191 [28672/50000]\tLoss: 0.1773\tLR: 0.000800\n",
            "Training Epoch: 191 [28800/50000]\tLoss: 0.1420\tLR: 0.000800\n",
            "Training Epoch: 191 [28928/50000]\tLoss: 0.1384\tLR: 0.000800\n",
            "Training Epoch: 191 [29056/50000]\tLoss: 0.1293\tLR: 0.000800\n",
            "Training Epoch: 191 [29184/50000]\tLoss: 0.1611\tLR: 0.000800\n",
            "Training Epoch: 191 [29312/50000]\tLoss: 0.1465\tLR: 0.000800\n",
            "Training Epoch: 191 [29440/50000]\tLoss: 0.1379\tLR: 0.000800\n",
            "Training Epoch: 191 [29568/50000]\tLoss: 0.1096\tLR: 0.000800\n",
            "Training Epoch: 191 [29696/50000]\tLoss: 0.1551\tLR: 0.000800\n",
            "Training Epoch: 191 [29824/50000]\tLoss: 0.1577\tLR: 0.000800\n",
            "Training Epoch: 191 [29952/50000]\tLoss: 0.1190\tLR: 0.000800\n",
            "Training Epoch: 191 [30080/50000]\tLoss: 0.1383\tLR: 0.000800\n",
            "Training Epoch: 191 [30208/50000]\tLoss: 0.1467\tLR: 0.000800\n",
            "Training Epoch: 191 [30336/50000]\tLoss: 0.2055\tLR: 0.000800\n",
            "Training Epoch: 191 [30464/50000]\tLoss: 0.1427\tLR: 0.000800\n",
            "Training Epoch: 191 [30592/50000]\tLoss: 0.1568\tLR: 0.000800\n",
            "Training Epoch: 191 [30720/50000]\tLoss: 0.1824\tLR: 0.000800\n",
            "Training Epoch: 191 [30848/50000]\tLoss: 0.1716\tLR: 0.000800\n",
            "Training Epoch: 191 [30976/50000]\tLoss: 0.1664\tLR: 0.000800\n",
            "Training Epoch: 191 [31104/50000]\tLoss: 0.1365\tLR: 0.000800\n",
            "Training Epoch: 191 [31232/50000]\tLoss: 0.1258\tLR: 0.000800\n",
            "Training Epoch: 191 [31360/50000]\tLoss: 0.1703\tLR: 0.000800\n",
            "Training Epoch: 191 [31488/50000]\tLoss: 0.1551\tLR: 0.000800\n",
            "Training Epoch: 191 [31616/50000]\tLoss: 0.0800\tLR: 0.000800\n",
            "Training Epoch: 191 [31744/50000]\tLoss: 0.1436\tLR: 0.000800\n",
            "Training Epoch: 191 [31872/50000]\tLoss: 0.1510\tLR: 0.000800\n",
            "Training Epoch: 191 [32000/50000]\tLoss: 0.2095\tLR: 0.000800\n",
            "Training Epoch: 191 [32128/50000]\tLoss: 0.1353\tLR: 0.000800\n",
            "Training Epoch: 191 [32256/50000]\tLoss: 0.1214\tLR: 0.000800\n",
            "Training Epoch: 191 [32384/50000]\tLoss: 0.2147\tLR: 0.000800\n",
            "Training Epoch: 191 [32512/50000]\tLoss: 0.1000\tLR: 0.000800\n",
            "Training Epoch: 191 [32640/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 191 [32768/50000]\tLoss: 0.0880\tLR: 0.000800\n",
            "Training Epoch: 191 [32896/50000]\tLoss: 0.1214\tLR: 0.000800\n",
            "Training Epoch: 191 [33024/50000]\tLoss: 0.1641\tLR: 0.000800\n",
            "Training Epoch: 191 [33152/50000]\tLoss: 0.1949\tLR: 0.000800\n",
            "Training Epoch: 191 [33280/50000]\tLoss: 0.1224\tLR: 0.000800\n",
            "Training Epoch: 191 [33408/50000]\tLoss: 0.1440\tLR: 0.000800\n",
            "Training Epoch: 191 [33536/50000]\tLoss: 0.1893\tLR: 0.000800\n",
            "Training Epoch: 191 [33664/50000]\tLoss: 0.1483\tLR: 0.000800\n",
            "Training Epoch: 191 [33792/50000]\tLoss: 0.1033\tLR: 0.000800\n",
            "Training Epoch: 191 [33920/50000]\tLoss: 0.1572\tLR: 0.000800\n",
            "Training Epoch: 191 [34048/50000]\tLoss: 0.1098\tLR: 0.000800\n",
            "Training Epoch: 191 [34176/50000]\tLoss: 0.1738\tLR: 0.000800\n",
            "Training Epoch: 191 [34304/50000]\tLoss: 0.1436\tLR: 0.000800\n",
            "Training Epoch: 191 [34432/50000]\tLoss: 0.1065\tLR: 0.000800\n",
            "Training Epoch: 191 [34560/50000]\tLoss: 0.1633\tLR: 0.000800\n",
            "Training Epoch: 191 [34688/50000]\tLoss: 0.1932\tLR: 0.000800\n",
            "Training Epoch: 191 [34816/50000]\tLoss: 0.1358\tLR: 0.000800\n",
            "Training Epoch: 191 [34944/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 191 [35072/50000]\tLoss: 0.1480\tLR: 0.000800\n",
            "Training Epoch: 191 [35200/50000]\tLoss: 0.0867\tLR: 0.000800\n",
            "Training Epoch: 191 [35328/50000]\tLoss: 0.2125\tLR: 0.000800\n",
            "Training Epoch: 191 [35456/50000]\tLoss: 0.1341\tLR: 0.000800\n",
            "Training Epoch: 191 [35584/50000]\tLoss: 0.1151\tLR: 0.000800\n",
            "Training Epoch: 191 [35712/50000]\tLoss: 0.0993\tLR: 0.000800\n",
            "Training Epoch: 191 [35840/50000]\tLoss: 0.1372\tLR: 0.000800\n",
            "Training Epoch: 191 [35968/50000]\tLoss: 0.0906\tLR: 0.000800\n",
            "Training Epoch: 191 [36096/50000]\tLoss: 0.1881\tLR: 0.000800\n",
            "Training Epoch: 191 [36224/50000]\tLoss: 0.1149\tLR: 0.000800\n",
            "Training Epoch: 191 [36352/50000]\tLoss: 0.1496\tLR: 0.000800\n",
            "Training Epoch: 191 [36480/50000]\tLoss: 0.0954\tLR: 0.000800\n",
            "Training Epoch: 191 [36608/50000]\tLoss: 0.1224\tLR: 0.000800\n",
            "Training Epoch: 191 [36736/50000]\tLoss: 0.2371\tLR: 0.000800\n",
            "Training Epoch: 191 [36864/50000]\tLoss: 0.1304\tLR: 0.000800\n",
            "Training Epoch: 191 [36992/50000]\tLoss: 0.1341\tLR: 0.000800\n",
            "Training Epoch: 191 [37120/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 191 [37248/50000]\tLoss: 0.1609\tLR: 0.000800\n",
            "Training Epoch: 191 [37376/50000]\tLoss: 0.1850\tLR: 0.000800\n",
            "Training Epoch: 191 [37504/50000]\tLoss: 0.2210\tLR: 0.000800\n",
            "Training Epoch: 191 [37632/50000]\tLoss: 0.1232\tLR: 0.000800\n",
            "Training Epoch: 191 [37760/50000]\tLoss: 0.1749\tLR: 0.000800\n",
            "Training Epoch: 191 [37888/50000]\tLoss: 0.1732\tLR: 0.000800\n",
            "Training Epoch: 191 [38016/50000]\tLoss: 0.1800\tLR: 0.000800\n",
            "Training Epoch: 191 [38144/50000]\tLoss: 0.1133\tLR: 0.000800\n",
            "Training Epoch: 191 [38272/50000]\tLoss: 0.2522\tLR: 0.000800\n",
            "Training Epoch: 191 [38400/50000]\tLoss: 0.1462\tLR: 0.000800\n",
            "Training Epoch: 191 [38528/50000]\tLoss: 0.1380\tLR: 0.000800\n",
            "Training Epoch: 191 [38656/50000]\tLoss: 0.1243\tLR: 0.000800\n",
            "Training Epoch: 191 [38784/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 191 [38912/50000]\tLoss: 0.1973\tLR: 0.000800\n",
            "Training Epoch: 191 [39040/50000]\tLoss: 0.1079\tLR: 0.000800\n",
            "Training Epoch: 191 [39168/50000]\tLoss: 0.0956\tLR: 0.000800\n",
            "Training Epoch: 191 [39296/50000]\tLoss: 0.1980\tLR: 0.000800\n",
            "Training Epoch: 191 [39424/50000]\tLoss: 0.1352\tLR: 0.000800\n",
            "Training Epoch: 191 [39552/50000]\tLoss: 0.1440\tLR: 0.000800\n",
            "Training Epoch: 191 [39680/50000]\tLoss: 0.1048\tLR: 0.000800\n",
            "Training Epoch: 191 [39808/50000]\tLoss: 0.1882\tLR: 0.000800\n",
            "Training Epoch: 191 [39936/50000]\tLoss: 0.1247\tLR: 0.000800\n",
            "Training Epoch: 191 [40064/50000]\tLoss: 0.1112\tLR: 0.000800\n",
            "Training Epoch: 191 [40192/50000]\tLoss: 0.2166\tLR: 0.000800\n",
            "Training Epoch: 191 [40320/50000]\tLoss: 0.2187\tLR: 0.000800\n",
            "Training Epoch: 191 [40448/50000]\tLoss: 0.0952\tLR: 0.000800\n",
            "Training Epoch: 191 [40576/50000]\tLoss: 0.1198\tLR: 0.000800\n",
            "Training Epoch: 191 [40704/50000]\tLoss: 0.1501\tLR: 0.000800\n",
            "Training Epoch: 191 [40832/50000]\tLoss: 0.1452\tLR: 0.000800\n",
            "Training Epoch: 191 [40960/50000]\tLoss: 0.1306\tLR: 0.000800\n",
            "Training Epoch: 191 [41088/50000]\tLoss: 0.1114\tLR: 0.000800\n",
            "Training Epoch: 191 [41216/50000]\tLoss: 0.1668\tLR: 0.000800\n",
            "Training Epoch: 191 [41344/50000]\tLoss: 0.1600\tLR: 0.000800\n",
            "Training Epoch: 191 [41472/50000]\tLoss: 0.1354\tLR: 0.000800\n",
            "Training Epoch: 191 [41600/50000]\tLoss: 0.1482\tLR: 0.000800\n",
            "Training Epoch: 191 [41728/50000]\tLoss: 0.1541\tLR: 0.000800\n",
            "Training Epoch: 191 [41856/50000]\tLoss: 0.1050\tLR: 0.000800\n",
            "Training Epoch: 191 [41984/50000]\tLoss: 0.1407\tLR: 0.000800\n",
            "Training Epoch: 191 [42112/50000]\tLoss: 0.1559\tLR: 0.000800\n",
            "Training Epoch: 191 [42240/50000]\tLoss: 0.1604\tLR: 0.000800\n",
            "Training Epoch: 191 [42368/50000]\tLoss: 0.1262\tLR: 0.000800\n",
            "Training Epoch: 191 [42496/50000]\tLoss: 0.1946\tLR: 0.000800\n",
            "Training Epoch: 191 [42624/50000]\tLoss: 0.1321\tLR: 0.000800\n",
            "Training Epoch: 191 [42752/50000]\tLoss: 0.1402\tLR: 0.000800\n",
            "Training Epoch: 191 [42880/50000]\tLoss: 0.1399\tLR: 0.000800\n",
            "Training Epoch: 191 [43008/50000]\tLoss: 0.1033\tLR: 0.000800\n",
            "Training Epoch: 191 [43136/50000]\tLoss: 0.2014\tLR: 0.000800\n",
            "Training Epoch: 191 [43264/50000]\tLoss: 0.1504\tLR: 0.000800\n",
            "Training Epoch: 191 [43392/50000]\tLoss: 0.1921\tLR: 0.000800\n",
            "Training Epoch: 191 [43520/50000]\tLoss: 0.1263\tLR: 0.000800\n",
            "Training Epoch: 191 [43648/50000]\tLoss: 0.0966\tLR: 0.000800\n",
            "Training Epoch: 191 [43776/50000]\tLoss: 0.1118\tLR: 0.000800\n",
            "Training Epoch: 191 [43904/50000]\tLoss: 0.1473\tLR: 0.000800\n",
            "Training Epoch: 191 [44032/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 191 [44160/50000]\tLoss: 0.1300\tLR: 0.000800\n",
            "Training Epoch: 191 [44288/50000]\tLoss: 0.1218\tLR: 0.000800\n",
            "Training Epoch: 191 [44416/50000]\tLoss: 0.1007\tLR: 0.000800\n",
            "Training Epoch: 191 [44544/50000]\tLoss: 0.1587\tLR: 0.000800\n",
            "Training Epoch: 191 [44672/50000]\tLoss: 0.1869\tLR: 0.000800\n",
            "Training Epoch: 191 [44800/50000]\tLoss: 0.1645\tLR: 0.000800\n",
            "Training Epoch: 191 [44928/50000]\tLoss: 0.0940\tLR: 0.000800\n",
            "Training Epoch: 191 [45056/50000]\tLoss: 0.1496\tLR: 0.000800\n",
            "Training Epoch: 191 [45184/50000]\tLoss: 0.1500\tLR: 0.000800\n",
            "Training Epoch: 191 [45312/50000]\tLoss: 0.1114\tLR: 0.000800\n",
            "Training Epoch: 191 [45440/50000]\tLoss: 0.1205\tLR: 0.000800\n",
            "Training Epoch: 191 [45568/50000]\tLoss: 0.1545\tLR: 0.000800\n",
            "Training Epoch: 191 [45696/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 191 [45824/50000]\tLoss: 0.1502\tLR: 0.000800\n",
            "Training Epoch: 191 [45952/50000]\tLoss: 0.1614\tLR: 0.000800\n",
            "Training Epoch: 191 [46080/50000]\tLoss: 0.1635\tLR: 0.000800\n",
            "Training Epoch: 191 [46208/50000]\tLoss: 0.1279\tLR: 0.000800\n",
            "Training Epoch: 191 [46336/50000]\tLoss: 0.1271\tLR: 0.000800\n",
            "Training Epoch: 191 [46464/50000]\tLoss: 0.1524\tLR: 0.000800\n",
            "Training Epoch: 191 [46592/50000]\tLoss: 0.1341\tLR: 0.000800\n",
            "Training Epoch: 191 [46720/50000]\tLoss: 0.1556\tLR: 0.000800\n",
            "Training Epoch: 191 [46848/50000]\tLoss: 0.0836\tLR: 0.000800\n",
            "Training Epoch: 191 [46976/50000]\tLoss: 0.0954\tLR: 0.000800\n",
            "Training Epoch: 191 [47104/50000]\tLoss: 0.1485\tLR: 0.000800\n",
            "Training Epoch: 191 [47232/50000]\tLoss: 0.1286\tLR: 0.000800\n",
            "Training Epoch: 191 [47360/50000]\tLoss: 0.1069\tLR: 0.000800\n",
            "Training Epoch: 191 [47488/50000]\tLoss: 0.1832\tLR: 0.000800\n",
            "Training Epoch: 191 [47616/50000]\tLoss: 0.1295\tLR: 0.000800\n",
            "Training Epoch: 191 [47744/50000]\tLoss: 0.1698\tLR: 0.000800\n",
            "Training Epoch: 191 [47872/50000]\tLoss: 0.1583\tLR: 0.000800\n",
            "Training Epoch: 191 [48000/50000]\tLoss: 0.1337\tLR: 0.000800\n",
            "Training Epoch: 191 [48128/50000]\tLoss: 0.1461\tLR: 0.000800\n",
            "Training Epoch: 191 [48256/50000]\tLoss: 0.1227\tLR: 0.000800\n",
            "Training Epoch: 191 [48384/50000]\tLoss: 0.1699\tLR: 0.000800\n",
            "Training Epoch: 191 [48512/50000]\tLoss: 0.1289\tLR: 0.000800\n",
            "Training Epoch: 191 [48640/50000]\tLoss: 0.1392\tLR: 0.000800\n",
            "Training Epoch: 191 [48768/50000]\tLoss: 0.1800\tLR: 0.000800\n",
            "Training Epoch: 191 [48896/50000]\tLoss: 0.1387\tLR: 0.000800\n",
            "Training Epoch: 191 [49024/50000]\tLoss: 0.1307\tLR: 0.000800\n",
            "Training Epoch: 191 [49152/50000]\tLoss: 0.2207\tLR: 0.000800\n",
            "Training Epoch: 191 [49280/50000]\tLoss: 0.1714\tLR: 0.000800\n",
            "Training Epoch: 191 [49408/50000]\tLoss: 0.2062\tLR: 0.000800\n",
            "Training Epoch: 191 [49536/50000]\tLoss: 0.1266\tLR: 0.000800\n",
            "Training Epoch: 191 [49664/50000]\tLoss: 0.2210\tLR: 0.000800\n",
            "Training Epoch: 191 [49792/50000]\tLoss: 0.1889\tLR: 0.000800\n",
            "Training Epoch: 191 [49920/50000]\tLoss: 0.2328\tLR: 0.000800\n",
            "Training Epoch: 191 [50000/50000]\tLoss: 0.1577\tLR: 0.000800\n",
            "epoch 191 training time consumed: 29.27s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  85954 GiB |  85954 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  84317 GiB |  84317 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1636 GiB |   1636 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  85954 GiB |  85954 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  84317 GiB |  84317 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1636 GiB |   1636 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  85698 GiB |  85698 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  84064 GiB |  84064 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1634 GiB |   1634 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  52587 GiB |  52587 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  50897 GiB |  50897 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1690 GiB |   1690 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   33305 K  |   33305 K  |\n",
            "|       from large pool |       8    |      61    |   11173 K  |   11173 K  |\n",
            "|       from small pool |     373    |     464    |   22132 K  |   22132 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   33305 K  |   33305 K  |\n",
            "|       from large pool |       8    |      61    |   11173 K  |   11173 K  |\n",
            "|       from small pool |     373    |     464    |   22132 K  |   22132 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   10949 K  |   10949 K  |\n",
            "|       from large pool |       5    |      14    |    5326 K  |    5326 K  |\n",
            "|       from small pool |      10    |      16    |    5622 K  |    5622 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 191, Average loss: 0.0112, Accuracy: 0.6666, Time consumed:3.69s\n",
            "\n",
            "Training Epoch: 192 [128/50000]\tLoss: 0.1212\tLR: 0.000800\n",
            "Training Epoch: 192 [256/50000]\tLoss: 0.1541\tLR: 0.000800\n",
            "Training Epoch: 192 [384/50000]\tLoss: 0.1541\tLR: 0.000800\n",
            "Training Epoch: 192 [512/50000]\tLoss: 0.1305\tLR: 0.000800\n",
            "Training Epoch: 192 [640/50000]\tLoss: 0.1360\tLR: 0.000800\n",
            "Training Epoch: 192 [768/50000]\tLoss: 0.1629\tLR: 0.000800\n",
            "Training Epoch: 192 [896/50000]\tLoss: 0.1457\tLR: 0.000800\n",
            "Training Epoch: 192 [1024/50000]\tLoss: 0.1714\tLR: 0.000800\n",
            "Training Epoch: 192 [1152/50000]\tLoss: 0.1374\tLR: 0.000800\n",
            "Training Epoch: 192 [1280/50000]\tLoss: 0.1272\tLR: 0.000800\n",
            "Training Epoch: 192 [1408/50000]\tLoss: 0.0944\tLR: 0.000800\n",
            "Training Epoch: 192 [1536/50000]\tLoss: 0.1627\tLR: 0.000800\n",
            "Training Epoch: 192 [1664/50000]\tLoss: 0.1043\tLR: 0.000800\n",
            "Training Epoch: 192 [1792/50000]\tLoss: 0.1666\tLR: 0.000800\n",
            "Training Epoch: 192 [1920/50000]\tLoss: 0.1067\tLR: 0.000800\n",
            "Training Epoch: 192 [2048/50000]\tLoss: 0.1742\tLR: 0.000800\n",
            "Training Epoch: 192 [2176/50000]\tLoss: 0.1042\tLR: 0.000800\n",
            "Training Epoch: 192 [2304/50000]\tLoss: 0.1746\tLR: 0.000800\n",
            "Training Epoch: 192 [2432/50000]\tLoss: 0.1174\tLR: 0.000800\n",
            "Training Epoch: 192 [2560/50000]\tLoss: 0.1357\tLR: 0.000800\n",
            "Training Epoch: 192 [2688/50000]\tLoss: 0.1354\tLR: 0.000800\n",
            "Training Epoch: 192 [2816/50000]\tLoss: 0.1264\tLR: 0.000800\n",
            "Training Epoch: 192 [2944/50000]\tLoss: 0.1340\tLR: 0.000800\n",
            "Training Epoch: 192 [3072/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 192 [3200/50000]\tLoss: 0.1195\tLR: 0.000800\n",
            "Training Epoch: 192 [3328/50000]\tLoss: 0.2158\tLR: 0.000800\n",
            "Training Epoch: 192 [3456/50000]\tLoss: 0.1328\tLR: 0.000800\n",
            "Training Epoch: 192 [3584/50000]\tLoss: 0.1495\tLR: 0.000800\n",
            "Training Epoch: 192 [3712/50000]\tLoss: 0.1886\tLR: 0.000800\n",
            "Training Epoch: 192 [3840/50000]\tLoss: 0.1557\tLR: 0.000800\n",
            "Training Epoch: 192 [3968/50000]\tLoss: 0.1581\tLR: 0.000800\n",
            "Training Epoch: 192 [4096/50000]\tLoss: 0.1575\tLR: 0.000800\n",
            "Training Epoch: 192 [4224/50000]\tLoss: 0.1067\tLR: 0.000800\n",
            "Training Epoch: 192 [4352/50000]\tLoss: 0.1655\tLR: 0.000800\n",
            "Training Epoch: 192 [4480/50000]\tLoss: 0.1531\tLR: 0.000800\n",
            "Training Epoch: 192 [4608/50000]\tLoss: 0.1166\tLR: 0.000800\n",
            "Training Epoch: 192 [4736/50000]\tLoss: 0.2314\tLR: 0.000800\n",
            "Training Epoch: 192 [4864/50000]\tLoss: 0.1123\tLR: 0.000800\n",
            "Training Epoch: 192 [4992/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 192 [5120/50000]\tLoss: 0.1699\tLR: 0.000800\n",
            "Training Epoch: 192 [5248/50000]\tLoss: 0.1298\tLR: 0.000800\n",
            "Training Epoch: 192 [5376/50000]\tLoss: 0.1368\tLR: 0.000800\n",
            "Training Epoch: 192 [5504/50000]\tLoss: 0.1357\tLR: 0.000800\n",
            "Training Epoch: 192 [5632/50000]\tLoss: 0.1498\tLR: 0.000800\n",
            "Training Epoch: 192 [5760/50000]\tLoss: 0.1253\tLR: 0.000800\n",
            "Training Epoch: 192 [5888/50000]\tLoss: 0.1184\tLR: 0.000800\n",
            "Training Epoch: 192 [6016/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 192 [6144/50000]\tLoss: 0.1535\tLR: 0.000800\n",
            "Training Epoch: 192 [6272/50000]\tLoss: 0.1463\tLR: 0.000800\n",
            "Training Epoch: 192 [6400/50000]\tLoss: 0.1321\tLR: 0.000800\n",
            "Training Epoch: 192 [6528/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 192 [6656/50000]\tLoss: 0.1313\tLR: 0.000800\n",
            "Training Epoch: 192 [6784/50000]\tLoss: 0.1063\tLR: 0.000800\n",
            "Training Epoch: 192 [6912/50000]\tLoss: 0.1661\tLR: 0.000800\n",
            "Training Epoch: 192 [7040/50000]\tLoss: 0.0991\tLR: 0.000800\n",
            "Training Epoch: 192 [7168/50000]\tLoss: 0.1369\tLR: 0.000800\n",
            "Training Epoch: 192 [7296/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 192 [7424/50000]\tLoss: 0.1339\tLR: 0.000800\n",
            "Training Epoch: 192 [7552/50000]\tLoss: 0.1648\tLR: 0.000800\n",
            "Training Epoch: 192 [7680/50000]\tLoss: 0.1341\tLR: 0.000800\n",
            "Training Epoch: 192 [7808/50000]\tLoss: 0.0846\tLR: 0.000800\n",
            "Training Epoch: 192 [7936/50000]\tLoss: 0.0923\tLR: 0.000800\n",
            "Training Epoch: 192 [8064/50000]\tLoss: 0.1264\tLR: 0.000800\n",
            "Training Epoch: 192 [8192/50000]\tLoss: 0.1290\tLR: 0.000800\n",
            "Training Epoch: 192 [8320/50000]\tLoss: 0.1135\tLR: 0.000800\n",
            "Training Epoch: 192 [8448/50000]\tLoss: 0.1149\tLR: 0.000800\n",
            "Training Epoch: 192 [8576/50000]\tLoss: 0.1920\tLR: 0.000800\n",
            "Training Epoch: 192 [8704/50000]\tLoss: 0.0793\tLR: 0.000800\n",
            "Training Epoch: 192 [8832/50000]\tLoss: 0.2174\tLR: 0.000800\n",
            "Training Epoch: 192 [8960/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 192 [9088/50000]\tLoss: 0.1841\tLR: 0.000800\n",
            "Training Epoch: 192 [9216/50000]\tLoss: 0.0875\tLR: 0.000800\n",
            "Training Epoch: 192 [9344/50000]\tLoss: 0.1440\tLR: 0.000800\n",
            "Training Epoch: 192 [9472/50000]\tLoss: 0.1687\tLR: 0.000800\n",
            "Training Epoch: 192 [9600/50000]\tLoss: 0.1594\tLR: 0.000800\n",
            "Training Epoch: 192 [9728/50000]\tLoss: 0.1547\tLR: 0.000800\n",
            "Training Epoch: 192 [9856/50000]\tLoss: 0.1200\tLR: 0.000800\n",
            "Training Epoch: 192 [9984/50000]\tLoss: 0.1263\tLR: 0.000800\n",
            "Training Epoch: 192 [10112/50000]\tLoss: 0.1650\tLR: 0.000800\n",
            "Training Epoch: 192 [10240/50000]\tLoss: 0.1943\tLR: 0.000800\n",
            "Training Epoch: 192 [10368/50000]\tLoss: 0.1281\tLR: 0.000800\n",
            "Training Epoch: 192 [10496/50000]\tLoss: 0.1365\tLR: 0.000800\n",
            "Training Epoch: 192 [10624/50000]\tLoss: 0.1364\tLR: 0.000800\n",
            "Training Epoch: 192 [10752/50000]\tLoss: 0.2071\tLR: 0.000800\n",
            "Training Epoch: 192 [10880/50000]\tLoss: 0.1791\tLR: 0.000800\n",
            "Training Epoch: 192 [11008/50000]\tLoss: 0.1397\tLR: 0.000800\n",
            "Training Epoch: 192 [11136/50000]\tLoss: 0.1416\tLR: 0.000800\n",
            "Training Epoch: 192 [11264/50000]\tLoss: 0.1924\tLR: 0.000800\n",
            "Training Epoch: 192 [11392/50000]\tLoss: 0.1755\tLR: 0.000800\n",
            "Training Epoch: 192 [11520/50000]\tLoss: 0.0880\tLR: 0.000800\n",
            "Training Epoch: 192 [11648/50000]\tLoss: 0.1470\tLR: 0.000800\n",
            "Training Epoch: 192 [11776/50000]\tLoss: 0.0942\tLR: 0.000800\n",
            "Training Epoch: 192 [11904/50000]\tLoss: 0.1029\tLR: 0.000800\n",
            "Training Epoch: 192 [12032/50000]\tLoss: 0.1826\tLR: 0.000800\n",
            "Training Epoch: 192 [12160/50000]\tLoss: 0.1704\tLR: 0.000800\n",
            "Training Epoch: 192 [12288/50000]\tLoss: 0.1794\tLR: 0.000800\n",
            "Training Epoch: 192 [12416/50000]\tLoss: 0.0826\tLR: 0.000800\n",
            "Training Epoch: 192 [12544/50000]\tLoss: 0.1905\tLR: 0.000800\n",
            "Training Epoch: 192 [12672/50000]\tLoss: 0.1282\tLR: 0.000800\n",
            "Training Epoch: 192 [12800/50000]\tLoss: 0.2130\tLR: 0.000800\n",
            "Training Epoch: 192 [12928/50000]\tLoss: 0.0924\tLR: 0.000800\n",
            "Training Epoch: 192 [13056/50000]\tLoss: 0.1017\tLR: 0.000800\n",
            "Training Epoch: 192 [13184/50000]\tLoss: 0.1351\tLR: 0.000800\n",
            "Training Epoch: 192 [13312/50000]\tLoss: 0.1938\tLR: 0.000800\n",
            "Training Epoch: 192 [13440/50000]\tLoss: 0.1443\tLR: 0.000800\n",
            "Training Epoch: 192 [13568/50000]\tLoss: 0.1722\tLR: 0.000800\n",
            "Training Epoch: 192 [13696/50000]\tLoss: 0.1590\tLR: 0.000800\n",
            "Training Epoch: 192 [13824/50000]\tLoss: 0.1420\tLR: 0.000800\n",
            "Training Epoch: 192 [13952/50000]\tLoss: 0.1018\tLR: 0.000800\n",
            "Training Epoch: 192 [14080/50000]\tLoss: 0.1032\tLR: 0.000800\n",
            "Training Epoch: 192 [14208/50000]\tLoss: 0.1632\tLR: 0.000800\n",
            "Training Epoch: 192 [14336/50000]\tLoss: 0.1312\tLR: 0.000800\n",
            "Training Epoch: 192 [14464/50000]\tLoss: 0.1215\tLR: 0.000800\n",
            "Training Epoch: 192 [14592/50000]\tLoss: 0.1031\tLR: 0.000800\n",
            "Training Epoch: 192 [14720/50000]\tLoss: 0.1324\tLR: 0.000800\n",
            "Training Epoch: 192 [14848/50000]\tLoss: 0.1637\tLR: 0.000800\n",
            "Training Epoch: 192 [14976/50000]\tLoss: 0.1483\tLR: 0.000800\n",
            "Training Epoch: 192 [15104/50000]\tLoss: 0.2334\tLR: 0.000800\n",
            "Training Epoch: 192 [15232/50000]\tLoss: 0.1143\tLR: 0.000800\n",
            "Training Epoch: 192 [15360/50000]\tLoss: 0.1432\tLR: 0.000800\n",
            "Training Epoch: 192 [15488/50000]\tLoss: 0.1284\tLR: 0.000800\n",
            "Training Epoch: 192 [15616/50000]\tLoss: 0.1162\tLR: 0.000800\n",
            "Training Epoch: 192 [15744/50000]\tLoss: 0.1631\tLR: 0.000800\n",
            "Training Epoch: 192 [15872/50000]\tLoss: 0.1755\tLR: 0.000800\n",
            "Training Epoch: 192 [16000/50000]\tLoss: 0.1152\tLR: 0.000800\n",
            "Training Epoch: 192 [16128/50000]\tLoss: 0.1403\tLR: 0.000800\n",
            "Training Epoch: 192 [16256/50000]\tLoss: 0.1246\tLR: 0.000800\n",
            "Training Epoch: 192 [16384/50000]\tLoss: 0.1188\tLR: 0.000800\n",
            "Training Epoch: 192 [16512/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 192 [16640/50000]\tLoss: 0.1528\tLR: 0.000800\n",
            "Training Epoch: 192 [16768/50000]\tLoss: 0.2190\tLR: 0.000800\n",
            "Training Epoch: 192 [16896/50000]\tLoss: 0.1897\tLR: 0.000800\n",
            "Training Epoch: 192 [17024/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 192 [17152/50000]\tLoss: 0.1307\tLR: 0.000800\n",
            "Training Epoch: 192 [17280/50000]\tLoss: 0.1309\tLR: 0.000800\n",
            "Training Epoch: 192 [17408/50000]\tLoss: 0.1349\tLR: 0.000800\n",
            "Training Epoch: 192 [17536/50000]\tLoss: 0.1019\tLR: 0.000800\n",
            "Training Epoch: 192 [17664/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 192 [17792/50000]\tLoss: 0.1135\tLR: 0.000800\n",
            "Training Epoch: 192 [17920/50000]\tLoss: 0.1392\tLR: 0.000800\n",
            "Training Epoch: 192 [18048/50000]\tLoss: 0.1273\tLR: 0.000800\n",
            "Training Epoch: 192 [18176/50000]\tLoss: 0.1714\tLR: 0.000800\n",
            "Training Epoch: 192 [18304/50000]\tLoss: 0.1686\tLR: 0.000800\n",
            "Training Epoch: 192 [18432/50000]\tLoss: 0.1125\tLR: 0.000800\n",
            "Training Epoch: 192 [18560/50000]\tLoss: 0.1666\tLR: 0.000800\n",
            "Training Epoch: 192 [18688/50000]\tLoss: 0.2304\tLR: 0.000800\n",
            "Training Epoch: 192 [18816/50000]\tLoss: 0.1483\tLR: 0.000800\n",
            "Training Epoch: 192 [18944/50000]\tLoss: 0.1305\tLR: 0.000800\n",
            "Training Epoch: 192 [19072/50000]\tLoss: 0.0886\tLR: 0.000800\n",
            "Training Epoch: 192 [19200/50000]\tLoss: 0.1298\tLR: 0.000800\n",
            "Training Epoch: 192 [19328/50000]\tLoss: 0.1167\tLR: 0.000800\n",
            "Training Epoch: 192 [19456/50000]\tLoss: 0.1141\tLR: 0.000800\n",
            "Training Epoch: 192 [19584/50000]\tLoss: 0.1628\tLR: 0.000800\n",
            "Training Epoch: 192 [19712/50000]\tLoss: 0.1828\tLR: 0.000800\n",
            "Training Epoch: 192 [19840/50000]\tLoss: 0.1757\tLR: 0.000800\n",
            "Training Epoch: 192 [19968/50000]\tLoss: 0.0921\tLR: 0.000800\n",
            "Training Epoch: 192 [20096/50000]\tLoss: 0.1929\tLR: 0.000800\n",
            "Training Epoch: 192 [20224/50000]\tLoss: 0.1759\tLR: 0.000800\n",
            "Training Epoch: 192 [20352/50000]\tLoss: 0.1718\tLR: 0.000800\n",
            "Training Epoch: 192 [20480/50000]\tLoss: 0.1152\tLR: 0.000800\n",
            "Training Epoch: 192 [20608/50000]\tLoss: 0.1224\tLR: 0.000800\n",
            "Training Epoch: 192 [20736/50000]\tLoss: 0.1756\tLR: 0.000800\n",
            "Training Epoch: 192 [20864/50000]\tLoss: 0.1171\tLR: 0.000800\n",
            "Training Epoch: 192 [20992/50000]\tLoss: 0.1893\tLR: 0.000800\n",
            "Training Epoch: 192 [21120/50000]\tLoss: 0.1562\tLR: 0.000800\n",
            "Training Epoch: 192 [21248/50000]\tLoss: 0.1273\tLR: 0.000800\n",
            "Training Epoch: 192 [21376/50000]\tLoss: 0.1753\tLR: 0.000800\n",
            "Training Epoch: 192 [21504/50000]\tLoss: 0.1901\tLR: 0.000800\n",
            "Training Epoch: 192 [21632/50000]\tLoss: 0.1707\tLR: 0.000800\n",
            "Training Epoch: 192 [21760/50000]\tLoss: 0.2028\tLR: 0.000800\n",
            "Training Epoch: 192 [21888/50000]\tLoss: 0.1886\tLR: 0.000800\n",
            "Training Epoch: 192 [22016/50000]\tLoss: 0.1212\tLR: 0.000800\n",
            "Training Epoch: 192 [22144/50000]\tLoss: 0.1193\tLR: 0.000800\n",
            "Training Epoch: 192 [22272/50000]\tLoss: 0.1610\tLR: 0.000800\n",
            "Training Epoch: 192 [22400/50000]\tLoss: 0.2161\tLR: 0.000800\n",
            "Training Epoch: 192 [22528/50000]\tLoss: 0.1339\tLR: 0.000800\n",
            "Training Epoch: 192 [22656/50000]\tLoss: 0.1713\tLR: 0.000800\n",
            "Training Epoch: 192 [22784/50000]\tLoss: 0.1288\tLR: 0.000800\n",
            "Training Epoch: 192 [22912/50000]\tLoss: 0.1488\tLR: 0.000800\n",
            "Training Epoch: 192 [23040/50000]\tLoss: 0.1439\tLR: 0.000800\n",
            "Training Epoch: 192 [23168/50000]\tLoss: 0.1031\tLR: 0.000800\n",
            "Training Epoch: 192 [23296/50000]\tLoss: 0.1453\tLR: 0.000800\n",
            "Training Epoch: 192 [23424/50000]\tLoss: 0.1505\tLR: 0.000800\n",
            "Training Epoch: 192 [23552/50000]\tLoss: 0.1474\tLR: 0.000800\n",
            "Training Epoch: 192 [23680/50000]\tLoss: 0.1766\tLR: 0.000800\n",
            "Training Epoch: 192 [23808/50000]\tLoss: 0.1761\tLR: 0.000800\n",
            "Training Epoch: 192 [23936/50000]\tLoss: 0.1792\tLR: 0.000800\n",
            "Training Epoch: 192 [24064/50000]\tLoss: 0.0934\tLR: 0.000800\n",
            "Training Epoch: 192 [24192/50000]\tLoss: 0.1392\tLR: 0.000800\n",
            "Training Epoch: 192 [24320/50000]\tLoss: 0.1560\tLR: 0.000800\n",
            "Training Epoch: 192 [24448/50000]\tLoss: 0.1384\tLR: 0.000800\n",
            "Training Epoch: 192 [24576/50000]\tLoss: 0.1093\tLR: 0.000800\n",
            "Training Epoch: 192 [24704/50000]\tLoss: 0.1176\tLR: 0.000800\n",
            "Training Epoch: 192 [24832/50000]\tLoss: 0.1328\tLR: 0.000800\n",
            "Training Epoch: 192 [24960/50000]\tLoss: 0.1312\tLR: 0.000800\n",
            "Training Epoch: 192 [25088/50000]\tLoss: 0.2214\tLR: 0.000800\n",
            "Training Epoch: 192 [25216/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 192 [25344/50000]\tLoss: 0.1288\tLR: 0.000800\n",
            "Training Epoch: 192 [25472/50000]\tLoss: 0.1406\tLR: 0.000800\n",
            "Training Epoch: 192 [25600/50000]\tLoss: 0.1415\tLR: 0.000800\n",
            "Training Epoch: 192 [25728/50000]\tLoss: 0.2629\tLR: 0.000800\n",
            "Training Epoch: 192 [25856/50000]\tLoss: 0.1714\tLR: 0.000800\n",
            "Training Epoch: 192 [25984/50000]\tLoss: 0.1339\tLR: 0.000800\n",
            "Training Epoch: 192 [26112/50000]\tLoss: 0.1101\tLR: 0.000800\n",
            "Training Epoch: 192 [26240/50000]\tLoss: 0.0957\tLR: 0.000800\n",
            "Training Epoch: 192 [26368/50000]\tLoss: 0.0674\tLR: 0.000800\n",
            "Training Epoch: 192 [26496/50000]\tLoss: 0.1476\tLR: 0.000800\n",
            "Training Epoch: 192 [26624/50000]\tLoss: 0.1501\tLR: 0.000800\n",
            "Training Epoch: 192 [26752/50000]\tLoss: 0.1167\tLR: 0.000800\n",
            "Training Epoch: 192 [26880/50000]\tLoss: 0.1898\tLR: 0.000800\n",
            "Training Epoch: 192 [27008/50000]\tLoss: 0.1659\tLR: 0.000800\n",
            "Training Epoch: 192 [27136/50000]\tLoss: 0.1033\tLR: 0.000800\n",
            "Training Epoch: 192 [27264/50000]\tLoss: 0.1919\tLR: 0.000800\n",
            "Training Epoch: 192 [27392/50000]\tLoss: 0.1243\tLR: 0.000800\n",
            "Training Epoch: 192 [27520/50000]\tLoss: 0.2209\tLR: 0.000800\n",
            "Training Epoch: 192 [27648/50000]\tLoss: 0.1647\tLR: 0.000800\n",
            "Training Epoch: 192 [27776/50000]\tLoss: 0.1281\tLR: 0.000800\n",
            "Training Epoch: 192 [27904/50000]\tLoss: 0.1419\tLR: 0.000800\n",
            "Training Epoch: 192 [28032/50000]\tLoss: 0.1200\tLR: 0.000800\n",
            "Training Epoch: 192 [28160/50000]\tLoss: 0.1787\tLR: 0.000800\n",
            "Training Epoch: 192 [28288/50000]\tLoss: 0.1593\tLR: 0.000800\n",
            "Training Epoch: 192 [28416/50000]\tLoss: 0.2000\tLR: 0.000800\n",
            "Training Epoch: 192 [28544/50000]\tLoss: 0.1506\tLR: 0.000800\n",
            "Training Epoch: 192 [28672/50000]\tLoss: 0.1354\tLR: 0.000800\n",
            "Training Epoch: 192 [28800/50000]\tLoss: 0.1339\tLR: 0.000800\n",
            "Training Epoch: 192 [28928/50000]\tLoss: 0.1703\tLR: 0.000800\n",
            "Training Epoch: 192 [29056/50000]\tLoss: 0.1834\tLR: 0.000800\n",
            "Training Epoch: 192 [29184/50000]\tLoss: 0.1382\tLR: 0.000800\n",
            "Training Epoch: 192 [29312/50000]\tLoss: 0.1698\tLR: 0.000800\n",
            "Training Epoch: 192 [29440/50000]\tLoss: 0.1358\tLR: 0.000800\n",
            "Training Epoch: 192 [29568/50000]\tLoss: 0.1485\tLR: 0.000800\n",
            "Training Epoch: 192 [29696/50000]\tLoss: 0.1277\tLR: 0.000800\n",
            "Training Epoch: 192 [29824/50000]\tLoss: 0.1706\tLR: 0.000800\n",
            "Training Epoch: 192 [29952/50000]\tLoss: 0.0620\tLR: 0.000800\n",
            "Training Epoch: 192 [30080/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 192 [30208/50000]\tLoss: 0.1567\tLR: 0.000800\n",
            "Training Epoch: 192 [30336/50000]\tLoss: 0.1056\tLR: 0.000800\n",
            "Training Epoch: 192 [30464/50000]\tLoss: 0.1391\tLR: 0.000800\n",
            "Training Epoch: 192 [30592/50000]\tLoss: 0.1108\tLR: 0.000800\n",
            "Training Epoch: 192 [30720/50000]\tLoss: 0.1744\tLR: 0.000800\n",
            "Training Epoch: 192 [30848/50000]\tLoss: 0.1303\tLR: 0.000800\n",
            "Training Epoch: 192 [30976/50000]\tLoss: 0.1695\tLR: 0.000800\n",
            "Training Epoch: 192 [31104/50000]\tLoss: 0.1961\tLR: 0.000800\n",
            "Training Epoch: 192 [31232/50000]\tLoss: 0.1128\tLR: 0.000800\n",
            "Training Epoch: 192 [31360/50000]\tLoss: 0.1076\tLR: 0.000800\n",
            "Training Epoch: 192 [31488/50000]\tLoss: 0.1949\tLR: 0.000800\n",
            "Training Epoch: 192 [31616/50000]\tLoss: 0.0961\tLR: 0.000800\n",
            "Training Epoch: 192 [31744/50000]\tLoss: 0.1257\tLR: 0.000800\n",
            "Training Epoch: 192 [31872/50000]\tLoss: 0.1906\tLR: 0.000800\n",
            "Training Epoch: 192 [32000/50000]\tLoss: 0.1321\tLR: 0.000800\n",
            "Training Epoch: 192 [32128/50000]\tLoss: 0.1166\tLR: 0.000800\n",
            "Training Epoch: 192 [32256/50000]\tLoss: 0.0871\tLR: 0.000800\n",
            "Training Epoch: 192 [32384/50000]\tLoss: 0.1178\tLR: 0.000800\n",
            "Training Epoch: 192 [32512/50000]\tLoss: 0.2097\tLR: 0.000800\n",
            "Training Epoch: 192 [32640/50000]\tLoss: 0.1004\tLR: 0.000800\n",
            "Training Epoch: 192 [32768/50000]\tLoss: 0.2179\tLR: 0.000800\n",
            "Training Epoch: 192 [32896/50000]\tLoss: 0.2044\tLR: 0.000800\n",
            "Training Epoch: 192 [33024/50000]\tLoss: 0.0960\tLR: 0.000800\n",
            "Training Epoch: 192 [33152/50000]\tLoss: 0.1125\tLR: 0.000800\n",
            "Training Epoch: 192 [33280/50000]\tLoss: 0.1327\tLR: 0.000800\n",
            "Training Epoch: 192 [33408/50000]\tLoss: 0.1123\tLR: 0.000800\n",
            "Training Epoch: 192 [33536/50000]\tLoss: 0.1247\tLR: 0.000800\n",
            "Training Epoch: 192 [33664/50000]\tLoss: 0.1319\tLR: 0.000800\n",
            "Training Epoch: 192 [33792/50000]\tLoss: 0.1718\tLR: 0.000800\n",
            "Training Epoch: 192 [33920/50000]\tLoss: 0.1347\tLR: 0.000800\n",
            "Training Epoch: 192 [34048/50000]\tLoss: 0.1567\tLR: 0.000800\n",
            "Training Epoch: 192 [34176/50000]\tLoss: 0.1343\tLR: 0.000800\n",
            "Training Epoch: 192 [34304/50000]\tLoss: 0.1907\tLR: 0.000800\n",
            "Training Epoch: 192 [34432/50000]\tLoss: 0.1414\tLR: 0.000800\n",
            "Training Epoch: 192 [34560/50000]\tLoss: 0.0769\tLR: 0.000800\n",
            "Training Epoch: 192 [34688/50000]\tLoss: 0.1634\tLR: 0.000800\n",
            "Training Epoch: 192 [34816/50000]\tLoss: 0.1888\tLR: 0.000800\n",
            "Training Epoch: 192 [34944/50000]\tLoss: 0.2347\tLR: 0.000800\n",
            "Training Epoch: 192 [35072/50000]\tLoss: 0.1647\tLR: 0.000800\n",
            "Training Epoch: 192 [35200/50000]\tLoss: 0.1586\tLR: 0.000800\n",
            "Training Epoch: 192 [35328/50000]\tLoss: 0.1303\tLR: 0.000800\n",
            "Training Epoch: 192 [35456/50000]\tLoss: 0.1562\tLR: 0.000800\n",
            "Training Epoch: 192 [35584/50000]\tLoss: 0.1083\tLR: 0.000800\n",
            "Training Epoch: 192 [35712/50000]\tLoss: 0.1093\tLR: 0.000800\n",
            "Training Epoch: 192 [35840/50000]\tLoss: 0.1442\tLR: 0.000800\n",
            "Training Epoch: 192 [35968/50000]\tLoss: 0.1510\tLR: 0.000800\n",
            "Training Epoch: 192 [36096/50000]\tLoss: 0.1553\tLR: 0.000800\n",
            "Training Epoch: 192 [36224/50000]\tLoss: 0.1176\tLR: 0.000800\n",
            "Training Epoch: 192 [36352/50000]\tLoss: 0.2163\tLR: 0.000800\n",
            "Training Epoch: 192 [36480/50000]\tLoss: 0.2015\tLR: 0.000800\n",
            "Training Epoch: 192 [36608/50000]\tLoss: 0.1293\tLR: 0.000800\n",
            "Training Epoch: 192 [36736/50000]\tLoss: 0.1870\tLR: 0.000800\n",
            "Training Epoch: 192 [36864/50000]\tLoss: 0.2135\tLR: 0.000800\n",
            "Training Epoch: 192 [36992/50000]\tLoss: 0.0979\tLR: 0.000800\n",
            "Training Epoch: 192 [37120/50000]\tLoss: 0.1566\tLR: 0.000800\n",
            "Training Epoch: 192 [37248/50000]\tLoss: 0.1515\tLR: 0.000800\n",
            "Training Epoch: 192 [37376/50000]\tLoss: 0.1798\tLR: 0.000800\n",
            "Training Epoch: 192 [37504/50000]\tLoss: 0.0850\tLR: 0.000800\n",
            "Training Epoch: 192 [37632/50000]\tLoss: 0.0963\tLR: 0.000800\n",
            "Training Epoch: 192 [37760/50000]\tLoss: 0.1323\tLR: 0.000800\n",
            "Training Epoch: 192 [37888/50000]\tLoss: 0.1417\tLR: 0.000800\n",
            "Training Epoch: 192 [38016/50000]\tLoss: 0.1624\tLR: 0.000800\n",
            "Training Epoch: 192 [38144/50000]\tLoss: 0.2082\tLR: 0.000800\n",
            "Training Epoch: 192 [38272/50000]\tLoss: 0.1307\tLR: 0.000800\n",
            "Training Epoch: 192 [38400/50000]\tLoss: 0.0903\tLR: 0.000800\n",
            "Training Epoch: 192 [38528/50000]\tLoss: 0.1637\tLR: 0.000800\n",
            "Training Epoch: 192 [38656/50000]\tLoss: 0.1641\tLR: 0.000800\n",
            "Training Epoch: 192 [38784/50000]\tLoss: 0.1444\tLR: 0.000800\n",
            "Training Epoch: 192 [38912/50000]\tLoss: 0.1902\tLR: 0.000800\n",
            "Training Epoch: 192 [39040/50000]\tLoss: 0.1024\tLR: 0.000800\n",
            "Training Epoch: 192 [39168/50000]\tLoss: 0.0946\tLR: 0.000800\n",
            "Training Epoch: 192 [39296/50000]\tLoss: 0.2013\tLR: 0.000800\n",
            "Training Epoch: 192 [39424/50000]\tLoss: 0.1916\tLR: 0.000800\n",
            "Training Epoch: 192 [39552/50000]\tLoss: 0.2068\tLR: 0.000800\n",
            "Training Epoch: 192 [39680/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 192 [39808/50000]\tLoss: 0.1710\tLR: 0.000800\n",
            "Training Epoch: 192 [39936/50000]\tLoss: 0.1783\tLR: 0.000800\n",
            "Training Epoch: 192 [40064/50000]\tLoss: 0.1161\tLR: 0.000800\n",
            "Training Epoch: 192 [40192/50000]\tLoss: 0.1200\tLR: 0.000800\n",
            "Training Epoch: 192 [40320/50000]\tLoss: 0.1805\tLR: 0.000800\n",
            "Training Epoch: 192 [40448/50000]\tLoss: 0.1184\tLR: 0.000800\n",
            "Training Epoch: 192 [40576/50000]\tLoss: 0.1501\tLR: 0.000800\n",
            "Training Epoch: 192 [40704/50000]\tLoss: 0.0901\tLR: 0.000800\n",
            "Training Epoch: 192 [40832/50000]\tLoss: 0.1442\tLR: 0.000800\n",
            "Training Epoch: 192 [40960/50000]\tLoss: 0.1668\tLR: 0.000800\n",
            "Training Epoch: 192 [41088/50000]\tLoss: 0.2247\tLR: 0.000800\n",
            "Training Epoch: 192 [41216/50000]\tLoss: 0.2015\tLR: 0.000800\n",
            "Training Epoch: 192 [41344/50000]\tLoss: 0.1598\tLR: 0.000800\n",
            "Training Epoch: 192 [41472/50000]\tLoss: 0.1683\tLR: 0.000800\n",
            "Training Epoch: 192 [41600/50000]\tLoss: 0.1780\tLR: 0.000800\n",
            "Training Epoch: 192 [41728/50000]\tLoss: 0.1313\tLR: 0.000800\n",
            "Training Epoch: 192 [41856/50000]\tLoss: 0.1525\tLR: 0.000800\n",
            "Training Epoch: 192 [41984/50000]\tLoss: 0.1658\tLR: 0.000800\n",
            "Training Epoch: 192 [42112/50000]\tLoss: 0.1480\tLR: 0.000800\n",
            "Training Epoch: 192 [42240/50000]\tLoss: 0.1144\tLR: 0.000800\n",
            "Training Epoch: 192 [42368/50000]\tLoss: 0.1525\tLR: 0.000800\n",
            "Training Epoch: 192 [42496/50000]\tLoss: 0.1721\tLR: 0.000800\n",
            "Training Epoch: 192 [42624/50000]\tLoss: 0.1838\tLR: 0.000800\n",
            "Training Epoch: 192 [42752/50000]\tLoss: 0.1554\tLR: 0.000800\n",
            "Training Epoch: 192 [42880/50000]\tLoss: 0.1566\tLR: 0.000800\n",
            "Training Epoch: 192 [43008/50000]\tLoss: 0.1688\tLR: 0.000800\n",
            "Training Epoch: 192 [43136/50000]\tLoss: 0.1307\tLR: 0.000800\n",
            "Training Epoch: 192 [43264/50000]\tLoss: 0.1932\tLR: 0.000800\n",
            "Training Epoch: 192 [43392/50000]\tLoss: 0.1138\tLR: 0.000800\n",
            "Training Epoch: 192 [43520/50000]\tLoss: 0.1280\tLR: 0.000800\n",
            "Training Epoch: 192 [43648/50000]\tLoss: 0.1456\tLR: 0.000800\n",
            "Training Epoch: 192 [43776/50000]\tLoss: 0.2029\tLR: 0.000800\n",
            "Training Epoch: 192 [43904/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 192 [44032/50000]\tLoss: 0.1722\tLR: 0.000800\n",
            "Training Epoch: 192 [44160/50000]\tLoss: 0.1740\tLR: 0.000800\n",
            "Training Epoch: 192 [44288/50000]\tLoss: 0.1414\tLR: 0.000800\n",
            "Training Epoch: 192 [44416/50000]\tLoss: 0.1753\tLR: 0.000800\n",
            "Training Epoch: 192 [44544/50000]\tLoss: 0.1262\tLR: 0.000800\n",
            "Training Epoch: 192 [44672/50000]\tLoss: 0.1413\tLR: 0.000800\n",
            "Training Epoch: 192 [44800/50000]\tLoss: 0.1311\tLR: 0.000800\n",
            "Training Epoch: 192 [44928/50000]\tLoss: 0.1206\tLR: 0.000800\n",
            "Training Epoch: 192 [45056/50000]\tLoss: 0.1326\tLR: 0.000800\n",
            "Training Epoch: 192 [45184/50000]\tLoss: 0.1057\tLR: 0.000800\n",
            "Training Epoch: 192 [45312/50000]\tLoss: 0.1314\tLR: 0.000800\n",
            "Training Epoch: 192 [45440/50000]\tLoss: 0.2138\tLR: 0.000800\n",
            "Training Epoch: 192 [45568/50000]\tLoss: 0.1209\tLR: 0.000800\n",
            "Training Epoch: 192 [45696/50000]\tLoss: 0.1278\tLR: 0.000800\n",
            "Training Epoch: 192 [45824/50000]\tLoss: 0.1432\tLR: 0.000800\n",
            "Training Epoch: 192 [45952/50000]\tLoss: 0.1706\tLR: 0.000800\n",
            "Training Epoch: 192 [46080/50000]\tLoss: 0.1245\tLR: 0.000800\n",
            "Training Epoch: 192 [46208/50000]\tLoss: 0.1669\tLR: 0.000800\n",
            "Training Epoch: 192 [46336/50000]\tLoss: 0.1250\tLR: 0.000800\n",
            "Training Epoch: 192 [46464/50000]\tLoss: 0.1281\tLR: 0.000800\n",
            "Training Epoch: 192 [46592/50000]\tLoss: 0.1323\tLR: 0.000800\n",
            "Training Epoch: 192 [46720/50000]\tLoss: 0.1440\tLR: 0.000800\n",
            "Training Epoch: 192 [46848/50000]\tLoss: 0.1501\tLR: 0.000800\n",
            "Training Epoch: 192 [46976/50000]\tLoss: 0.1670\tLR: 0.000800\n",
            "Training Epoch: 192 [47104/50000]\tLoss: 0.1561\tLR: 0.000800\n",
            "Training Epoch: 192 [47232/50000]\tLoss: 0.0993\tLR: 0.000800\n",
            "Training Epoch: 192 [47360/50000]\tLoss: 0.1433\tLR: 0.000800\n",
            "Training Epoch: 192 [47488/50000]\tLoss: 0.1380\tLR: 0.000800\n",
            "Training Epoch: 192 [47616/50000]\tLoss: 0.1795\tLR: 0.000800\n",
            "Training Epoch: 192 [47744/50000]\tLoss: 0.1604\tLR: 0.000800\n",
            "Training Epoch: 192 [47872/50000]\tLoss: 0.1056\tLR: 0.000800\n",
            "Training Epoch: 192 [48000/50000]\tLoss: 0.2006\tLR: 0.000800\n",
            "Training Epoch: 192 [48128/50000]\tLoss: 0.1505\tLR: 0.000800\n",
            "Training Epoch: 192 [48256/50000]\tLoss: 0.1892\tLR: 0.000800\n",
            "Training Epoch: 192 [48384/50000]\tLoss: 0.1258\tLR: 0.000800\n",
            "Training Epoch: 192 [48512/50000]\tLoss: 0.1302\tLR: 0.000800\n",
            "Training Epoch: 192 [48640/50000]\tLoss: 0.1340\tLR: 0.000800\n",
            "Training Epoch: 192 [48768/50000]\tLoss: 0.1546\tLR: 0.000800\n",
            "Training Epoch: 192 [48896/50000]\tLoss: 0.1394\tLR: 0.000800\n",
            "Training Epoch: 192 [49024/50000]\tLoss: 0.1237\tLR: 0.000800\n",
            "Training Epoch: 192 [49152/50000]\tLoss: 0.1491\tLR: 0.000800\n",
            "Training Epoch: 192 [49280/50000]\tLoss: 0.1036\tLR: 0.000800\n",
            "Training Epoch: 192 [49408/50000]\tLoss: 0.1567\tLR: 0.000800\n",
            "Training Epoch: 192 [49536/50000]\tLoss: 0.1793\tLR: 0.000800\n",
            "Training Epoch: 192 [49664/50000]\tLoss: 0.1341\tLR: 0.000800\n",
            "Training Epoch: 192 [49792/50000]\tLoss: 0.1242\tLR: 0.000800\n",
            "Training Epoch: 192 [49920/50000]\tLoss: 0.1081\tLR: 0.000800\n",
            "Training Epoch: 192 [50000/50000]\tLoss: 0.2217\tLR: 0.000800\n",
            "epoch 192 training time consumed: 27.90s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  86404 GiB |  86404 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  84759 GiB |  84759 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1644 GiB |   1644 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  86404 GiB |  86404 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  84759 GiB |  84759 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1644 GiB |   1644 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  86147 GiB |  86147 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  84504 GiB |  84504 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1643 GiB |   1643 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  52862 GiB |  52862 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  51163 GiB |  51163 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1699 GiB |   1699 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   33480 K  |   33479 K  |\n",
            "|       from large pool |       8    |      61    |   11231 K  |   11231 K  |\n",
            "|       from small pool |     373    |     464    |   22248 K  |   22247 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   33480 K  |   33479 K  |\n",
            "|       from large pool |       8    |      61    |   11231 K  |   11231 K  |\n",
            "|       from small pool |     373    |     464    |   22248 K  |   22247 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   11007 K  |   11007 K  |\n",
            "|       from large pool |       5    |      14    |    5354 K  |    5354 K  |\n",
            "|       from small pool |      10    |      16    |    5652 K  |    5652 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 192, Average loss: 0.0112, Accuracy: 0.6665, Time consumed:2.70s\n",
            "\n",
            "Training Epoch: 193 [128/50000]\tLoss: 0.1182\tLR: 0.000800\n",
            "Training Epoch: 193 [256/50000]\tLoss: 0.1308\tLR: 0.000800\n",
            "Training Epoch: 193 [384/50000]\tLoss: 0.1121\tLR: 0.000800\n",
            "Training Epoch: 193 [512/50000]\tLoss: 0.1418\tLR: 0.000800\n",
            "Training Epoch: 193 [640/50000]\tLoss: 0.0996\tLR: 0.000800\n",
            "Training Epoch: 193 [768/50000]\tLoss: 0.1305\tLR: 0.000800\n",
            "Training Epoch: 193 [896/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 193 [1024/50000]\tLoss: 0.1431\tLR: 0.000800\n",
            "Training Epoch: 193 [1152/50000]\tLoss: 0.1816\tLR: 0.000800\n",
            "Training Epoch: 193 [1280/50000]\tLoss: 0.1680\tLR: 0.000800\n",
            "Training Epoch: 193 [1408/50000]\tLoss: 0.1169\tLR: 0.000800\n",
            "Training Epoch: 193 [1536/50000]\tLoss: 0.1558\tLR: 0.000800\n",
            "Training Epoch: 193 [1664/50000]\tLoss: 0.1108\tLR: 0.000800\n",
            "Training Epoch: 193 [1792/50000]\tLoss: 0.1328\tLR: 0.000800\n",
            "Training Epoch: 193 [1920/50000]\tLoss: 0.1634\tLR: 0.000800\n",
            "Training Epoch: 193 [2048/50000]\tLoss: 0.1691\tLR: 0.000800\n",
            "Training Epoch: 193 [2176/50000]\tLoss: 0.1119\tLR: 0.000800\n",
            "Training Epoch: 193 [2304/50000]\tLoss: 0.1540\tLR: 0.000800\n",
            "Training Epoch: 193 [2432/50000]\tLoss: 0.1241\tLR: 0.000800\n",
            "Training Epoch: 193 [2560/50000]\tLoss: 0.0884\tLR: 0.000800\n",
            "Training Epoch: 193 [2688/50000]\tLoss: 0.1613\tLR: 0.000800\n",
            "Training Epoch: 193 [2816/50000]\tLoss: 0.0929\tLR: 0.000800\n",
            "Training Epoch: 193 [2944/50000]\tLoss: 0.1369\tLR: 0.000800\n",
            "Training Epoch: 193 [3072/50000]\tLoss: 0.1420\tLR: 0.000800\n",
            "Training Epoch: 193 [3200/50000]\tLoss: 0.1078\tLR: 0.000800\n",
            "Training Epoch: 193 [3328/50000]\tLoss: 0.1240\tLR: 0.000800\n",
            "Training Epoch: 193 [3456/50000]\tLoss: 0.1526\tLR: 0.000800\n",
            "Training Epoch: 193 [3584/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 193 [3712/50000]\tLoss: 0.1711\tLR: 0.000800\n",
            "Training Epoch: 193 [3840/50000]\tLoss: 0.1234\tLR: 0.000800\n",
            "Training Epoch: 193 [3968/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 193 [4096/50000]\tLoss: 0.1577\tLR: 0.000800\n",
            "Training Epoch: 193 [4224/50000]\tLoss: 0.1018\tLR: 0.000800\n",
            "Training Epoch: 193 [4352/50000]\tLoss: 0.1523\tLR: 0.000800\n",
            "Training Epoch: 193 [4480/50000]\tLoss: 0.0988\tLR: 0.000800\n",
            "Training Epoch: 193 [4608/50000]\tLoss: 0.1472\tLR: 0.000800\n",
            "Training Epoch: 193 [4736/50000]\tLoss: 0.1197\tLR: 0.000800\n",
            "Training Epoch: 193 [4864/50000]\tLoss: 0.1606\tLR: 0.000800\n",
            "Training Epoch: 193 [4992/50000]\tLoss: 0.1517\tLR: 0.000800\n",
            "Training Epoch: 193 [5120/50000]\tLoss: 0.1055\tLR: 0.000800\n",
            "Training Epoch: 193 [5248/50000]\tLoss: 0.1689\tLR: 0.000800\n",
            "Training Epoch: 193 [5376/50000]\tLoss: 0.1311\tLR: 0.000800\n",
            "Training Epoch: 193 [5504/50000]\tLoss: 0.1431\tLR: 0.000800\n",
            "Training Epoch: 193 [5632/50000]\tLoss: 0.1135\tLR: 0.000800\n",
            "Training Epoch: 193 [5760/50000]\tLoss: 0.2210\tLR: 0.000800\n",
            "Training Epoch: 193 [5888/50000]\tLoss: 0.1548\tLR: 0.000800\n",
            "Training Epoch: 193 [6016/50000]\tLoss: 0.1035\tLR: 0.000800\n",
            "Training Epoch: 193 [6144/50000]\tLoss: 0.1156\tLR: 0.000800\n",
            "Training Epoch: 193 [6272/50000]\tLoss: 0.0954\tLR: 0.000800\n",
            "Training Epoch: 193 [6400/50000]\tLoss: 0.2070\tLR: 0.000800\n",
            "Training Epoch: 193 [6528/50000]\tLoss: 0.0860\tLR: 0.000800\n",
            "Training Epoch: 193 [6656/50000]\tLoss: 0.1563\tLR: 0.000800\n",
            "Training Epoch: 193 [6784/50000]\tLoss: 0.0999\tLR: 0.000800\n",
            "Training Epoch: 193 [6912/50000]\tLoss: 0.2194\tLR: 0.000800\n",
            "Training Epoch: 193 [7040/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 193 [7168/50000]\tLoss: 0.1028\tLR: 0.000800\n",
            "Training Epoch: 193 [7296/50000]\tLoss: 0.1706\tLR: 0.000800\n",
            "Training Epoch: 193 [7424/50000]\tLoss: 0.1537\tLR: 0.000800\n",
            "Training Epoch: 193 [7552/50000]\tLoss: 0.1070\tLR: 0.000800\n",
            "Training Epoch: 193 [7680/50000]\tLoss: 0.1338\tLR: 0.000800\n",
            "Training Epoch: 193 [7808/50000]\tLoss: 0.0840\tLR: 0.000800\n",
            "Training Epoch: 193 [7936/50000]\tLoss: 0.1612\tLR: 0.000800\n",
            "Training Epoch: 193 [8064/50000]\tLoss: 0.1405\tLR: 0.000800\n",
            "Training Epoch: 193 [8192/50000]\tLoss: 0.1170\tLR: 0.000800\n",
            "Training Epoch: 193 [8320/50000]\tLoss: 0.1136\tLR: 0.000800\n",
            "Training Epoch: 193 [8448/50000]\tLoss: 0.1999\tLR: 0.000800\n",
            "Training Epoch: 193 [8576/50000]\tLoss: 0.1616\tLR: 0.000800\n",
            "Training Epoch: 193 [8704/50000]\tLoss: 0.1223\tLR: 0.000800\n",
            "Training Epoch: 193 [8832/50000]\tLoss: 0.1520\tLR: 0.000800\n",
            "Training Epoch: 193 [8960/50000]\tLoss: 0.0969\tLR: 0.000800\n",
            "Training Epoch: 193 [9088/50000]\tLoss: 0.1646\tLR: 0.000800\n",
            "Training Epoch: 193 [9216/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 193 [9344/50000]\tLoss: 0.1620\tLR: 0.000800\n",
            "Training Epoch: 193 [9472/50000]\tLoss: 0.1691\tLR: 0.000800\n",
            "Training Epoch: 193 [9600/50000]\tLoss: 0.1299\tLR: 0.000800\n",
            "Training Epoch: 193 [9728/50000]\tLoss: 0.0973\tLR: 0.000800\n",
            "Training Epoch: 193 [9856/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 193 [9984/50000]\tLoss: 0.2151\tLR: 0.000800\n",
            "Training Epoch: 193 [10112/50000]\tLoss: 0.1723\tLR: 0.000800\n",
            "Training Epoch: 193 [10240/50000]\tLoss: 0.1677\tLR: 0.000800\n",
            "Training Epoch: 193 [10368/50000]\tLoss: 0.0873\tLR: 0.000800\n",
            "Training Epoch: 193 [10496/50000]\tLoss: 0.1449\tLR: 0.000800\n",
            "Training Epoch: 193 [10624/50000]\tLoss: 0.0913\tLR: 0.000800\n",
            "Training Epoch: 193 [10752/50000]\tLoss: 0.1715\tLR: 0.000800\n",
            "Training Epoch: 193 [10880/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 193 [11008/50000]\tLoss: 0.1327\tLR: 0.000800\n",
            "Training Epoch: 193 [11136/50000]\tLoss: 0.0957\tLR: 0.000800\n",
            "Training Epoch: 193 [11264/50000]\tLoss: 0.0965\tLR: 0.000800\n",
            "Training Epoch: 193 [11392/50000]\tLoss: 0.1644\tLR: 0.000800\n",
            "Training Epoch: 193 [11520/50000]\tLoss: 0.1460\tLR: 0.000800\n",
            "Training Epoch: 193 [11648/50000]\tLoss: 0.2863\tLR: 0.000800\n",
            "Training Epoch: 193 [11776/50000]\tLoss: 0.0867\tLR: 0.000800\n",
            "Training Epoch: 193 [11904/50000]\tLoss: 0.1070\tLR: 0.000800\n",
            "Training Epoch: 193 [12032/50000]\tLoss: 0.1665\tLR: 0.000800\n",
            "Training Epoch: 193 [12160/50000]\tLoss: 0.1658\tLR: 0.000800\n",
            "Training Epoch: 193 [12288/50000]\tLoss: 0.1806\tLR: 0.000800\n",
            "Training Epoch: 193 [12416/50000]\tLoss: 0.1316\tLR: 0.000800\n",
            "Training Epoch: 193 [12544/50000]\tLoss: 0.1948\tLR: 0.000800\n",
            "Training Epoch: 193 [12672/50000]\tLoss: 0.1495\tLR: 0.000800\n",
            "Training Epoch: 193 [12800/50000]\tLoss: 0.0862\tLR: 0.000800\n",
            "Training Epoch: 193 [12928/50000]\tLoss: 0.1195\tLR: 0.000800\n",
            "Training Epoch: 193 [13056/50000]\tLoss: 0.1066\tLR: 0.000800\n",
            "Training Epoch: 193 [13184/50000]\tLoss: 0.1799\tLR: 0.000800\n",
            "Training Epoch: 193 [13312/50000]\tLoss: 0.0885\tLR: 0.000800\n",
            "Training Epoch: 193 [13440/50000]\tLoss: 0.1315\tLR: 0.000800\n",
            "Training Epoch: 193 [13568/50000]\tLoss: 0.1764\tLR: 0.000800\n",
            "Training Epoch: 193 [13696/50000]\tLoss: 0.1504\tLR: 0.000800\n",
            "Training Epoch: 193 [13824/50000]\tLoss: 0.1324\tLR: 0.000800\n",
            "Training Epoch: 193 [13952/50000]\tLoss: 0.1750\tLR: 0.000800\n",
            "Training Epoch: 193 [14080/50000]\tLoss: 0.1550\tLR: 0.000800\n",
            "Training Epoch: 193 [14208/50000]\tLoss: 0.1374\tLR: 0.000800\n",
            "Training Epoch: 193 [14336/50000]\tLoss: 0.1785\tLR: 0.000800\n",
            "Training Epoch: 193 [14464/50000]\tLoss: 0.1714\tLR: 0.000800\n",
            "Training Epoch: 193 [14592/50000]\tLoss: 0.1321\tLR: 0.000800\n",
            "Training Epoch: 193 [14720/50000]\tLoss: 0.1699\tLR: 0.000800\n",
            "Training Epoch: 193 [14848/50000]\tLoss: 0.1327\tLR: 0.000800\n",
            "Training Epoch: 193 [14976/50000]\tLoss: 0.1357\tLR: 0.000800\n",
            "Training Epoch: 193 [15104/50000]\tLoss: 0.1935\tLR: 0.000800\n",
            "Training Epoch: 193 [15232/50000]\tLoss: 0.1523\tLR: 0.000800\n",
            "Training Epoch: 193 [15360/50000]\tLoss: 0.1346\tLR: 0.000800\n",
            "Training Epoch: 193 [15488/50000]\tLoss: 0.1288\tLR: 0.000800\n",
            "Training Epoch: 193 [15616/50000]\tLoss: 0.1001\tLR: 0.000800\n",
            "Training Epoch: 193 [15744/50000]\tLoss: 0.1443\tLR: 0.000800\n",
            "Training Epoch: 193 [15872/50000]\tLoss: 0.1641\tLR: 0.000800\n",
            "Training Epoch: 193 [16000/50000]\tLoss: 0.1918\tLR: 0.000800\n",
            "Training Epoch: 193 [16128/50000]\tLoss: 0.1554\tLR: 0.000800\n",
            "Training Epoch: 193 [16256/50000]\tLoss: 0.1235\tLR: 0.000800\n",
            "Training Epoch: 193 [16384/50000]\tLoss: 0.1103\tLR: 0.000800\n",
            "Training Epoch: 193 [16512/50000]\tLoss: 0.1252\tLR: 0.000800\n",
            "Training Epoch: 193 [16640/50000]\tLoss: 0.1434\tLR: 0.000800\n",
            "Training Epoch: 193 [16768/50000]\tLoss: 0.1443\tLR: 0.000800\n",
            "Training Epoch: 193 [16896/50000]\tLoss: 0.2113\tLR: 0.000800\n",
            "Training Epoch: 193 [17024/50000]\tLoss: 0.1221\tLR: 0.000800\n",
            "Training Epoch: 193 [17152/50000]\tLoss: 0.1147\tLR: 0.000800\n",
            "Training Epoch: 193 [17280/50000]\tLoss: 0.1617\tLR: 0.000800\n",
            "Training Epoch: 193 [17408/50000]\tLoss: 0.1108\tLR: 0.000800\n",
            "Training Epoch: 193 [17536/50000]\tLoss: 0.2251\tLR: 0.000800\n",
            "Training Epoch: 193 [17664/50000]\tLoss: 0.1546\tLR: 0.000800\n",
            "Training Epoch: 193 [17792/50000]\tLoss: 0.1248\tLR: 0.000800\n",
            "Training Epoch: 193 [17920/50000]\tLoss: 0.1378\tLR: 0.000800\n",
            "Training Epoch: 193 [18048/50000]\tLoss: 0.0875\tLR: 0.000800\n",
            "Training Epoch: 193 [18176/50000]\tLoss: 0.1197\tLR: 0.000800\n",
            "Training Epoch: 193 [18304/50000]\tLoss: 0.0967\tLR: 0.000800\n",
            "Training Epoch: 193 [18432/50000]\tLoss: 0.1495\tLR: 0.000800\n",
            "Training Epoch: 193 [18560/50000]\tLoss: 0.1230\tLR: 0.000800\n",
            "Training Epoch: 193 [18688/50000]\tLoss: 0.1429\tLR: 0.000800\n",
            "Training Epoch: 193 [18816/50000]\tLoss: 0.1161\tLR: 0.000800\n",
            "Training Epoch: 193 [18944/50000]\tLoss: 0.1644\tLR: 0.000800\n",
            "Training Epoch: 193 [19072/50000]\tLoss: 0.0958\tLR: 0.000800\n",
            "Training Epoch: 193 [19200/50000]\tLoss: 0.1730\tLR: 0.000800\n",
            "Training Epoch: 193 [19328/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 193 [19456/50000]\tLoss: 0.1407\tLR: 0.000800\n",
            "Training Epoch: 193 [19584/50000]\tLoss: 0.1914\tLR: 0.000800\n",
            "Training Epoch: 193 [19712/50000]\tLoss: 0.0962\tLR: 0.000800\n",
            "Training Epoch: 193 [19840/50000]\tLoss: 0.1065\tLR: 0.000800\n",
            "Training Epoch: 193 [19968/50000]\tLoss: 0.1827\tLR: 0.000800\n",
            "Training Epoch: 193 [20096/50000]\tLoss: 0.1089\tLR: 0.000800\n",
            "Training Epoch: 193 [20224/50000]\tLoss: 0.1081\tLR: 0.000800\n",
            "Training Epoch: 193 [20352/50000]\tLoss: 0.1405\tLR: 0.000800\n",
            "Training Epoch: 193 [20480/50000]\tLoss: 0.1631\tLR: 0.000800\n",
            "Training Epoch: 193 [20608/50000]\tLoss: 0.0681\tLR: 0.000800\n",
            "Training Epoch: 193 [20736/50000]\tLoss: 0.2130\tLR: 0.000800\n",
            "Training Epoch: 193 [20864/50000]\tLoss: 0.1326\tLR: 0.000800\n",
            "Training Epoch: 193 [20992/50000]\tLoss: 0.0962\tLR: 0.000800\n",
            "Training Epoch: 193 [21120/50000]\tLoss: 0.1934\tLR: 0.000800\n",
            "Training Epoch: 193 [21248/50000]\tLoss: 0.1618\tLR: 0.000800\n",
            "Training Epoch: 193 [21376/50000]\tLoss: 0.1364\tLR: 0.000800\n",
            "Training Epoch: 193 [21504/50000]\tLoss: 0.1176\tLR: 0.000800\n",
            "Training Epoch: 193 [21632/50000]\tLoss: 0.1095\tLR: 0.000800\n",
            "Training Epoch: 193 [21760/50000]\tLoss: 0.1154\tLR: 0.000800\n",
            "Training Epoch: 193 [21888/50000]\tLoss: 0.1451\tLR: 0.000800\n",
            "Training Epoch: 193 [22016/50000]\tLoss: 0.1494\tLR: 0.000800\n",
            "Training Epoch: 193 [22144/50000]\tLoss: 0.1634\tLR: 0.000800\n",
            "Training Epoch: 193 [22272/50000]\tLoss: 0.1522\tLR: 0.000800\n",
            "Training Epoch: 193 [22400/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 193 [22528/50000]\tLoss: 0.1209\tLR: 0.000800\n",
            "Training Epoch: 193 [22656/50000]\tLoss: 0.1348\tLR: 0.000800\n",
            "Training Epoch: 193 [22784/50000]\tLoss: 0.1901\tLR: 0.000800\n",
            "Training Epoch: 193 [22912/50000]\tLoss: 0.1338\tLR: 0.000800\n",
            "Training Epoch: 193 [23040/50000]\tLoss: 0.1580\tLR: 0.000800\n",
            "Training Epoch: 193 [23168/50000]\tLoss: 0.1539\tLR: 0.000800\n",
            "Training Epoch: 193 [23296/50000]\tLoss: 0.0925\tLR: 0.000800\n",
            "Training Epoch: 193 [23424/50000]\tLoss: 0.1903\tLR: 0.000800\n",
            "Training Epoch: 193 [23552/50000]\tLoss: 0.1584\tLR: 0.000800\n",
            "Training Epoch: 193 [23680/50000]\tLoss: 0.1328\tLR: 0.000800\n",
            "Training Epoch: 193 [23808/50000]\tLoss: 0.1588\tLR: 0.000800\n",
            "Training Epoch: 193 [23936/50000]\tLoss: 0.1846\tLR: 0.000800\n",
            "Training Epoch: 193 [24064/50000]\tLoss: 0.1552\tLR: 0.000800\n",
            "Training Epoch: 193 [24192/50000]\tLoss: 0.0951\tLR: 0.000800\n",
            "Training Epoch: 193 [24320/50000]\tLoss: 0.1513\tLR: 0.000800\n",
            "Training Epoch: 193 [24448/50000]\tLoss: 0.1220\tLR: 0.000800\n",
            "Training Epoch: 193 [24576/50000]\tLoss: 0.1921\tLR: 0.000800\n",
            "Training Epoch: 193 [24704/50000]\tLoss: 0.1243\tLR: 0.000800\n",
            "Training Epoch: 193 [24832/50000]\tLoss: 0.1429\tLR: 0.000800\n",
            "Training Epoch: 193 [24960/50000]\tLoss: 0.1217\tLR: 0.000800\n",
            "Training Epoch: 193 [25088/50000]\tLoss: 0.0883\tLR: 0.000800\n",
            "Training Epoch: 193 [25216/50000]\tLoss: 0.1697\tLR: 0.000800\n",
            "Training Epoch: 193 [25344/50000]\tLoss: 0.1789\tLR: 0.000800\n",
            "Training Epoch: 193 [25472/50000]\tLoss: 0.1292\tLR: 0.000800\n",
            "Training Epoch: 193 [25600/50000]\tLoss: 0.1062\tLR: 0.000800\n",
            "Training Epoch: 193 [25728/50000]\tLoss: 0.1108\tLR: 0.000800\n",
            "Training Epoch: 193 [25856/50000]\tLoss: 0.1049\tLR: 0.000800\n",
            "Training Epoch: 193 [25984/50000]\tLoss: 0.1488\tLR: 0.000800\n",
            "Training Epoch: 193 [26112/50000]\tLoss: 0.1270\tLR: 0.000800\n",
            "Training Epoch: 193 [26240/50000]\tLoss: 0.1444\tLR: 0.000800\n",
            "Training Epoch: 193 [26368/50000]\tLoss: 0.0877\tLR: 0.000800\n",
            "Training Epoch: 193 [26496/50000]\tLoss: 0.1015\tLR: 0.000800\n",
            "Training Epoch: 193 [26624/50000]\tLoss: 0.1856\tLR: 0.000800\n",
            "Training Epoch: 193 [26752/50000]\tLoss: 0.1250\tLR: 0.000800\n",
            "Training Epoch: 193 [26880/50000]\tLoss: 0.1635\tLR: 0.000800\n",
            "Training Epoch: 193 [27008/50000]\tLoss: 0.1068\tLR: 0.000800\n",
            "Training Epoch: 193 [27136/50000]\tLoss: 0.0824\tLR: 0.000800\n",
            "Training Epoch: 193 [27264/50000]\tLoss: 0.2081\tLR: 0.000800\n",
            "Training Epoch: 193 [27392/50000]\tLoss: 0.0831\tLR: 0.000800\n",
            "Training Epoch: 193 [27520/50000]\tLoss: 0.1981\tLR: 0.000800\n",
            "Training Epoch: 193 [27648/50000]\tLoss: 0.1330\tLR: 0.000800\n",
            "Training Epoch: 193 [27776/50000]\tLoss: 0.1298\tLR: 0.000800\n",
            "Training Epoch: 193 [27904/50000]\tLoss: 0.1498\tLR: 0.000800\n",
            "Training Epoch: 193 [28032/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 193 [28160/50000]\tLoss: 0.1246\tLR: 0.000800\n",
            "Training Epoch: 193 [28288/50000]\tLoss: 0.1944\tLR: 0.000800\n",
            "Training Epoch: 193 [28416/50000]\tLoss: 0.1519\tLR: 0.000800\n",
            "Training Epoch: 193 [28544/50000]\tLoss: 0.1110\tLR: 0.000800\n",
            "Training Epoch: 193 [28672/50000]\tLoss: 0.1608\tLR: 0.000800\n",
            "Training Epoch: 193 [28800/50000]\tLoss: 0.2557\tLR: 0.000800\n",
            "Training Epoch: 193 [28928/50000]\tLoss: 0.1478\tLR: 0.000800\n",
            "Training Epoch: 193 [29056/50000]\tLoss: 0.1668\tLR: 0.000800\n",
            "Training Epoch: 193 [29184/50000]\tLoss: 0.1911\tLR: 0.000800\n",
            "Training Epoch: 193 [29312/50000]\tLoss: 0.1559\tLR: 0.000800\n",
            "Training Epoch: 193 [29440/50000]\tLoss: 0.1494\tLR: 0.000800\n",
            "Training Epoch: 193 [29568/50000]\tLoss: 0.1812\tLR: 0.000800\n",
            "Training Epoch: 193 [29696/50000]\tLoss: 0.1482\tLR: 0.000800\n",
            "Training Epoch: 193 [29824/50000]\tLoss: 0.1856\tLR: 0.000800\n",
            "Training Epoch: 193 [29952/50000]\tLoss: 0.1657\tLR: 0.000800\n",
            "Training Epoch: 193 [30080/50000]\tLoss: 0.1538\tLR: 0.000800\n",
            "Training Epoch: 193 [30208/50000]\tLoss: 0.1227\tLR: 0.000800\n",
            "Training Epoch: 193 [30336/50000]\tLoss: 0.1279\tLR: 0.000800\n",
            "Training Epoch: 193 [30464/50000]\tLoss: 0.1100\tLR: 0.000800\n",
            "Training Epoch: 193 [30592/50000]\tLoss: 0.0838\tLR: 0.000800\n",
            "Training Epoch: 193 [30720/50000]\tLoss: 0.1703\tLR: 0.000800\n",
            "Training Epoch: 193 [30848/50000]\tLoss: 0.1391\tLR: 0.000800\n",
            "Training Epoch: 193 [30976/50000]\tLoss: 0.1436\tLR: 0.000800\n",
            "Training Epoch: 193 [31104/50000]\tLoss: 0.2224\tLR: 0.000800\n",
            "Training Epoch: 193 [31232/50000]\tLoss: 0.1354\tLR: 0.000800\n",
            "Training Epoch: 193 [31360/50000]\tLoss: 0.1032\tLR: 0.000800\n",
            "Training Epoch: 193 [31488/50000]\tLoss: 0.1484\tLR: 0.000800\n",
            "Training Epoch: 193 [31616/50000]\tLoss: 0.0833\tLR: 0.000800\n",
            "Training Epoch: 193 [31744/50000]\tLoss: 0.1259\tLR: 0.000800\n",
            "Training Epoch: 193 [31872/50000]\tLoss: 0.1398\tLR: 0.000800\n",
            "Training Epoch: 193 [32000/50000]\tLoss: 0.1568\tLR: 0.000800\n",
            "Training Epoch: 193 [32128/50000]\tLoss: 0.1223\tLR: 0.000800\n",
            "Training Epoch: 193 [32256/50000]\tLoss: 0.1097\tLR: 0.000800\n",
            "Training Epoch: 193 [32384/50000]\tLoss: 0.1614\tLR: 0.000800\n",
            "Training Epoch: 193 [32512/50000]\tLoss: 0.1697\tLR: 0.000800\n",
            "Training Epoch: 193 [32640/50000]\tLoss: 0.1126\tLR: 0.000800\n",
            "Training Epoch: 193 [32768/50000]\tLoss: 0.1418\tLR: 0.000800\n",
            "Training Epoch: 193 [32896/50000]\tLoss: 0.1681\tLR: 0.000800\n",
            "Training Epoch: 193 [33024/50000]\tLoss: 0.1339\tLR: 0.000800\n",
            "Training Epoch: 193 [33152/50000]\tLoss: 0.1044\tLR: 0.000800\n",
            "Training Epoch: 193 [33280/50000]\tLoss: 0.1457\tLR: 0.000800\n",
            "Training Epoch: 193 [33408/50000]\tLoss: 0.1706\tLR: 0.000800\n",
            "Training Epoch: 193 [33536/50000]\tLoss: 0.1604\tLR: 0.000800\n",
            "Training Epoch: 193 [33664/50000]\tLoss: 0.1230\tLR: 0.000800\n",
            "Training Epoch: 193 [33792/50000]\tLoss: 0.1453\tLR: 0.000800\n",
            "Training Epoch: 193 [33920/50000]\tLoss: 0.2394\tLR: 0.000800\n",
            "Training Epoch: 193 [34048/50000]\tLoss: 0.1709\tLR: 0.000800\n",
            "Training Epoch: 193 [34176/50000]\tLoss: 0.1299\tLR: 0.000800\n",
            "Training Epoch: 193 [34304/50000]\tLoss: 0.1469\tLR: 0.000800\n",
            "Training Epoch: 193 [34432/50000]\tLoss: 0.1797\tLR: 0.000800\n",
            "Training Epoch: 193 [34560/50000]\tLoss: 0.1624\tLR: 0.000800\n",
            "Training Epoch: 193 [34688/50000]\tLoss: 0.0964\tLR: 0.000800\n",
            "Training Epoch: 193 [34816/50000]\tLoss: 0.1755\tLR: 0.000800\n",
            "Training Epoch: 193 [34944/50000]\tLoss: 0.1309\tLR: 0.000800\n",
            "Training Epoch: 193 [35072/50000]\tLoss: 0.1210\tLR: 0.000800\n",
            "Training Epoch: 193 [35200/50000]\tLoss: 0.0827\tLR: 0.000800\n",
            "Training Epoch: 193 [35328/50000]\tLoss: 0.1343\tLR: 0.000800\n",
            "Training Epoch: 193 [35456/50000]\tLoss: 0.2215\tLR: 0.000800\n",
            "Training Epoch: 193 [35584/50000]\tLoss: 0.1312\tLR: 0.000800\n",
            "Training Epoch: 193 [35712/50000]\tLoss: 0.1116\tLR: 0.000800\n",
            "Training Epoch: 193 [35840/50000]\tLoss: 0.1580\tLR: 0.000800\n",
            "Training Epoch: 193 [35968/50000]\tLoss: 0.1788\tLR: 0.000800\n",
            "Training Epoch: 193 [36096/50000]\tLoss: 0.0999\tLR: 0.000800\n",
            "Training Epoch: 193 [36224/50000]\tLoss: 0.2696\tLR: 0.000800\n",
            "Training Epoch: 193 [36352/50000]\tLoss: 0.1857\tLR: 0.000800\n",
            "Training Epoch: 193 [36480/50000]\tLoss: 0.1830\tLR: 0.000800\n",
            "Training Epoch: 193 [36608/50000]\tLoss: 0.2030\tLR: 0.000800\n",
            "Training Epoch: 193 [36736/50000]\tLoss: 0.1479\tLR: 0.000800\n",
            "Training Epoch: 193 [36864/50000]\tLoss: 0.1577\tLR: 0.000800\n",
            "Training Epoch: 193 [36992/50000]\tLoss: 0.1675\tLR: 0.000800\n",
            "Training Epoch: 193 [37120/50000]\tLoss: 0.1674\tLR: 0.000800\n",
            "Training Epoch: 193 [37248/50000]\tLoss: 0.0904\tLR: 0.000800\n",
            "Training Epoch: 193 [37376/50000]\tLoss: 0.1349\tLR: 0.000800\n",
            "Training Epoch: 193 [37504/50000]\tLoss: 0.1178\tLR: 0.000800\n",
            "Training Epoch: 193 [37632/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 193 [37760/50000]\tLoss: 0.1942\tLR: 0.000800\n",
            "Training Epoch: 193 [37888/50000]\tLoss: 0.1686\tLR: 0.000800\n",
            "Training Epoch: 193 [38016/50000]\tLoss: 0.1327\tLR: 0.000800\n",
            "Training Epoch: 193 [38144/50000]\tLoss: 0.1410\tLR: 0.000800\n",
            "Training Epoch: 193 [38272/50000]\tLoss: 0.1178\tLR: 0.000800\n",
            "Training Epoch: 193 [38400/50000]\tLoss: 0.1151\tLR: 0.000800\n",
            "Training Epoch: 193 [38528/50000]\tLoss: 0.2011\tLR: 0.000800\n",
            "Training Epoch: 193 [38656/50000]\tLoss: 0.1210\tLR: 0.000800\n",
            "Training Epoch: 193 [38784/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 193 [38912/50000]\tLoss: 0.2188\tLR: 0.000800\n",
            "Training Epoch: 193 [39040/50000]\tLoss: 0.0802\tLR: 0.000800\n",
            "Training Epoch: 193 [39168/50000]\tLoss: 0.2232\tLR: 0.000800\n",
            "Training Epoch: 193 [39296/50000]\tLoss: 0.1329\tLR: 0.000800\n",
            "Training Epoch: 193 [39424/50000]\tLoss: 0.1462\tLR: 0.000800\n",
            "Training Epoch: 193 [39552/50000]\tLoss: 0.1480\tLR: 0.000800\n",
            "Training Epoch: 193 [39680/50000]\tLoss: 0.1598\tLR: 0.000800\n",
            "Training Epoch: 193 [39808/50000]\tLoss: 0.0916\tLR: 0.000800\n",
            "Training Epoch: 193 [39936/50000]\tLoss: 0.1533\tLR: 0.000800\n",
            "Training Epoch: 193 [40064/50000]\tLoss: 0.1578\tLR: 0.000800\n",
            "Training Epoch: 193 [40192/50000]\tLoss: 0.1381\tLR: 0.000800\n",
            "Training Epoch: 193 [40320/50000]\tLoss: 0.1043\tLR: 0.000800\n",
            "Training Epoch: 193 [40448/50000]\tLoss: 0.0926\tLR: 0.000800\n",
            "Training Epoch: 193 [40576/50000]\tLoss: 0.2447\tLR: 0.000800\n",
            "Training Epoch: 193 [40704/50000]\tLoss: 0.1493\tLR: 0.000800\n",
            "Training Epoch: 193 [40832/50000]\tLoss: 0.1619\tLR: 0.000800\n",
            "Training Epoch: 193 [40960/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 193 [41088/50000]\tLoss: 0.1612\tLR: 0.000800\n",
            "Training Epoch: 193 [41216/50000]\tLoss: 0.2309\tLR: 0.000800\n",
            "Training Epoch: 193 [41344/50000]\tLoss: 0.1901\tLR: 0.000800\n",
            "Training Epoch: 193 [41472/50000]\tLoss: 0.1444\tLR: 0.000800\n",
            "Training Epoch: 193 [41600/50000]\tLoss: 0.1499\tLR: 0.000800\n",
            "Training Epoch: 193 [41728/50000]\tLoss: 0.1267\tLR: 0.000800\n",
            "Training Epoch: 193 [41856/50000]\tLoss: 0.2152\tLR: 0.000800\n",
            "Training Epoch: 193 [41984/50000]\tLoss: 0.1557\tLR: 0.000800\n",
            "Training Epoch: 193 [42112/50000]\tLoss: 0.1102\tLR: 0.000800\n",
            "Training Epoch: 193 [42240/50000]\tLoss: 0.1096\tLR: 0.000800\n",
            "Training Epoch: 193 [42368/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 193 [42496/50000]\tLoss: 0.0978\tLR: 0.000800\n",
            "Training Epoch: 193 [42624/50000]\tLoss: 0.1499\tLR: 0.000800\n",
            "Training Epoch: 193 [42752/50000]\tLoss: 0.1459\tLR: 0.000800\n",
            "Training Epoch: 193 [42880/50000]\tLoss: 0.1811\tLR: 0.000800\n",
            "Training Epoch: 193 [43008/50000]\tLoss: 0.1690\tLR: 0.000800\n",
            "Training Epoch: 193 [43136/50000]\tLoss: 0.1712\tLR: 0.000800\n",
            "Training Epoch: 193 [43264/50000]\tLoss: 0.0950\tLR: 0.000800\n",
            "Training Epoch: 193 [43392/50000]\tLoss: 0.1699\tLR: 0.000800\n",
            "Training Epoch: 193 [43520/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 193 [43648/50000]\tLoss: 0.1176\tLR: 0.000800\n",
            "Training Epoch: 193 [43776/50000]\tLoss: 0.1316\tLR: 0.000800\n",
            "Training Epoch: 193 [43904/50000]\tLoss: 0.1260\tLR: 0.000800\n",
            "Training Epoch: 193 [44032/50000]\tLoss: 0.0960\tLR: 0.000800\n",
            "Training Epoch: 193 [44160/50000]\tLoss: 0.1042\tLR: 0.000800\n",
            "Training Epoch: 193 [44288/50000]\tLoss: 0.1101\tLR: 0.000800\n",
            "Training Epoch: 193 [44416/50000]\tLoss: 0.1051\tLR: 0.000800\n",
            "Training Epoch: 193 [44544/50000]\tLoss: 0.1472\tLR: 0.000800\n",
            "Training Epoch: 193 [44672/50000]\tLoss: 0.1729\tLR: 0.000800\n",
            "Training Epoch: 193 [44800/50000]\tLoss: 0.1006\tLR: 0.000800\n",
            "Training Epoch: 193 [44928/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 193 [45056/50000]\tLoss: 0.1055\tLR: 0.000800\n",
            "Training Epoch: 193 [45184/50000]\tLoss: 0.1888\tLR: 0.000800\n",
            "Training Epoch: 193 [45312/50000]\tLoss: 0.1268\tLR: 0.000800\n",
            "Training Epoch: 193 [45440/50000]\tLoss: 0.1276\tLR: 0.000800\n",
            "Training Epoch: 193 [45568/50000]\tLoss: 0.1855\tLR: 0.000800\n",
            "Training Epoch: 193 [45696/50000]\tLoss: 0.1461\tLR: 0.000800\n",
            "Training Epoch: 193 [45824/50000]\tLoss: 0.1660\tLR: 0.000800\n",
            "Training Epoch: 193 [45952/50000]\tLoss: 0.1403\tLR: 0.000800\n",
            "Training Epoch: 193 [46080/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 193 [46208/50000]\tLoss: 0.1568\tLR: 0.000800\n",
            "Training Epoch: 193 [46336/50000]\tLoss: 0.1664\tLR: 0.000800\n",
            "Training Epoch: 193 [46464/50000]\tLoss: 0.1153\tLR: 0.000800\n",
            "Training Epoch: 193 [46592/50000]\tLoss: 0.1240\tLR: 0.000800\n",
            "Training Epoch: 193 [46720/50000]\tLoss: 0.1604\tLR: 0.000800\n",
            "Training Epoch: 193 [46848/50000]\tLoss: 0.1273\tLR: 0.000800\n",
            "Training Epoch: 193 [46976/50000]\tLoss: 0.1828\tLR: 0.000800\n",
            "Training Epoch: 193 [47104/50000]\tLoss: 0.2272\tLR: 0.000800\n",
            "Training Epoch: 193 [47232/50000]\tLoss: 0.1031\tLR: 0.000800\n",
            "Training Epoch: 193 [47360/50000]\tLoss: 0.1561\tLR: 0.000800\n",
            "Training Epoch: 193 [47488/50000]\tLoss: 0.1787\tLR: 0.000800\n",
            "Training Epoch: 193 [47616/50000]\tLoss: 0.1581\tLR: 0.000800\n",
            "Training Epoch: 193 [47744/50000]\tLoss: 0.1458\tLR: 0.000800\n",
            "Training Epoch: 193 [47872/50000]\tLoss: 0.1432\tLR: 0.000800\n",
            "Training Epoch: 193 [48000/50000]\tLoss: 0.1351\tLR: 0.000800\n",
            "Training Epoch: 193 [48128/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 193 [48256/50000]\tLoss: 0.2190\tLR: 0.000800\n",
            "Training Epoch: 193 [48384/50000]\tLoss: 0.2339\tLR: 0.000800\n",
            "Training Epoch: 193 [48512/50000]\tLoss: 0.1292\tLR: 0.000800\n",
            "Training Epoch: 193 [48640/50000]\tLoss: 0.1616\tLR: 0.000800\n",
            "Training Epoch: 193 [48768/50000]\tLoss: 0.1494\tLR: 0.000800\n",
            "Training Epoch: 193 [48896/50000]\tLoss: 0.1575\tLR: 0.000800\n",
            "Training Epoch: 193 [49024/50000]\tLoss: 0.1060\tLR: 0.000800\n",
            "Training Epoch: 193 [49152/50000]\tLoss: 0.1447\tLR: 0.000800\n",
            "Training Epoch: 193 [49280/50000]\tLoss: 0.2130\tLR: 0.000800\n",
            "Training Epoch: 193 [49408/50000]\tLoss: 0.1166\tLR: 0.000800\n",
            "Training Epoch: 193 [49536/50000]\tLoss: 0.1543\tLR: 0.000800\n",
            "Training Epoch: 193 [49664/50000]\tLoss: 0.1703\tLR: 0.000800\n",
            "Training Epoch: 193 [49792/50000]\tLoss: 0.1547\tLR: 0.000800\n",
            "Training Epoch: 193 [49920/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 193 [50000/50000]\tLoss: 0.1340\tLR: 0.000800\n",
            "epoch 193 training time consumed: 28.02s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  86854 GiB |  86854 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  85200 GiB |  85200 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1653 GiB |   1653 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  86854 GiB |  86854 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  85200 GiB |  85200 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1653 GiB |   1653 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  86596 GiB |  86596 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  84944 GiB |  84944 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1651 GiB |   1651 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  53137 GiB |  53137 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  51430 GiB |  51430 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1707 GiB |   1707 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   33654 K  |   33654 K  |\n",
            "|       from large pool |       8    |      61    |   11290 K  |   11290 K  |\n",
            "|       from small pool |     373    |     464    |   22364 K  |   22363 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   33654 K  |   33654 K  |\n",
            "|       from large pool |       8    |      61    |   11290 K  |   11290 K  |\n",
            "|       from small pool |     373    |     464    |   22364 K  |   22363 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   11064 K  |   11064 K  |\n",
            "|       from large pool |       5    |      14    |    5382 K  |    5382 K  |\n",
            "|       from small pool |      10    |      16    |    5681 K  |    5681 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 193, Average loss: 0.0113, Accuracy: 0.6653, Time consumed:2.78s\n",
            "\n",
            "Training Epoch: 194 [128/50000]\tLoss: 0.1047\tLR: 0.000800\n",
            "Training Epoch: 194 [256/50000]\tLoss: 0.1586\tLR: 0.000800\n",
            "Training Epoch: 194 [384/50000]\tLoss: 0.1230\tLR: 0.000800\n",
            "Training Epoch: 194 [512/50000]\tLoss: 0.2215\tLR: 0.000800\n",
            "Training Epoch: 194 [640/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 194 [768/50000]\tLoss: 0.1338\tLR: 0.000800\n",
            "Training Epoch: 194 [896/50000]\tLoss: 0.1561\tLR: 0.000800\n",
            "Training Epoch: 194 [1024/50000]\tLoss: 0.1143\tLR: 0.000800\n",
            "Training Epoch: 194 [1152/50000]\tLoss: 0.1764\tLR: 0.000800\n",
            "Training Epoch: 194 [1280/50000]\tLoss: 0.1110\tLR: 0.000800\n",
            "Training Epoch: 194 [1408/50000]\tLoss: 0.1138\tLR: 0.000800\n",
            "Training Epoch: 194 [1536/50000]\tLoss: 0.1281\tLR: 0.000800\n",
            "Training Epoch: 194 [1664/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 194 [1792/50000]\tLoss: 0.1962\tLR: 0.000800\n",
            "Training Epoch: 194 [1920/50000]\tLoss: 0.2124\tLR: 0.000800\n",
            "Training Epoch: 194 [2048/50000]\tLoss: 0.1286\tLR: 0.000800\n",
            "Training Epoch: 194 [2176/50000]\tLoss: 0.1975\tLR: 0.000800\n",
            "Training Epoch: 194 [2304/50000]\tLoss: 0.1022\tLR: 0.000800\n",
            "Training Epoch: 194 [2432/50000]\tLoss: 0.1804\tLR: 0.000800\n",
            "Training Epoch: 194 [2560/50000]\tLoss: 0.2090\tLR: 0.000800\n",
            "Training Epoch: 194 [2688/50000]\tLoss: 0.1756\tLR: 0.000800\n",
            "Training Epoch: 194 [2816/50000]\tLoss: 0.1668\tLR: 0.000800\n",
            "Training Epoch: 194 [2944/50000]\tLoss: 0.1108\tLR: 0.000800\n",
            "Training Epoch: 194 [3072/50000]\tLoss: 0.1403\tLR: 0.000800\n",
            "Training Epoch: 194 [3200/50000]\tLoss: 0.1009\tLR: 0.000800\n",
            "Training Epoch: 194 [3328/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 194 [3456/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 194 [3584/50000]\tLoss: 0.2230\tLR: 0.000800\n",
            "Training Epoch: 194 [3712/50000]\tLoss: 0.1752\tLR: 0.000800\n",
            "Training Epoch: 194 [3840/50000]\tLoss: 0.1207\tLR: 0.000800\n",
            "Training Epoch: 194 [3968/50000]\tLoss: 0.1264\tLR: 0.000800\n",
            "Training Epoch: 194 [4096/50000]\tLoss: 0.1682\tLR: 0.000800\n",
            "Training Epoch: 194 [4224/50000]\tLoss: 0.0903\tLR: 0.000800\n",
            "Training Epoch: 194 [4352/50000]\tLoss: 0.1576\tLR: 0.000800\n",
            "Training Epoch: 194 [4480/50000]\tLoss: 0.2025\tLR: 0.000800\n",
            "Training Epoch: 194 [4608/50000]\tLoss: 0.0905\tLR: 0.000800\n",
            "Training Epoch: 194 [4736/50000]\tLoss: 0.1409\tLR: 0.000800\n",
            "Training Epoch: 194 [4864/50000]\tLoss: 0.1442\tLR: 0.000800\n",
            "Training Epoch: 194 [4992/50000]\tLoss: 0.2009\tLR: 0.000800\n",
            "Training Epoch: 194 [5120/50000]\tLoss: 0.1289\tLR: 0.000800\n",
            "Training Epoch: 194 [5248/50000]\tLoss: 0.1446\tLR: 0.000800\n",
            "Training Epoch: 194 [5376/50000]\tLoss: 0.1300\tLR: 0.000800\n",
            "Training Epoch: 194 [5504/50000]\tLoss: 0.1537\tLR: 0.000800\n",
            "Training Epoch: 194 [5632/50000]\tLoss: 0.1499\tLR: 0.000800\n",
            "Training Epoch: 194 [5760/50000]\tLoss: 0.1270\tLR: 0.000800\n",
            "Training Epoch: 194 [5888/50000]\tLoss: 0.1771\tLR: 0.000800\n",
            "Training Epoch: 194 [6016/50000]\tLoss: 0.1489\tLR: 0.000800\n",
            "Training Epoch: 194 [6144/50000]\tLoss: 0.0862\tLR: 0.000800\n",
            "Training Epoch: 194 [6272/50000]\tLoss: 0.1095\tLR: 0.000800\n",
            "Training Epoch: 194 [6400/50000]\tLoss: 0.1651\tLR: 0.000800\n",
            "Training Epoch: 194 [6528/50000]\tLoss: 0.1318\tLR: 0.000800\n",
            "Training Epoch: 194 [6656/50000]\tLoss: 0.0912\tLR: 0.000800\n",
            "Training Epoch: 194 [6784/50000]\tLoss: 0.2050\tLR: 0.000800\n",
            "Training Epoch: 194 [6912/50000]\tLoss: 0.0848\tLR: 0.000800\n",
            "Training Epoch: 194 [7040/50000]\tLoss: 0.1013\tLR: 0.000800\n",
            "Training Epoch: 194 [7168/50000]\tLoss: 0.1656\tLR: 0.000800\n",
            "Training Epoch: 194 [7296/50000]\tLoss: 0.1628\tLR: 0.000800\n",
            "Training Epoch: 194 [7424/50000]\tLoss: 0.1510\tLR: 0.000800\n",
            "Training Epoch: 194 [7552/50000]\tLoss: 0.2064\tLR: 0.000800\n",
            "Training Epoch: 194 [7680/50000]\tLoss: 0.0925\tLR: 0.000800\n",
            "Training Epoch: 194 [7808/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 194 [7936/50000]\tLoss: 0.1354\tLR: 0.000800\n",
            "Training Epoch: 194 [8064/50000]\tLoss: 0.1615\tLR: 0.000800\n",
            "Training Epoch: 194 [8192/50000]\tLoss: 0.1343\tLR: 0.000800\n",
            "Training Epoch: 194 [8320/50000]\tLoss: 0.1225\tLR: 0.000800\n",
            "Training Epoch: 194 [8448/50000]\tLoss: 0.1802\tLR: 0.000800\n",
            "Training Epoch: 194 [8576/50000]\tLoss: 0.1858\tLR: 0.000800\n",
            "Training Epoch: 194 [8704/50000]\tLoss: 0.1566\tLR: 0.000800\n",
            "Training Epoch: 194 [8832/50000]\tLoss: 0.1320\tLR: 0.000800\n",
            "Training Epoch: 194 [8960/50000]\tLoss: 0.1456\tLR: 0.000800\n",
            "Training Epoch: 194 [9088/50000]\tLoss: 0.1769\tLR: 0.000800\n",
            "Training Epoch: 194 [9216/50000]\tLoss: 0.1615\tLR: 0.000800\n",
            "Training Epoch: 194 [9344/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 194 [9472/50000]\tLoss: 0.1474\tLR: 0.000800\n",
            "Training Epoch: 194 [9600/50000]\tLoss: 0.1477\tLR: 0.000800\n",
            "Training Epoch: 194 [9728/50000]\tLoss: 0.1389\tLR: 0.000800\n",
            "Training Epoch: 194 [9856/50000]\tLoss: 0.1562\tLR: 0.000800\n",
            "Training Epoch: 194 [9984/50000]\tLoss: 0.1174\tLR: 0.000800\n",
            "Training Epoch: 194 [10112/50000]\tLoss: 0.1489\tLR: 0.000800\n",
            "Training Epoch: 194 [10240/50000]\tLoss: 0.1699\tLR: 0.000800\n",
            "Training Epoch: 194 [10368/50000]\tLoss: 0.1817\tLR: 0.000800\n",
            "Training Epoch: 194 [10496/50000]\tLoss: 0.0957\tLR: 0.000800\n",
            "Training Epoch: 194 [10624/50000]\tLoss: 0.0775\tLR: 0.000800\n",
            "Training Epoch: 194 [10752/50000]\tLoss: 0.2126\tLR: 0.000800\n",
            "Training Epoch: 194 [10880/50000]\tLoss: 0.1110\tLR: 0.000800\n",
            "Training Epoch: 194 [11008/50000]\tLoss: 0.1285\tLR: 0.000800\n",
            "Training Epoch: 194 [11136/50000]\tLoss: 0.1503\tLR: 0.000800\n",
            "Training Epoch: 194 [11264/50000]\tLoss: 0.0776\tLR: 0.000800\n",
            "Training Epoch: 194 [11392/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 194 [11520/50000]\tLoss: 0.1974\tLR: 0.000800\n",
            "Training Epoch: 194 [11648/50000]\tLoss: 0.1434\tLR: 0.000800\n",
            "Training Epoch: 194 [11776/50000]\tLoss: 0.1599\tLR: 0.000800\n",
            "Training Epoch: 194 [11904/50000]\tLoss: 0.1455\tLR: 0.000800\n",
            "Training Epoch: 194 [12032/50000]\tLoss: 0.1371\tLR: 0.000800\n",
            "Training Epoch: 194 [12160/50000]\tLoss: 0.1320\tLR: 0.000800\n",
            "Training Epoch: 194 [12288/50000]\tLoss: 0.1225\tLR: 0.000800\n",
            "Training Epoch: 194 [12416/50000]\tLoss: 0.1718\tLR: 0.000800\n",
            "Training Epoch: 194 [12544/50000]\tLoss: 0.1702\tLR: 0.000800\n",
            "Training Epoch: 194 [12672/50000]\tLoss: 0.1286\tLR: 0.000800\n",
            "Training Epoch: 194 [12800/50000]\tLoss: 0.1415\tLR: 0.000800\n",
            "Training Epoch: 194 [12928/50000]\tLoss: 0.1728\tLR: 0.000800\n",
            "Training Epoch: 194 [13056/50000]\tLoss: 0.1105\tLR: 0.000800\n",
            "Training Epoch: 194 [13184/50000]\tLoss: 0.0765\tLR: 0.000800\n",
            "Training Epoch: 194 [13312/50000]\tLoss: 0.1550\tLR: 0.000800\n",
            "Training Epoch: 194 [13440/50000]\tLoss: 0.0843\tLR: 0.000800\n",
            "Training Epoch: 194 [13568/50000]\tLoss: 0.1467\tLR: 0.000800\n",
            "Training Epoch: 194 [13696/50000]\tLoss: 0.1938\tLR: 0.000800\n",
            "Training Epoch: 194 [13824/50000]\tLoss: 0.0968\tLR: 0.000800\n",
            "Training Epoch: 194 [13952/50000]\tLoss: 0.1457\tLR: 0.000800\n",
            "Training Epoch: 194 [14080/50000]\tLoss: 0.1331\tLR: 0.000800\n",
            "Training Epoch: 194 [14208/50000]\tLoss: 0.1000\tLR: 0.000800\n",
            "Training Epoch: 194 [14336/50000]\tLoss: 0.1101\tLR: 0.000800\n",
            "Training Epoch: 194 [14464/50000]\tLoss: 0.1655\tLR: 0.000800\n",
            "Training Epoch: 194 [14592/50000]\tLoss: 0.1455\tLR: 0.000800\n",
            "Training Epoch: 194 [14720/50000]\tLoss: 0.2004\tLR: 0.000800\n",
            "Training Epoch: 194 [14848/50000]\tLoss: 0.2044\tLR: 0.000800\n",
            "Training Epoch: 194 [14976/50000]\tLoss: 0.2017\tLR: 0.000800\n",
            "Training Epoch: 194 [15104/50000]\tLoss: 0.0738\tLR: 0.000800\n",
            "Training Epoch: 194 [15232/50000]\tLoss: 0.1761\tLR: 0.000800\n",
            "Training Epoch: 194 [15360/50000]\tLoss: 0.1372\tLR: 0.000800\n",
            "Training Epoch: 194 [15488/50000]\tLoss: 0.1394\tLR: 0.000800\n",
            "Training Epoch: 194 [15616/50000]\tLoss: 0.1512\tLR: 0.000800\n",
            "Training Epoch: 194 [15744/50000]\tLoss: 0.1788\tLR: 0.000800\n",
            "Training Epoch: 194 [15872/50000]\tLoss: 0.1183\tLR: 0.000800\n",
            "Training Epoch: 194 [16000/50000]\tLoss: 0.1267\tLR: 0.000800\n",
            "Training Epoch: 194 [16128/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 194 [16256/50000]\tLoss: 0.1545\tLR: 0.000800\n",
            "Training Epoch: 194 [16384/50000]\tLoss: 0.0813\tLR: 0.000800\n",
            "Training Epoch: 194 [16512/50000]\tLoss: 0.1541\tLR: 0.000800\n",
            "Training Epoch: 194 [16640/50000]\tLoss: 0.1545\tLR: 0.000800\n",
            "Training Epoch: 194 [16768/50000]\tLoss: 0.1369\tLR: 0.000800\n",
            "Training Epoch: 194 [16896/50000]\tLoss: 0.1382\tLR: 0.000800\n",
            "Training Epoch: 194 [17024/50000]\tLoss: 0.1343\tLR: 0.000800\n",
            "Training Epoch: 194 [17152/50000]\tLoss: 0.1204\tLR: 0.000800\n",
            "Training Epoch: 194 [17280/50000]\tLoss: 0.1510\tLR: 0.000800\n",
            "Training Epoch: 194 [17408/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 194 [17536/50000]\tLoss: 0.1599\tLR: 0.000800\n",
            "Training Epoch: 194 [17664/50000]\tLoss: 0.1709\tLR: 0.000800\n",
            "Training Epoch: 194 [17792/50000]\tLoss: 0.1248\tLR: 0.000800\n",
            "Training Epoch: 194 [17920/50000]\tLoss: 0.2297\tLR: 0.000800\n",
            "Training Epoch: 194 [18048/50000]\tLoss: 0.1060\tLR: 0.000800\n",
            "Training Epoch: 194 [18176/50000]\tLoss: 0.1487\tLR: 0.000800\n",
            "Training Epoch: 194 [18304/50000]\tLoss: 0.2069\tLR: 0.000800\n",
            "Training Epoch: 194 [18432/50000]\tLoss: 0.1564\tLR: 0.000800\n",
            "Training Epoch: 194 [18560/50000]\tLoss: 0.1457\tLR: 0.000800\n",
            "Training Epoch: 194 [18688/50000]\tLoss: 0.1493\tLR: 0.000800\n",
            "Training Epoch: 194 [18816/50000]\tLoss: 0.0990\tLR: 0.000800\n",
            "Training Epoch: 194 [18944/50000]\tLoss: 0.1808\tLR: 0.000800\n",
            "Training Epoch: 194 [19072/50000]\tLoss: 0.1170\tLR: 0.000800\n",
            "Training Epoch: 194 [19200/50000]\tLoss: 0.2392\tLR: 0.000800\n",
            "Training Epoch: 194 [19328/50000]\tLoss: 0.1829\tLR: 0.000800\n",
            "Training Epoch: 194 [19456/50000]\tLoss: 0.1410\tLR: 0.000800\n",
            "Training Epoch: 194 [19584/50000]\tLoss: 0.1239\tLR: 0.000800\n",
            "Training Epoch: 194 [19712/50000]\tLoss: 0.1051\tLR: 0.000800\n",
            "Training Epoch: 194 [19840/50000]\tLoss: 0.1343\tLR: 0.000800\n",
            "Training Epoch: 194 [19968/50000]\tLoss: 0.1414\tLR: 0.000800\n",
            "Training Epoch: 194 [20096/50000]\tLoss: 0.1328\tLR: 0.000800\n",
            "Training Epoch: 194 [20224/50000]\tLoss: 0.1297\tLR: 0.000800\n",
            "Training Epoch: 194 [20352/50000]\tLoss: 0.2055\tLR: 0.000800\n",
            "Training Epoch: 194 [20480/50000]\tLoss: 0.1414\tLR: 0.000800\n",
            "Training Epoch: 194 [20608/50000]\tLoss: 0.1267\tLR: 0.000800\n",
            "Training Epoch: 194 [20736/50000]\tLoss: 0.1044\tLR: 0.000800\n",
            "Training Epoch: 194 [20864/50000]\tLoss: 0.1520\tLR: 0.000800\n",
            "Training Epoch: 194 [20992/50000]\tLoss: 0.1278\tLR: 0.000800\n",
            "Training Epoch: 194 [21120/50000]\tLoss: 0.1411\tLR: 0.000800\n",
            "Training Epoch: 194 [21248/50000]\tLoss: 0.1639\tLR: 0.000800\n",
            "Training Epoch: 194 [21376/50000]\tLoss: 0.1685\tLR: 0.000800\n",
            "Training Epoch: 194 [21504/50000]\tLoss: 0.1894\tLR: 0.000800\n",
            "Training Epoch: 194 [21632/50000]\tLoss: 0.1414\tLR: 0.000800\n",
            "Training Epoch: 194 [21760/50000]\tLoss: 0.1770\tLR: 0.000800\n",
            "Training Epoch: 194 [21888/50000]\tLoss: 0.0940\tLR: 0.000800\n",
            "Training Epoch: 194 [22016/50000]\tLoss: 0.1569\tLR: 0.000800\n",
            "Training Epoch: 194 [22144/50000]\tLoss: 0.1365\tLR: 0.000800\n",
            "Training Epoch: 194 [22272/50000]\tLoss: 0.1065\tLR: 0.000800\n",
            "Training Epoch: 194 [22400/50000]\tLoss: 0.1951\tLR: 0.000800\n",
            "Training Epoch: 194 [22528/50000]\tLoss: 0.1745\tLR: 0.000800\n",
            "Training Epoch: 194 [22656/50000]\tLoss: 0.1177\tLR: 0.000800\n",
            "Training Epoch: 194 [22784/50000]\tLoss: 0.2047\tLR: 0.000800\n",
            "Training Epoch: 194 [22912/50000]\tLoss: 0.1024\tLR: 0.000800\n",
            "Training Epoch: 194 [23040/50000]\tLoss: 0.1037\tLR: 0.000800\n",
            "Training Epoch: 194 [23168/50000]\tLoss: 0.1648\tLR: 0.000800\n",
            "Training Epoch: 194 [23296/50000]\tLoss: 0.0884\tLR: 0.000800\n",
            "Training Epoch: 194 [23424/50000]\tLoss: 0.0840\tLR: 0.000800\n",
            "Training Epoch: 194 [23552/50000]\tLoss: 0.1237\tLR: 0.000800\n",
            "Training Epoch: 194 [23680/50000]\tLoss: 0.1350\tLR: 0.000800\n",
            "Training Epoch: 194 [23808/50000]\tLoss: 0.1423\tLR: 0.000800\n",
            "Training Epoch: 194 [23936/50000]\tLoss: 0.1682\tLR: 0.000800\n",
            "Training Epoch: 194 [24064/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 194 [24192/50000]\tLoss: 0.1264\tLR: 0.000800\n",
            "Training Epoch: 194 [24320/50000]\tLoss: 0.1188\tLR: 0.000800\n",
            "Training Epoch: 194 [24448/50000]\tLoss: 0.1545\tLR: 0.000800\n",
            "Training Epoch: 194 [24576/50000]\tLoss: 0.1436\tLR: 0.000800\n",
            "Training Epoch: 194 [24704/50000]\tLoss: 0.1630\tLR: 0.000800\n",
            "Training Epoch: 194 [24832/50000]\tLoss: 0.1727\tLR: 0.000800\n",
            "Training Epoch: 194 [24960/50000]\tLoss: 0.1653\tLR: 0.000800\n",
            "Training Epoch: 194 [25088/50000]\tLoss: 0.1426\tLR: 0.000800\n",
            "Training Epoch: 194 [25216/50000]\tLoss: 0.1706\tLR: 0.000800\n",
            "Training Epoch: 194 [25344/50000]\tLoss: 0.1592\tLR: 0.000800\n",
            "Training Epoch: 194 [25472/50000]\tLoss: 0.1206\tLR: 0.000800\n",
            "Training Epoch: 194 [25600/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 194 [25728/50000]\tLoss: 0.1309\tLR: 0.000800\n",
            "Training Epoch: 194 [25856/50000]\tLoss: 0.1219\tLR: 0.000800\n",
            "Training Epoch: 194 [25984/50000]\tLoss: 0.1399\tLR: 0.000800\n",
            "Training Epoch: 194 [26112/50000]\tLoss: 0.1239\tLR: 0.000800\n",
            "Training Epoch: 194 [26240/50000]\tLoss: 0.1042\tLR: 0.000800\n",
            "Training Epoch: 194 [26368/50000]\tLoss: 0.1131\tLR: 0.000800\n",
            "Training Epoch: 194 [26496/50000]\tLoss: 0.1343\tLR: 0.000800\n",
            "Training Epoch: 194 [26624/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 194 [26752/50000]\tLoss: 0.1004\tLR: 0.000800\n",
            "Training Epoch: 194 [26880/50000]\tLoss: 0.1622\tLR: 0.000800\n",
            "Training Epoch: 194 [27008/50000]\tLoss: 0.1272\tLR: 0.000800\n",
            "Training Epoch: 194 [27136/50000]\tLoss: 0.2461\tLR: 0.000800\n",
            "Training Epoch: 194 [27264/50000]\tLoss: 0.1573\tLR: 0.000800\n",
            "Training Epoch: 194 [27392/50000]\tLoss: 0.1452\tLR: 0.000800\n",
            "Training Epoch: 194 [27520/50000]\tLoss: 0.1424\tLR: 0.000800\n",
            "Training Epoch: 194 [27648/50000]\tLoss: 0.1107\tLR: 0.000800\n",
            "Training Epoch: 194 [27776/50000]\tLoss: 0.1523\tLR: 0.000800\n",
            "Training Epoch: 194 [27904/50000]\tLoss: 0.1618\tLR: 0.000800\n",
            "Training Epoch: 194 [28032/50000]\tLoss: 0.1322\tLR: 0.000800\n",
            "Training Epoch: 194 [28160/50000]\tLoss: 0.1593\tLR: 0.000800\n",
            "Training Epoch: 194 [28288/50000]\tLoss: 0.0894\tLR: 0.000800\n",
            "Training Epoch: 194 [28416/50000]\tLoss: 0.1457\tLR: 0.000800\n",
            "Training Epoch: 194 [28544/50000]\tLoss: 0.1314\tLR: 0.000800\n",
            "Training Epoch: 194 [28672/50000]\tLoss: 0.1087\tLR: 0.000800\n",
            "Training Epoch: 194 [28800/50000]\tLoss: 0.1680\tLR: 0.000800\n",
            "Training Epoch: 194 [28928/50000]\tLoss: 0.1206\tLR: 0.000800\n",
            "Training Epoch: 194 [29056/50000]\tLoss: 0.1682\tLR: 0.000800\n",
            "Training Epoch: 194 [29184/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 194 [29312/50000]\tLoss: 0.1295\tLR: 0.000800\n",
            "Training Epoch: 194 [29440/50000]\tLoss: 0.1259\tLR: 0.000800\n",
            "Training Epoch: 194 [29568/50000]\tLoss: 0.1887\tLR: 0.000800\n",
            "Training Epoch: 194 [29696/50000]\tLoss: 0.1530\tLR: 0.000800\n",
            "Training Epoch: 194 [29824/50000]\tLoss: 0.1634\tLR: 0.000800\n",
            "Training Epoch: 194 [29952/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 194 [30080/50000]\tLoss: 0.1844\tLR: 0.000800\n",
            "Training Epoch: 194 [30208/50000]\tLoss: 0.1650\tLR: 0.000800\n",
            "Training Epoch: 194 [30336/50000]\tLoss: 0.2172\tLR: 0.000800\n",
            "Training Epoch: 194 [30464/50000]\tLoss: 0.1171\tLR: 0.000800\n",
            "Training Epoch: 194 [30592/50000]\tLoss: 0.1570\tLR: 0.000800\n",
            "Training Epoch: 194 [30720/50000]\tLoss: 0.1652\tLR: 0.000800\n",
            "Training Epoch: 194 [30848/50000]\tLoss: 0.1398\tLR: 0.000800\n",
            "Training Epoch: 194 [30976/50000]\tLoss: 0.0830\tLR: 0.000800\n",
            "Training Epoch: 194 [31104/50000]\tLoss: 0.1590\tLR: 0.000800\n",
            "Training Epoch: 194 [31232/50000]\tLoss: 0.1060\tLR: 0.000800\n",
            "Training Epoch: 194 [31360/50000]\tLoss: 0.1318\tLR: 0.000800\n",
            "Training Epoch: 194 [31488/50000]\tLoss: 0.1489\tLR: 0.000800\n",
            "Training Epoch: 194 [31616/50000]\tLoss: 0.2361\tLR: 0.000800\n",
            "Training Epoch: 194 [31744/50000]\tLoss: 0.2858\tLR: 0.000800\n",
            "Training Epoch: 194 [31872/50000]\tLoss: 0.1439\tLR: 0.000800\n",
            "Training Epoch: 194 [32000/50000]\tLoss: 0.1417\tLR: 0.000800\n",
            "Training Epoch: 194 [32128/50000]\tLoss: 0.1372\tLR: 0.000800\n",
            "Training Epoch: 194 [32256/50000]\tLoss: 0.0940\tLR: 0.000800\n",
            "Training Epoch: 194 [32384/50000]\tLoss: 0.1138\tLR: 0.000800\n",
            "Training Epoch: 194 [32512/50000]\tLoss: 0.1290\tLR: 0.000800\n",
            "Training Epoch: 194 [32640/50000]\tLoss: 0.1189\tLR: 0.000800\n",
            "Training Epoch: 194 [32768/50000]\tLoss: 0.1241\tLR: 0.000800\n",
            "Training Epoch: 194 [32896/50000]\tLoss: 0.1039\tLR: 0.000800\n",
            "Training Epoch: 194 [33024/50000]\tLoss: 0.1027\tLR: 0.000800\n",
            "Training Epoch: 194 [33152/50000]\tLoss: 0.1051\tLR: 0.000800\n",
            "Training Epoch: 194 [33280/50000]\tLoss: 0.0843\tLR: 0.000800\n",
            "Training Epoch: 194 [33408/50000]\tLoss: 0.1060\tLR: 0.000800\n",
            "Training Epoch: 194 [33536/50000]\tLoss: 0.2166\tLR: 0.000800\n",
            "Training Epoch: 194 [33664/50000]\tLoss: 0.1314\tLR: 0.000800\n",
            "Training Epoch: 194 [33792/50000]\tLoss: 0.1245\tLR: 0.000800\n",
            "Training Epoch: 194 [33920/50000]\tLoss: 0.1544\tLR: 0.000800\n",
            "Training Epoch: 194 [34048/50000]\tLoss: 0.1660\tLR: 0.000800\n",
            "Training Epoch: 194 [34176/50000]\tLoss: 0.1311\tLR: 0.000800\n",
            "Training Epoch: 194 [34304/50000]\tLoss: 0.1451\tLR: 0.000800\n",
            "Training Epoch: 194 [34432/50000]\tLoss: 0.1292\tLR: 0.000800\n",
            "Training Epoch: 194 [34560/50000]\tLoss: 0.1469\tLR: 0.000800\n",
            "Training Epoch: 194 [34688/50000]\tLoss: 0.1925\tLR: 0.000800\n",
            "Training Epoch: 194 [34816/50000]\tLoss: 0.1039\tLR: 0.000800\n",
            "Training Epoch: 194 [34944/50000]\tLoss: 0.1349\tLR: 0.000800\n",
            "Training Epoch: 194 [35072/50000]\tLoss: 0.1063\tLR: 0.000800\n",
            "Training Epoch: 194 [35200/50000]\tLoss: 0.1138\tLR: 0.000800\n",
            "Training Epoch: 194 [35328/50000]\tLoss: 0.1417\tLR: 0.000800\n",
            "Training Epoch: 194 [35456/50000]\tLoss: 0.1135\tLR: 0.000800\n",
            "Training Epoch: 194 [35584/50000]\tLoss: 0.1164\tLR: 0.000800\n",
            "Training Epoch: 194 [35712/50000]\tLoss: 0.1202\tLR: 0.000800\n",
            "Training Epoch: 194 [35840/50000]\tLoss: 0.1392\tLR: 0.000800\n",
            "Training Epoch: 194 [35968/50000]\tLoss: 0.1747\tLR: 0.000800\n",
            "Training Epoch: 194 [36096/50000]\tLoss: 0.1911\tLR: 0.000800\n",
            "Training Epoch: 194 [36224/50000]\tLoss: 0.1054\tLR: 0.000800\n",
            "Training Epoch: 194 [36352/50000]\tLoss: 0.1246\tLR: 0.000800\n",
            "Training Epoch: 194 [36480/50000]\tLoss: 0.1438\tLR: 0.000800\n",
            "Training Epoch: 194 [36608/50000]\tLoss: 0.1728\tLR: 0.000800\n",
            "Training Epoch: 194 [36736/50000]\tLoss: 0.1475\tLR: 0.000800\n",
            "Training Epoch: 194 [36864/50000]\tLoss: 0.1409\tLR: 0.000800\n",
            "Training Epoch: 194 [36992/50000]\tLoss: 0.1632\tLR: 0.000800\n",
            "Training Epoch: 194 [37120/50000]\tLoss: 0.1304\tLR: 0.000800\n",
            "Training Epoch: 194 [37248/50000]\tLoss: 0.1351\tLR: 0.000800\n",
            "Training Epoch: 194 [37376/50000]\tLoss: 0.0874\tLR: 0.000800\n",
            "Training Epoch: 194 [37504/50000]\tLoss: 0.1226\tLR: 0.000800\n",
            "Training Epoch: 194 [37632/50000]\tLoss: 0.1112\tLR: 0.000800\n",
            "Training Epoch: 194 [37760/50000]\tLoss: 0.2039\tLR: 0.000800\n",
            "Training Epoch: 194 [37888/50000]\tLoss: 0.1587\tLR: 0.000800\n",
            "Training Epoch: 194 [38016/50000]\tLoss: 0.1561\tLR: 0.000800\n",
            "Training Epoch: 194 [38144/50000]\tLoss: 0.1674\tLR: 0.000800\n",
            "Training Epoch: 194 [38272/50000]\tLoss: 0.1419\tLR: 0.000800\n",
            "Training Epoch: 194 [38400/50000]\tLoss: 0.1669\tLR: 0.000800\n",
            "Training Epoch: 194 [38528/50000]\tLoss: 0.1402\tLR: 0.000800\n",
            "Training Epoch: 194 [38656/50000]\tLoss: 0.1149\tLR: 0.000800\n",
            "Training Epoch: 194 [38784/50000]\tLoss: 0.1173\tLR: 0.000800\n",
            "Training Epoch: 194 [38912/50000]\tLoss: 0.1467\tLR: 0.000800\n",
            "Training Epoch: 194 [39040/50000]\tLoss: 0.1605\tLR: 0.000800\n",
            "Training Epoch: 194 [39168/50000]\tLoss: 0.1343\tLR: 0.000800\n",
            "Training Epoch: 194 [39296/50000]\tLoss: 0.1226\tLR: 0.000800\n",
            "Training Epoch: 194 [39424/50000]\tLoss: 0.1038\tLR: 0.000800\n",
            "Training Epoch: 194 [39552/50000]\tLoss: 0.1709\tLR: 0.000800\n",
            "Training Epoch: 194 [39680/50000]\tLoss: 0.1775\tLR: 0.000800\n",
            "Training Epoch: 194 [39808/50000]\tLoss: 0.1797\tLR: 0.000800\n",
            "Training Epoch: 194 [39936/50000]\tLoss: 0.1727\tLR: 0.000800\n",
            "Training Epoch: 194 [40064/50000]\tLoss: 0.1095\tLR: 0.000800\n",
            "Training Epoch: 194 [40192/50000]\tLoss: 0.1582\tLR: 0.000800\n",
            "Training Epoch: 194 [40320/50000]\tLoss: 0.1563\tLR: 0.000800\n",
            "Training Epoch: 194 [40448/50000]\tLoss: 0.1220\tLR: 0.000800\n",
            "Training Epoch: 194 [40576/50000]\tLoss: 0.2003\tLR: 0.000800\n",
            "Training Epoch: 194 [40704/50000]\tLoss: 0.1393\tLR: 0.000800\n",
            "Training Epoch: 194 [40832/50000]\tLoss: 0.1102\tLR: 0.000800\n",
            "Training Epoch: 194 [40960/50000]\tLoss: 0.1588\tLR: 0.000800\n",
            "Training Epoch: 194 [41088/50000]\tLoss: 0.1488\tLR: 0.000800\n",
            "Training Epoch: 194 [41216/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 194 [41344/50000]\tLoss: 0.1364\tLR: 0.000800\n",
            "Training Epoch: 194 [41472/50000]\tLoss: 0.1162\tLR: 0.000800\n",
            "Training Epoch: 194 [41600/50000]\tLoss: 0.1432\tLR: 0.000800\n",
            "Training Epoch: 194 [41728/50000]\tLoss: 0.1176\tLR: 0.000800\n",
            "Training Epoch: 194 [41856/50000]\tLoss: 0.1469\tLR: 0.000800\n",
            "Training Epoch: 194 [41984/50000]\tLoss: 0.1741\tLR: 0.000800\n",
            "Training Epoch: 194 [42112/50000]\tLoss: 0.1853\tLR: 0.000800\n",
            "Training Epoch: 194 [42240/50000]\tLoss: 0.1126\tLR: 0.000800\n",
            "Training Epoch: 194 [42368/50000]\tLoss: 0.1081\tLR: 0.000800\n",
            "Training Epoch: 194 [42496/50000]\tLoss: 0.1191\tLR: 0.000800\n",
            "Training Epoch: 194 [42624/50000]\tLoss: 0.1519\tLR: 0.000800\n",
            "Training Epoch: 194 [42752/50000]\tLoss: 0.1448\tLR: 0.000800\n",
            "Training Epoch: 194 [42880/50000]\tLoss: 0.1655\tLR: 0.000800\n",
            "Training Epoch: 194 [43008/50000]\tLoss: 0.1690\tLR: 0.000800\n",
            "Training Epoch: 194 [43136/50000]\tLoss: 0.1335\tLR: 0.000800\n",
            "Training Epoch: 194 [43264/50000]\tLoss: 0.2089\tLR: 0.000800\n",
            "Training Epoch: 194 [43392/50000]\tLoss: 0.1716\tLR: 0.000800\n",
            "Training Epoch: 194 [43520/50000]\tLoss: 0.0873\tLR: 0.000800\n",
            "Training Epoch: 194 [43648/50000]\tLoss: 0.1386\tLR: 0.000800\n",
            "Training Epoch: 194 [43776/50000]\tLoss: 0.1541\tLR: 0.000800\n",
            "Training Epoch: 194 [43904/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 194 [44032/50000]\tLoss: 0.1402\tLR: 0.000800\n",
            "Training Epoch: 194 [44160/50000]\tLoss: 0.2190\tLR: 0.000800\n",
            "Training Epoch: 194 [44288/50000]\tLoss: 0.1404\tLR: 0.000800\n",
            "Training Epoch: 194 [44416/50000]\tLoss: 0.1254\tLR: 0.000800\n",
            "Training Epoch: 194 [44544/50000]\tLoss: 0.1091\tLR: 0.000800\n",
            "Training Epoch: 194 [44672/50000]\tLoss: 0.1893\tLR: 0.000800\n",
            "Training Epoch: 194 [44800/50000]\tLoss: 0.1515\tLR: 0.000800\n",
            "Training Epoch: 194 [44928/50000]\tLoss: 0.1035\tLR: 0.000800\n",
            "Training Epoch: 194 [45056/50000]\tLoss: 0.1090\tLR: 0.000800\n",
            "Training Epoch: 194 [45184/50000]\tLoss: 0.1437\tLR: 0.000800\n",
            "Training Epoch: 194 [45312/50000]\tLoss: 0.1518\tLR: 0.000800\n",
            "Training Epoch: 194 [45440/50000]\tLoss: 0.1227\tLR: 0.000800\n",
            "Training Epoch: 194 [45568/50000]\tLoss: 0.1509\tLR: 0.000800\n",
            "Training Epoch: 194 [45696/50000]\tLoss: 0.1693\tLR: 0.000800\n",
            "Training Epoch: 194 [45824/50000]\tLoss: 0.1438\tLR: 0.000800\n",
            "Training Epoch: 194 [45952/50000]\tLoss: 0.1862\tLR: 0.000800\n",
            "Training Epoch: 194 [46080/50000]\tLoss: 0.1460\tLR: 0.000800\n",
            "Training Epoch: 194 [46208/50000]\tLoss: 0.1954\tLR: 0.000800\n",
            "Training Epoch: 194 [46336/50000]\tLoss: 0.1163\tLR: 0.000800\n",
            "Training Epoch: 194 [46464/50000]\tLoss: 0.1502\tLR: 0.000800\n",
            "Training Epoch: 194 [46592/50000]\tLoss: 0.1085\tLR: 0.000800\n",
            "Training Epoch: 194 [46720/50000]\tLoss: 0.1731\tLR: 0.000800\n",
            "Training Epoch: 194 [46848/50000]\tLoss: 0.1700\tLR: 0.000800\n",
            "Training Epoch: 194 [46976/50000]\tLoss: 0.0697\tLR: 0.000800\n",
            "Training Epoch: 194 [47104/50000]\tLoss: 0.1891\tLR: 0.000800\n",
            "Training Epoch: 194 [47232/50000]\tLoss: 0.1871\tLR: 0.000800\n",
            "Training Epoch: 194 [47360/50000]\tLoss: 0.1638\tLR: 0.000800\n",
            "Training Epoch: 194 [47488/50000]\tLoss: 0.1467\tLR: 0.000800\n",
            "Training Epoch: 194 [47616/50000]\tLoss: 0.1058\tLR: 0.000800\n",
            "Training Epoch: 194 [47744/50000]\tLoss: 0.2017\tLR: 0.000800\n",
            "Training Epoch: 194 [47872/50000]\tLoss: 0.1448\tLR: 0.000800\n",
            "Training Epoch: 194 [48000/50000]\tLoss: 0.1094\tLR: 0.000800\n",
            "Training Epoch: 194 [48128/50000]\tLoss: 0.2044\tLR: 0.000800\n",
            "Training Epoch: 194 [48256/50000]\tLoss: 0.1223\tLR: 0.000800\n",
            "Training Epoch: 194 [48384/50000]\tLoss: 0.1156\tLR: 0.000800\n",
            "Training Epoch: 194 [48512/50000]\tLoss: 0.1820\tLR: 0.000800\n",
            "Training Epoch: 194 [48640/50000]\tLoss: 0.1810\tLR: 0.000800\n",
            "Training Epoch: 194 [48768/50000]\tLoss: 0.1648\tLR: 0.000800\n",
            "Training Epoch: 194 [48896/50000]\tLoss: 0.1337\tLR: 0.000800\n",
            "Training Epoch: 194 [49024/50000]\tLoss: 0.1315\tLR: 0.000800\n",
            "Training Epoch: 194 [49152/50000]\tLoss: 0.1224\tLR: 0.000800\n",
            "Training Epoch: 194 [49280/50000]\tLoss: 0.1578\tLR: 0.000800\n",
            "Training Epoch: 194 [49408/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 194 [49536/50000]\tLoss: 0.0787\tLR: 0.000800\n",
            "Training Epoch: 194 [49664/50000]\tLoss: 0.1532\tLR: 0.000800\n",
            "Training Epoch: 194 [49792/50000]\tLoss: 0.1178\tLR: 0.000800\n",
            "Training Epoch: 194 [49920/50000]\tLoss: 0.1654\tLR: 0.000800\n",
            "Training Epoch: 194 [50000/50000]\tLoss: 0.2214\tLR: 0.000800\n",
            "epoch 194 training time consumed: 29.33s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  87304 GiB |  87304 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  85642 GiB |  85642 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1662 GiB |   1662 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  87304 GiB |  87304 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  85642 GiB |  85642 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1662 GiB |   1662 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  87045 GiB |  87044 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  85384 GiB |  85384 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1660 GiB |   1660 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  53413 GiB |  53412 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  51696 GiB |  51696 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1716 GiB |   1716 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   33828 K  |   33828 K  |\n",
            "|       from large pool |       8    |      61    |   11348 K  |   11348 K  |\n",
            "|       from small pool |     373    |     464    |   22480 K  |   22479 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   33828 K  |   33828 K  |\n",
            "|       from large pool |       8    |      61    |   11348 K  |   11348 K  |\n",
            "|       from small pool |     373    |     464    |   22480 K  |   22479 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   11121 K  |   11121 K  |\n",
            "|       from large pool |       5    |      14    |    5410 K  |    5410 K  |\n",
            "|       from small pool |      10    |      16    |    5711 K  |    5711 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 194, Average loss: 0.0113, Accuracy: 0.6666, Time consumed:2.66s\n",
            "\n",
            "Training Epoch: 195 [128/50000]\tLoss: 0.1620\tLR: 0.000800\n",
            "Training Epoch: 195 [256/50000]\tLoss: 0.1644\tLR: 0.000800\n",
            "Training Epoch: 195 [384/50000]\tLoss: 0.1006\tLR: 0.000800\n",
            "Training Epoch: 195 [512/50000]\tLoss: 0.2109\tLR: 0.000800\n",
            "Training Epoch: 195 [640/50000]\tLoss: 0.1031\tLR: 0.000800\n",
            "Training Epoch: 195 [768/50000]\tLoss: 0.1240\tLR: 0.000800\n",
            "Training Epoch: 195 [896/50000]\tLoss: 0.1373\tLR: 0.000800\n",
            "Training Epoch: 195 [1024/50000]\tLoss: 0.1379\tLR: 0.000800\n",
            "Training Epoch: 195 [1152/50000]\tLoss: 0.1633\tLR: 0.000800\n",
            "Training Epoch: 195 [1280/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 195 [1408/50000]\tLoss: 0.1519\tLR: 0.000800\n",
            "Training Epoch: 195 [1536/50000]\tLoss: 0.1727\tLR: 0.000800\n",
            "Training Epoch: 195 [1664/50000]\tLoss: 0.2208\tLR: 0.000800\n",
            "Training Epoch: 195 [1792/50000]\tLoss: 0.1046\tLR: 0.000800\n",
            "Training Epoch: 195 [1920/50000]\tLoss: 0.1829\tLR: 0.000800\n",
            "Training Epoch: 195 [2048/50000]\tLoss: 0.1544\tLR: 0.000800\n",
            "Training Epoch: 195 [2176/50000]\tLoss: 0.1015\tLR: 0.000800\n",
            "Training Epoch: 195 [2304/50000]\tLoss: 0.1238\tLR: 0.000800\n",
            "Training Epoch: 195 [2432/50000]\tLoss: 0.1853\tLR: 0.000800\n",
            "Training Epoch: 195 [2560/50000]\tLoss: 0.0695\tLR: 0.000800\n",
            "Training Epoch: 195 [2688/50000]\tLoss: 0.1105\tLR: 0.000800\n",
            "Training Epoch: 195 [2816/50000]\tLoss: 0.1174\tLR: 0.000800\n",
            "Training Epoch: 195 [2944/50000]\tLoss: 0.1514\tLR: 0.000800\n",
            "Training Epoch: 195 [3072/50000]\tLoss: 0.1577\tLR: 0.000800\n",
            "Training Epoch: 195 [3200/50000]\tLoss: 0.1210\tLR: 0.000800\n",
            "Training Epoch: 195 [3328/50000]\tLoss: 0.1698\tLR: 0.000800\n",
            "Training Epoch: 195 [3456/50000]\tLoss: 0.1303\tLR: 0.000800\n",
            "Training Epoch: 195 [3584/50000]\tLoss: 0.1854\tLR: 0.000800\n",
            "Training Epoch: 195 [3712/50000]\tLoss: 0.1357\tLR: 0.000800\n",
            "Training Epoch: 195 [3840/50000]\tLoss: 0.1519\tLR: 0.000800\n",
            "Training Epoch: 195 [3968/50000]\tLoss: 0.1433\tLR: 0.000800\n",
            "Training Epoch: 195 [4096/50000]\tLoss: 0.1431\tLR: 0.000800\n",
            "Training Epoch: 195 [4224/50000]\tLoss: 0.1466\tLR: 0.000800\n",
            "Training Epoch: 195 [4352/50000]\tLoss: 0.0772\tLR: 0.000800\n",
            "Training Epoch: 195 [4480/50000]\tLoss: 0.0935\tLR: 0.000800\n",
            "Training Epoch: 195 [4608/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 195 [4736/50000]\tLoss: 0.1353\tLR: 0.000800\n",
            "Training Epoch: 195 [4864/50000]\tLoss: 0.0599\tLR: 0.000800\n",
            "Training Epoch: 195 [4992/50000]\tLoss: 0.1096\tLR: 0.000800\n",
            "Training Epoch: 195 [5120/50000]\tLoss: 0.1188\tLR: 0.000800\n",
            "Training Epoch: 195 [5248/50000]\tLoss: 0.1074\tLR: 0.000800\n",
            "Training Epoch: 195 [5376/50000]\tLoss: 0.1357\tLR: 0.000800\n",
            "Training Epoch: 195 [5504/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 195 [5632/50000]\tLoss: 0.1345\tLR: 0.000800\n",
            "Training Epoch: 195 [5760/50000]\tLoss: 0.0879\tLR: 0.000800\n",
            "Training Epoch: 195 [5888/50000]\tLoss: 0.1130\tLR: 0.000800\n",
            "Training Epoch: 195 [6016/50000]\tLoss: 0.1313\tLR: 0.000800\n",
            "Training Epoch: 195 [6144/50000]\tLoss: 0.1964\tLR: 0.000800\n",
            "Training Epoch: 195 [6272/50000]\tLoss: 0.1588\tLR: 0.000800\n",
            "Training Epoch: 195 [6400/50000]\tLoss: 0.1796\tLR: 0.000800\n",
            "Training Epoch: 195 [6528/50000]\tLoss: 0.1089\tLR: 0.000800\n",
            "Training Epoch: 195 [6656/50000]\tLoss: 0.1409\tLR: 0.000800\n",
            "Training Epoch: 195 [6784/50000]\tLoss: 0.1197\tLR: 0.000800\n",
            "Training Epoch: 195 [6912/50000]\tLoss: 0.0756\tLR: 0.000800\n",
            "Training Epoch: 195 [7040/50000]\tLoss: 0.1211\tLR: 0.000800\n",
            "Training Epoch: 195 [7168/50000]\tLoss: 0.1017\tLR: 0.000800\n",
            "Training Epoch: 195 [7296/50000]\tLoss: 0.0820\tLR: 0.000800\n",
            "Training Epoch: 195 [7424/50000]\tLoss: 0.1263\tLR: 0.000800\n",
            "Training Epoch: 195 [7552/50000]\tLoss: 0.1426\tLR: 0.000800\n",
            "Training Epoch: 195 [7680/50000]\tLoss: 0.1637\tLR: 0.000800\n",
            "Training Epoch: 195 [7808/50000]\tLoss: 0.1550\tLR: 0.000800\n",
            "Training Epoch: 195 [7936/50000]\tLoss: 0.1918\tLR: 0.000800\n",
            "Training Epoch: 195 [8064/50000]\tLoss: 0.1412\tLR: 0.000800\n",
            "Training Epoch: 195 [8192/50000]\tLoss: 0.1483\tLR: 0.000800\n",
            "Training Epoch: 195 [8320/50000]\tLoss: 0.2025\tLR: 0.000800\n",
            "Training Epoch: 195 [8448/50000]\tLoss: 0.1143\tLR: 0.000800\n",
            "Training Epoch: 195 [8576/50000]\tLoss: 0.2216\tLR: 0.000800\n",
            "Training Epoch: 195 [8704/50000]\tLoss: 0.1532\tLR: 0.000800\n",
            "Training Epoch: 195 [8832/50000]\tLoss: 0.0896\tLR: 0.000800\n",
            "Training Epoch: 195 [8960/50000]\tLoss: 0.1372\tLR: 0.000800\n",
            "Training Epoch: 195 [9088/50000]\tLoss: 0.1671\tLR: 0.000800\n",
            "Training Epoch: 195 [9216/50000]\tLoss: 0.0930\tLR: 0.000800\n",
            "Training Epoch: 195 [9344/50000]\tLoss: 0.1177\tLR: 0.000800\n",
            "Training Epoch: 195 [9472/50000]\tLoss: 0.1398\tLR: 0.000800\n",
            "Training Epoch: 195 [9600/50000]\tLoss: 0.1204\tLR: 0.000800\n",
            "Training Epoch: 195 [9728/50000]\tLoss: 0.1101\tLR: 0.000800\n",
            "Training Epoch: 195 [9856/50000]\tLoss: 0.1321\tLR: 0.000800\n",
            "Training Epoch: 195 [9984/50000]\tLoss: 0.1184\tLR: 0.000800\n",
            "Training Epoch: 195 [10112/50000]\tLoss: 0.0993\tLR: 0.000800\n",
            "Training Epoch: 195 [10240/50000]\tLoss: 0.1080\tLR: 0.000800\n",
            "Training Epoch: 195 [10368/50000]\tLoss: 0.1258\tLR: 0.000800\n",
            "Training Epoch: 195 [10496/50000]\tLoss: 0.1268\tLR: 0.000800\n",
            "Training Epoch: 195 [10624/50000]\tLoss: 0.1246\tLR: 0.000800\n",
            "Training Epoch: 195 [10752/50000]\tLoss: 0.0991\tLR: 0.000800\n",
            "Training Epoch: 195 [10880/50000]\tLoss: 0.1207\tLR: 0.000800\n",
            "Training Epoch: 195 [11008/50000]\tLoss: 0.1581\tLR: 0.000800\n",
            "Training Epoch: 195 [11136/50000]\tLoss: 0.1826\tLR: 0.000800\n",
            "Training Epoch: 195 [11264/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 195 [11392/50000]\tLoss: 0.1495\tLR: 0.000800\n",
            "Training Epoch: 195 [11520/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 195 [11648/50000]\tLoss: 0.2130\tLR: 0.000800\n",
            "Training Epoch: 195 [11776/50000]\tLoss: 0.1298\tLR: 0.000800\n",
            "Training Epoch: 195 [11904/50000]\tLoss: 0.1341\tLR: 0.000800\n",
            "Training Epoch: 195 [12032/50000]\tLoss: 0.1428\tLR: 0.000800\n",
            "Training Epoch: 195 [12160/50000]\tLoss: 0.1392\tLR: 0.000800\n",
            "Training Epoch: 195 [12288/50000]\tLoss: 0.1567\tLR: 0.000800\n",
            "Training Epoch: 195 [12416/50000]\tLoss: 0.1113\tLR: 0.000800\n",
            "Training Epoch: 195 [12544/50000]\tLoss: 0.1086\tLR: 0.000800\n",
            "Training Epoch: 195 [12672/50000]\tLoss: 0.1428\tLR: 0.000800\n",
            "Training Epoch: 195 [12800/50000]\tLoss: 0.1404\tLR: 0.000800\n",
            "Training Epoch: 195 [12928/50000]\tLoss: 0.1603\tLR: 0.000800\n",
            "Training Epoch: 195 [13056/50000]\tLoss: 0.1268\tLR: 0.000800\n",
            "Training Epoch: 195 [13184/50000]\tLoss: 0.1959\tLR: 0.000800\n",
            "Training Epoch: 195 [13312/50000]\tLoss: 0.1447\tLR: 0.000800\n",
            "Training Epoch: 195 [13440/50000]\tLoss: 0.1565\tLR: 0.000800\n",
            "Training Epoch: 195 [13568/50000]\tLoss: 0.1147\tLR: 0.000800\n",
            "Training Epoch: 195 [13696/50000]\tLoss: 0.1644\tLR: 0.000800\n",
            "Training Epoch: 195 [13824/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 195 [13952/50000]\tLoss: 0.1022\tLR: 0.000800\n",
            "Training Epoch: 195 [14080/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 195 [14208/50000]\tLoss: 0.1340\tLR: 0.000800\n",
            "Training Epoch: 195 [14336/50000]\tLoss: 0.1379\tLR: 0.000800\n",
            "Training Epoch: 195 [14464/50000]\tLoss: 0.1488\tLR: 0.000800\n",
            "Training Epoch: 195 [14592/50000]\tLoss: 0.0990\tLR: 0.000800\n",
            "Training Epoch: 195 [14720/50000]\tLoss: 0.2068\tLR: 0.000800\n",
            "Training Epoch: 195 [14848/50000]\tLoss: 0.1120\tLR: 0.000800\n",
            "Training Epoch: 195 [14976/50000]\tLoss: 0.1212\tLR: 0.000800\n",
            "Training Epoch: 195 [15104/50000]\tLoss: 0.1878\tLR: 0.000800\n",
            "Training Epoch: 195 [15232/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 195 [15360/50000]\tLoss: 0.1865\tLR: 0.000800\n",
            "Training Epoch: 195 [15488/50000]\tLoss: 0.1209\tLR: 0.000800\n",
            "Training Epoch: 195 [15616/50000]\tLoss: 0.1082\tLR: 0.000800\n",
            "Training Epoch: 195 [15744/50000]\tLoss: 0.1742\tLR: 0.000800\n",
            "Training Epoch: 195 [15872/50000]\tLoss: 0.1534\tLR: 0.000800\n",
            "Training Epoch: 195 [16000/50000]\tLoss: 0.1005\tLR: 0.000800\n",
            "Training Epoch: 195 [16128/50000]\tLoss: 0.1112\tLR: 0.000800\n",
            "Training Epoch: 195 [16256/50000]\tLoss: 0.1072\tLR: 0.000800\n",
            "Training Epoch: 195 [16384/50000]\tLoss: 0.1439\tLR: 0.000800\n",
            "Training Epoch: 195 [16512/50000]\tLoss: 0.1157\tLR: 0.000800\n",
            "Training Epoch: 195 [16640/50000]\tLoss: 0.1291\tLR: 0.000800\n",
            "Training Epoch: 195 [16768/50000]\tLoss: 0.1444\tLR: 0.000800\n",
            "Training Epoch: 195 [16896/50000]\tLoss: 0.1030\tLR: 0.000800\n",
            "Training Epoch: 195 [17024/50000]\tLoss: 0.1080\tLR: 0.000800\n",
            "Training Epoch: 195 [17152/50000]\tLoss: 0.1275\tLR: 0.000800\n",
            "Training Epoch: 195 [17280/50000]\tLoss: 0.1371\tLR: 0.000800\n",
            "Training Epoch: 195 [17408/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 195 [17536/50000]\tLoss: 0.1451\tLR: 0.000800\n",
            "Training Epoch: 195 [17664/50000]\tLoss: 0.0899\tLR: 0.000800\n",
            "Training Epoch: 195 [17792/50000]\tLoss: 0.1508\tLR: 0.000800\n",
            "Training Epoch: 195 [17920/50000]\tLoss: 0.1215\tLR: 0.000800\n",
            "Training Epoch: 195 [18048/50000]\tLoss: 0.1393\tLR: 0.000800\n",
            "Training Epoch: 195 [18176/50000]\tLoss: 0.1473\tLR: 0.000800\n",
            "Training Epoch: 195 [18304/50000]\tLoss: 0.1310\tLR: 0.000800\n",
            "Training Epoch: 195 [18432/50000]\tLoss: 0.1345\tLR: 0.000800\n",
            "Training Epoch: 195 [18560/50000]\tLoss: 0.1111\tLR: 0.000800\n",
            "Training Epoch: 195 [18688/50000]\tLoss: 0.1513\tLR: 0.000800\n",
            "Training Epoch: 195 [18816/50000]\tLoss: 0.1413\tLR: 0.000800\n",
            "Training Epoch: 195 [18944/50000]\tLoss: 0.1993\tLR: 0.000800\n",
            "Training Epoch: 195 [19072/50000]\tLoss: 0.1777\tLR: 0.000800\n",
            "Training Epoch: 195 [19200/50000]\tLoss: 0.1527\tLR: 0.000800\n",
            "Training Epoch: 195 [19328/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 195 [19456/50000]\tLoss: 0.1620\tLR: 0.000800\n",
            "Training Epoch: 195 [19584/50000]\tLoss: 0.1879\tLR: 0.000800\n",
            "Training Epoch: 195 [19712/50000]\tLoss: 0.1742\tLR: 0.000800\n",
            "Training Epoch: 195 [19840/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 195 [19968/50000]\tLoss: 0.1851\tLR: 0.000800\n",
            "Training Epoch: 195 [20096/50000]\tLoss: 0.1335\tLR: 0.000800\n",
            "Training Epoch: 195 [20224/50000]\tLoss: 0.1227\tLR: 0.000800\n",
            "Training Epoch: 195 [20352/50000]\tLoss: 0.1505\tLR: 0.000800\n",
            "Training Epoch: 195 [20480/50000]\tLoss: 0.1881\tLR: 0.000800\n",
            "Training Epoch: 195 [20608/50000]\tLoss: 0.1358\tLR: 0.000800\n",
            "Training Epoch: 195 [20736/50000]\tLoss: 0.1166\tLR: 0.000800\n",
            "Training Epoch: 195 [20864/50000]\tLoss: 0.1625\tLR: 0.000800\n",
            "Training Epoch: 195 [20992/50000]\tLoss: 0.1147\tLR: 0.000800\n",
            "Training Epoch: 195 [21120/50000]\tLoss: 0.2117\tLR: 0.000800\n",
            "Training Epoch: 195 [21248/50000]\tLoss: 0.1081\tLR: 0.000800\n",
            "Training Epoch: 195 [21376/50000]\tLoss: 0.1930\tLR: 0.000800\n",
            "Training Epoch: 195 [21504/50000]\tLoss: 0.1376\tLR: 0.000800\n",
            "Training Epoch: 195 [21632/50000]\tLoss: 0.0937\tLR: 0.000800\n",
            "Training Epoch: 195 [21760/50000]\tLoss: 0.0755\tLR: 0.000800\n",
            "Training Epoch: 195 [21888/50000]\tLoss: 0.1764\tLR: 0.000800\n",
            "Training Epoch: 195 [22016/50000]\tLoss: 0.1530\tLR: 0.000800\n",
            "Training Epoch: 195 [22144/50000]\tLoss: 0.1323\tLR: 0.000800\n",
            "Training Epoch: 195 [22272/50000]\tLoss: 0.1130\tLR: 0.000800\n",
            "Training Epoch: 195 [22400/50000]\tLoss: 0.1233\tLR: 0.000800\n",
            "Training Epoch: 195 [22528/50000]\tLoss: 0.1268\tLR: 0.000800\n",
            "Training Epoch: 195 [22656/50000]\tLoss: 0.1307\tLR: 0.000800\n",
            "Training Epoch: 195 [22784/50000]\tLoss: 0.1365\tLR: 0.000800\n",
            "Training Epoch: 195 [22912/50000]\tLoss: 0.0991\tLR: 0.000800\n",
            "Training Epoch: 195 [23040/50000]\tLoss: 0.1029\tLR: 0.000800\n",
            "Training Epoch: 195 [23168/50000]\tLoss: 0.1079\tLR: 0.000800\n",
            "Training Epoch: 195 [23296/50000]\tLoss: 0.1683\tLR: 0.000800\n",
            "Training Epoch: 195 [23424/50000]\tLoss: 0.1435\tLR: 0.000800\n",
            "Training Epoch: 195 [23552/50000]\tLoss: 0.1220\tLR: 0.000800\n",
            "Training Epoch: 195 [23680/50000]\tLoss: 0.1421\tLR: 0.000800\n",
            "Training Epoch: 195 [23808/50000]\tLoss: 0.1282\tLR: 0.000800\n",
            "Training Epoch: 195 [23936/50000]\tLoss: 0.1354\tLR: 0.000800\n",
            "Training Epoch: 195 [24064/50000]\tLoss: 0.1187\tLR: 0.000800\n",
            "Training Epoch: 195 [24192/50000]\tLoss: 0.1135\tLR: 0.000800\n",
            "Training Epoch: 195 [24320/50000]\tLoss: 0.1507\tLR: 0.000800\n",
            "Training Epoch: 195 [24448/50000]\tLoss: 0.1374\tLR: 0.000800\n",
            "Training Epoch: 195 [24576/50000]\tLoss: 0.1618\tLR: 0.000800\n",
            "Training Epoch: 195 [24704/50000]\tLoss: 0.1570\tLR: 0.000800\n",
            "Training Epoch: 195 [24832/50000]\tLoss: 0.1432\tLR: 0.000800\n",
            "Training Epoch: 195 [24960/50000]\tLoss: 0.1081\tLR: 0.000800\n",
            "Training Epoch: 195 [25088/50000]\tLoss: 0.1770\tLR: 0.000800\n",
            "Training Epoch: 195 [25216/50000]\tLoss: 0.0759\tLR: 0.000800\n",
            "Training Epoch: 195 [25344/50000]\tLoss: 0.1097\tLR: 0.000800\n",
            "Training Epoch: 195 [25472/50000]\tLoss: 0.1494\tLR: 0.000800\n",
            "Training Epoch: 195 [25600/50000]\tLoss: 0.1012\tLR: 0.000800\n",
            "Training Epoch: 195 [25728/50000]\tLoss: 0.1168\tLR: 0.000800\n",
            "Training Epoch: 195 [25856/50000]\tLoss: 0.1696\tLR: 0.000800\n",
            "Training Epoch: 195 [25984/50000]\tLoss: 0.2003\tLR: 0.000800\n",
            "Training Epoch: 195 [26112/50000]\tLoss: 0.1421\tLR: 0.000800\n",
            "Training Epoch: 195 [26240/50000]\tLoss: 0.1419\tLR: 0.000800\n",
            "Training Epoch: 195 [26368/50000]\tLoss: 0.0928\tLR: 0.000800\n",
            "Training Epoch: 195 [26496/50000]\tLoss: 0.1438\tLR: 0.000800\n",
            "Training Epoch: 195 [26624/50000]\tLoss: 0.1357\tLR: 0.000800\n",
            "Training Epoch: 195 [26752/50000]\tLoss: 0.1553\tLR: 0.000800\n",
            "Training Epoch: 195 [26880/50000]\tLoss: 0.1903\tLR: 0.000800\n",
            "Training Epoch: 195 [27008/50000]\tLoss: 0.1110\tLR: 0.000800\n",
            "Training Epoch: 195 [27136/50000]\tLoss: 0.1370\tLR: 0.000800\n",
            "Training Epoch: 195 [27264/50000]\tLoss: 0.1176\tLR: 0.000800\n",
            "Training Epoch: 195 [27392/50000]\tLoss: 0.1580\tLR: 0.000800\n",
            "Training Epoch: 195 [27520/50000]\tLoss: 0.1031\tLR: 0.000800\n",
            "Training Epoch: 195 [27648/50000]\tLoss: 0.1088\tLR: 0.000800\n",
            "Training Epoch: 195 [27776/50000]\tLoss: 0.1137\tLR: 0.000800\n",
            "Training Epoch: 195 [27904/50000]\tLoss: 0.0855\tLR: 0.000800\n",
            "Training Epoch: 195 [28032/50000]\tLoss: 0.1370\tLR: 0.000800\n",
            "Training Epoch: 195 [28160/50000]\tLoss: 0.1051\tLR: 0.000800\n",
            "Training Epoch: 195 [28288/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 195 [28416/50000]\tLoss: 0.1087\tLR: 0.000800\n",
            "Training Epoch: 195 [28544/50000]\tLoss: 0.2028\tLR: 0.000800\n",
            "Training Epoch: 195 [28672/50000]\tLoss: 0.2051\tLR: 0.000800\n",
            "Training Epoch: 195 [28800/50000]\tLoss: 0.1435\tLR: 0.000800\n",
            "Training Epoch: 195 [28928/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 195 [29056/50000]\tLoss: 0.0989\tLR: 0.000800\n",
            "Training Epoch: 195 [29184/50000]\tLoss: 0.1439\tLR: 0.000800\n",
            "Training Epoch: 195 [29312/50000]\tLoss: 0.0975\tLR: 0.000800\n",
            "Training Epoch: 195 [29440/50000]\tLoss: 0.1738\tLR: 0.000800\n",
            "Training Epoch: 195 [29568/50000]\tLoss: 0.1628\tLR: 0.000800\n",
            "Training Epoch: 195 [29696/50000]\tLoss: 0.1385\tLR: 0.000800\n",
            "Training Epoch: 195 [29824/50000]\tLoss: 0.2156\tLR: 0.000800\n",
            "Training Epoch: 195 [29952/50000]\tLoss: 0.1823\tLR: 0.000800\n",
            "Training Epoch: 195 [30080/50000]\tLoss: 0.1461\tLR: 0.000800\n",
            "Training Epoch: 195 [30208/50000]\tLoss: 0.1602\tLR: 0.000800\n",
            "Training Epoch: 195 [30336/50000]\tLoss: 0.1138\tLR: 0.000800\n",
            "Training Epoch: 195 [30464/50000]\tLoss: 0.1689\tLR: 0.000800\n",
            "Training Epoch: 195 [30592/50000]\tLoss: 0.1833\tLR: 0.000800\n",
            "Training Epoch: 195 [30720/50000]\tLoss: 0.1446\tLR: 0.000800\n",
            "Training Epoch: 195 [30848/50000]\tLoss: 0.2051\tLR: 0.000800\n",
            "Training Epoch: 195 [30976/50000]\tLoss: 0.1376\tLR: 0.000800\n",
            "Training Epoch: 195 [31104/50000]\tLoss: 0.1807\tLR: 0.000800\n",
            "Training Epoch: 195 [31232/50000]\tLoss: 0.1739\tLR: 0.000800\n",
            "Training Epoch: 195 [31360/50000]\tLoss: 0.0802\tLR: 0.000800\n",
            "Training Epoch: 195 [31488/50000]\tLoss: 0.1005\tLR: 0.000800\n",
            "Training Epoch: 195 [31616/50000]\tLoss: 0.1718\tLR: 0.000800\n",
            "Training Epoch: 195 [31744/50000]\tLoss: 0.1808\tLR: 0.000800\n",
            "Training Epoch: 195 [31872/50000]\tLoss: 0.1615\tLR: 0.000800\n",
            "Training Epoch: 195 [32000/50000]\tLoss: 0.1095\tLR: 0.000800\n",
            "Training Epoch: 195 [32128/50000]\tLoss: 0.1361\tLR: 0.000800\n",
            "Training Epoch: 195 [32256/50000]\tLoss: 0.1486\tLR: 0.000800\n",
            "Training Epoch: 195 [32384/50000]\tLoss: 0.1507\tLR: 0.000800\n",
            "Training Epoch: 195 [32512/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 195 [32640/50000]\tLoss: 0.2030\tLR: 0.000800\n",
            "Training Epoch: 195 [32768/50000]\tLoss: 0.1178\tLR: 0.000800\n",
            "Training Epoch: 195 [32896/50000]\tLoss: 0.1768\tLR: 0.000800\n",
            "Training Epoch: 195 [33024/50000]\tLoss: 0.0966\tLR: 0.000800\n",
            "Training Epoch: 195 [33152/50000]\tLoss: 0.1777\tLR: 0.000800\n",
            "Training Epoch: 195 [33280/50000]\tLoss: 0.1359\tLR: 0.000800\n",
            "Training Epoch: 195 [33408/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 195 [33536/50000]\tLoss: 0.1624\tLR: 0.000800\n",
            "Training Epoch: 195 [33664/50000]\tLoss: 0.1476\tLR: 0.000800\n",
            "Training Epoch: 195 [33792/50000]\tLoss: 0.1707\tLR: 0.000800\n",
            "Training Epoch: 195 [33920/50000]\tLoss: 0.2054\tLR: 0.000800\n",
            "Training Epoch: 195 [34048/50000]\tLoss: 0.1670\tLR: 0.000800\n",
            "Training Epoch: 195 [34176/50000]\tLoss: 0.1132\tLR: 0.000800\n",
            "Training Epoch: 195 [34304/50000]\tLoss: 0.1704\tLR: 0.000800\n",
            "Training Epoch: 195 [34432/50000]\tLoss: 0.0979\tLR: 0.000800\n",
            "Training Epoch: 195 [34560/50000]\tLoss: 0.1876\tLR: 0.000800\n",
            "Training Epoch: 195 [34688/50000]\tLoss: 0.1268\tLR: 0.000800\n",
            "Training Epoch: 195 [34816/50000]\tLoss: 0.1227\tLR: 0.000800\n",
            "Training Epoch: 195 [34944/50000]\tLoss: 0.1306\tLR: 0.000800\n",
            "Training Epoch: 195 [35072/50000]\tLoss: 0.1383\tLR: 0.000800\n",
            "Training Epoch: 195 [35200/50000]\tLoss: 0.1693\tLR: 0.000800\n",
            "Training Epoch: 195 [35328/50000]\tLoss: 0.2116\tLR: 0.000800\n",
            "Training Epoch: 195 [35456/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 195 [35584/50000]\tLoss: 0.1880\tLR: 0.000800\n",
            "Training Epoch: 195 [35712/50000]\tLoss: 0.1380\tLR: 0.000800\n",
            "Training Epoch: 195 [35840/50000]\tLoss: 0.0926\tLR: 0.000800\n",
            "Training Epoch: 195 [35968/50000]\tLoss: 0.1588\tLR: 0.000800\n",
            "Training Epoch: 195 [36096/50000]\tLoss: 0.1072\tLR: 0.000800\n",
            "Training Epoch: 195 [36224/50000]\tLoss: 0.0997\tLR: 0.000800\n",
            "Training Epoch: 195 [36352/50000]\tLoss: 0.1786\tLR: 0.000800\n",
            "Training Epoch: 195 [36480/50000]\tLoss: 0.1757\tLR: 0.000800\n",
            "Training Epoch: 195 [36608/50000]\tLoss: 0.1930\tLR: 0.000800\n",
            "Training Epoch: 195 [36736/50000]\tLoss: 0.1570\tLR: 0.000800\n",
            "Training Epoch: 195 [36864/50000]\tLoss: 0.1157\tLR: 0.000800\n",
            "Training Epoch: 195 [36992/50000]\tLoss: 0.0962\tLR: 0.000800\n",
            "Training Epoch: 195 [37120/50000]\tLoss: 0.1291\tLR: 0.000800\n",
            "Training Epoch: 195 [37248/50000]\tLoss: 0.1657\tLR: 0.000800\n",
            "Training Epoch: 195 [37376/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 195 [37504/50000]\tLoss: 0.1726\tLR: 0.000800\n",
            "Training Epoch: 195 [37632/50000]\tLoss: 0.1734\tLR: 0.000800\n",
            "Training Epoch: 195 [37760/50000]\tLoss: 0.1390\tLR: 0.000800\n",
            "Training Epoch: 195 [37888/50000]\tLoss: 0.1512\tLR: 0.000800\n",
            "Training Epoch: 195 [38016/50000]\tLoss: 0.1877\tLR: 0.000800\n",
            "Training Epoch: 195 [38144/50000]\tLoss: 0.0892\tLR: 0.000800\n",
            "Training Epoch: 195 [38272/50000]\tLoss: 0.1286\tLR: 0.000800\n",
            "Training Epoch: 195 [38400/50000]\tLoss: 0.1640\tLR: 0.000800\n",
            "Training Epoch: 195 [38528/50000]\tLoss: 0.1648\tLR: 0.000800\n",
            "Training Epoch: 195 [38656/50000]\tLoss: 0.1687\tLR: 0.000800\n",
            "Training Epoch: 195 [38784/50000]\tLoss: 0.2058\tLR: 0.000800\n",
            "Training Epoch: 195 [38912/50000]\tLoss: 0.2061\tLR: 0.000800\n",
            "Training Epoch: 195 [39040/50000]\tLoss: 0.1319\tLR: 0.000800\n",
            "Training Epoch: 195 [39168/50000]\tLoss: 0.0991\tLR: 0.000800\n",
            "Training Epoch: 195 [39296/50000]\tLoss: 0.1200\tLR: 0.000800\n",
            "Training Epoch: 195 [39424/50000]\tLoss: 0.1104\tLR: 0.000800\n",
            "Training Epoch: 195 [39552/50000]\tLoss: 0.1731\tLR: 0.000800\n",
            "Training Epoch: 195 [39680/50000]\tLoss: 0.1756\tLR: 0.000800\n",
            "Training Epoch: 195 [39808/50000]\tLoss: 0.1352\tLR: 0.000800\n",
            "Training Epoch: 195 [39936/50000]\tLoss: 0.1616\tLR: 0.000800\n",
            "Training Epoch: 195 [40064/50000]\tLoss: 0.1321\tLR: 0.000800\n",
            "Training Epoch: 195 [40192/50000]\tLoss: 0.1356\tLR: 0.000800\n",
            "Training Epoch: 195 [40320/50000]\tLoss: 0.1923\tLR: 0.000800\n",
            "Training Epoch: 195 [40448/50000]\tLoss: 0.1080\tLR: 0.000800\n",
            "Training Epoch: 195 [40576/50000]\tLoss: 0.1914\tLR: 0.000800\n",
            "Training Epoch: 195 [40704/50000]\tLoss: 0.1452\tLR: 0.000800\n",
            "Training Epoch: 195 [40832/50000]\tLoss: 0.1141\tLR: 0.000800\n",
            "Training Epoch: 195 [40960/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 195 [41088/50000]\tLoss: 0.1400\tLR: 0.000800\n",
            "Training Epoch: 195 [41216/50000]\tLoss: 0.1497\tLR: 0.000800\n",
            "Training Epoch: 195 [41344/50000]\tLoss: 0.1582\tLR: 0.000800\n",
            "Training Epoch: 195 [41472/50000]\tLoss: 0.1494\tLR: 0.000800\n",
            "Training Epoch: 195 [41600/50000]\tLoss: 0.1630\tLR: 0.000800\n",
            "Training Epoch: 195 [41728/50000]\tLoss: 0.2239\tLR: 0.000800\n",
            "Training Epoch: 195 [41856/50000]\tLoss: 0.1213\tLR: 0.000800\n",
            "Training Epoch: 195 [41984/50000]\tLoss: 0.1479\tLR: 0.000800\n",
            "Training Epoch: 195 [42112/50000]\tLoss: 0.1518\tLR: 0.000800\n",
            "Training Epoch: 195 [42240/50000]\tLoss: 0.1636\tLR: 0.000800\n",
            "Training Epoch: 195 [42368/50000]\tLoss: 0.1240\tLR: 0.000800\n",
            "Training Epoch: 195 [42496/50000]\tLoss: 0.1275\tLR: 0.000800\n",
            "Training Epoch: 195 [42624/50000]\tLoss: 0.2383\tLR: 0.000800\n",
            "Training Epoch: 195 [42752/50000]\tLoss: 0.1127\tLR: 0.000800\n",
            "Training Epoch: 195 [42880/50000]\tLoss: 0.1747\tLR: 0.000800\n",
            "Training Epoch: 195 [43008/50000]\tLoss: 0.2110\tLR: 0.000800\n",
            "Training Epoch: 195 [43136/50000]\tLoss: 0.0785\tLR: 0.000800\n",
            "Training Epoch: 195 [43264/50000]\tLoss: 0.2079\tLR: 0.000800\n",
            "Training Epoch: 195 [43392/50000]\tLoss: 0.1446\tLR: 0.000800\n",
            "Training Epoch: 195 [43520/50000]\tLoss: 0.1707\tLR: 0.000800\n",
            "Training Epoch: 195 [43648/50000]\tLoss: 0.1160\tLR: 0.000800\n",
            "Training Epoch: 195 [43776/50000]\tLoss: 0.1402\tLR: 0.000800\n",
            "Training Epoch: 195 [43904/50000]\tLoss: 0.1969\tLR: 0.000800\n",
            "Training Epoch: 195 [44032/50000]\tLoss: 0.1079\tLR: 0.000800\n",
            "Training Epoch: 195 [44160/50000]\tLoss: 0.1387\tLR: 0.000800\n",
            "Training Epoch: 195 [44288/50000]\tLoss: 0.1707\tLR: 0.000800\n",
            "Training Epoch: 195 [44416/50000]\tLoss: 0.1125\tLR: 0.000800\n",
            "Training Epoch: 195 [44544/50000]\tLoss: 0.1505\tLR: 0.000800\n",
            "Training Epoch: 195 [44672/50000]\tLoss: 0.1382\tLR: 0.000800\n",
            "Training Epoch: 195 [44800/50000]\tLoss: 0.1644\tLR: 0.000800\n",
            "Training Epoch: 195 [44928/50000]\tLoss: 0.1300\tLR: 0.000800\n",
            "Training Epoch: 195 [45056/50000]\tLoss: 0.1499\tLR: 0.000800\n",
            "Training Epoch: 195 [45184/50000]\tLoss: 0.1910\tLR: 0.000800\n",
            "Training Epoch: 195 [45312/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 195 [45440/50000]\tLoss: 0.1670\tLR: 0.000800\n",
            "Training Epoch: 195 [45568/50000]\tLoss: 0.1338\tLR: 0.000800\n",
            "Training Epoch: 195 [45696/50000]\tLoss: 0.1121\tLR: 0.000800\n",
            "Training Epoch: 195 [45824/50000]\tLoss: 0.1096\tLR: 0.000800\n",
            "Training Epoch: 195 [45952/50000]\tLoss: 0.1060\tLR: 0.000800\n",
            "Training Epoch: 195 [46080/50000]\tLoss: 0.1700\tLR: 0.000800\n",
            "Training Epoch: 195 [46208/50000]\tLoss: 0.1213\tLR: 0.000800\n",
            "Training Epoch: 195 [46336/50000]\tLoss: 0.1141\tLR: 0.000800\n",
            "Training Epoch: 195 [46464/50000]\tLoss: 0.1955\tLR: 0.000800\n",
            "Training Epoch: 195 [46592/50000]\tLoss: 0.1389\tLR: 0.000800\n",
            "Training Epoch: 195 [46720/50000]\tLoss: 0.2009\tLR: 0.000800\n",
            "Training Epoch: 195 [46848/50000]\tLoss: 0.3155\tLR: 0.000800\n",
            "Training Epoch: 195 [46976/50000]\tLoss: 0.1877\tLR: 0.000800\n",
            "Training Epoch: 195 [47104/50000]\tLoss: 0.1481\tLR: 0.000800\n",
            "Training Epoch: 195 [47232/50000]\tLoss: 0.0976\tLR: 0.000800\n",
            "Training Epoch: 195 [47360/50000]\tLoss: 0.1589\tLR: 0.000800\n",
            "Training Epoch: 195 [47488/50000]\tLoss: 0.1427\tLR: 0.000800\n",
            "Training Epoch: 195 [47616/50000]\tLoss: 0.0921\tLR: 0.000800\n",
            "Training Epoch: 195 [47744/50000]\tLoss: 0.1349\tLR: 0.000800\n",
            "Training Epoch: 195 [47872/50000]\tLoss: 0.1278\tLR: 0.000800\n",
            "Training Epoch: 195 [48000/50000]\tLoss: 0.1388\tLR: 0.000800\n",
            "Training Epoch: 195 [48128/50000]\tLoss: 0.1413\tLR: 0.000800\n",
            "Training Epoch: 195 [48256/50000]\tLoss: 0.1899\tLR: 0.000800\n",
            "Training Epoch: 195 [48384/50000]\tLoss: 0.1979\tLR: 0.000800\n",
            "Training Epoch: 195 [48512/50000]\tLoss: 0.1788\tLR: 0.000800\n",
            "Training Epoch: 195 [48640/50000]\tLoss: 0.1268\tLR: 0.000800\n",
            "Training Epoch: 195 [48768/50000]\tLoss: 0.2148\tLR: 0.000800\n",
            "Training Epoch: 195 [48896/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 195 [49024/50000]\tLoss: 0.1164\tLR: 0.000800\n",
            "Training Epoch: 195 [49152/50000]\tLoss: 0.1402\tLR: 0.000800\n",
            "Training Epoch: 195 [49280/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 195 [49408/50000]\tLoss: 0.1652\tLR: 0.000800\n",
            "Training Epoch: 195 [49536/50000]\tLoss: 0.1429\tLR: 0.000800\n",
            "Training Epoch: 195 [49664/50000]\tLoss: 0.1581\tLR: 0.000800\n",
            "Training Epoch: 195 [49792/50000]\tLoss: 0.0959\tLR: 0.000800\n",
            "Training Epoch: 195 [49920/50000]\tLoss: 0.1621\tLR: 0.000800\n",
            "Training Epoch: 195 [50000/50000]\tLoss: 0.1677\tLR: 0.000800\n",
            "epoch 195 training time consumed: 27.99s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  87754 GiB |  87754 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  86083 GiB |  86083 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1670 GiB |   1670 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  87754 GiB |  87754 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  86083 GiB |  86083 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1670 GiB |   1670 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  87493 GiB |  87493 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  85824 GiB |  85824 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1668 GiB |   1668 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  53688 GiB |  53688 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  51963 GiB |  51962 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1725 GiB |   1725 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   34003 K  |   34002 K  |\n",
            "|       from large pool |       8    |      61    |   11407 K  |   11407 K  |\n",
            "|       from small pool |     373    |     464    |   22595 K  |   22595 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   34003 K  |   34002 K  |\n",
            "|       from large pool |       8    |      61    |   11407 K  |   11407 K  |\n",
            "|       from small pool |     373    |     464    |   22595 K  |   22595 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   11178 K  |   11178 K  |\n",
            "|       from large pool |       5    |      14    |    5438 K  |    5438 K  |\n",
            "|       from small pool |      10    |      16    |    5740 K  |    5740 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 195, Average loss: 0.0112, Accuracy: 0.6684, Time consumed:2.62s\n",
            "\n",
            "Training Epoch: 196 [128/50000]\tLoss: 0.1284\tLR: 0.000800\n",
            "Training Epoch: 196 [256/50000]\tLoss: 0.1204\tLR: 0.000800\n",
            "Training Epoch: 196 [384/50000]\tLoss: 0.1301\tLR: 0.000800\n",
            "Training Epoch: 196 [512/50000]\tLoss: 0.1305\tLR: 0.000800\n",
            "Training Epoch: 196 [640/50000]\tLoss: 0.1588\tLR: 0.000800\n",
            "Training Epoch: 196 [768/50000]\tLoss: 0.0903\tLR: 0.000800\n",
            "Training Epoch: 196 [896/50000]\tLoss: 0.1144\tLR: 0.000800\n",
            "Training Epoch: 196 [1024/50000]\tLoss: 0.1847\tLR: 0.000800\n",
            "Training Epoch: 196 [1152/50000]\tLoss: 0.1846\tLR: 0.000800\n",
            "Training Epoch: 196 [1280/50000]\tLoss: 0.1328\tLR: 0.000800\n",
            "Training Epoch: 196 [1408/50000]\tLoss: 0.1154\tLR: 0.000800\n",
            "Training Epoch: 196 [1536/50000]\tLoss: 0.1532\tLR: 0.000800\n",
            "Training Epoch: 196 [1664/50000]\tLoss: 0.1347\tLR: 0.000800\n",
            "Training Epoch: 196 [1792/50000]\tLoss: 0.1218\tLR: 0.000800\n",
            "Training Epoch: 196 [1920/50000]\tLoss: 0.1128\tLR: 0.000800\n",
            "Training Epoch: 196 [2048/50000]\tLoss: 0.0769\tLR: 0.000800\n",
            "Training Epoch: 196 [2176/50000]\tLoss: 0.1134\tLR: 0.000800\n",
            "Training Epoch: 196 [2304/50000]\tLoss: 0.1097\tLR: 0.000800\n",
            "Training Epoch: 196 [2432/50000]\tLoss: 0.1590\tLR: 0.000800\n",
            "Training Epoch: 196 [2560/50000]\tLoss: 0.1721\tLR: 0.000800\n",
            "Training Epoch: 196 [2688/50000]\tLoss: 0.1875\tLR: 0.000800\n",
            "Training Epoch: 196 [2816/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 196 [2944/50000]\tLoss: 0.1337\tLR: 0.000800\n",
            "Training Epoch: 196 [3072/50000]\tLoss: 0.2177\tLR: 0.000800\n",
            "Training Epoch: 196 [3200/50000]\tLoss: 0.1127\tLR: 0.000800\n",
            "Training Epoch: 196 [3328/50000]\tLoss: 0.1776\tLR: 0.000800\n",
            "Training Epoch: 196 [3456/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 196 [3584/50000]\tLoss: 0.1445\tLR: 0.000800\n",
            "Training Epoch: 196 [3712/50000]\tLoss: 0.1429\tLR: 0.000800\n",
            "Training Epoch: 196 [3840/50000]\tLoss: 0.1844\tLR: 0.000800\n",
            "Training Epoch: 196 [3968/50000]\tLoss: 0.1402\tLR: 0.000800\n",
            "Training Epoch: 196 [4096/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 196 [4224/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 196 [4352/50000]\tLoss: 0.1097\tLR: 0.000800\n",
            "Training Epoch: 196 [4480/50000]\tLoss: 0.0977\tLR: 0.000800\n",
            "Training Epoch: 196 [4608/50000]\tLoss: 0.1465\tLR: 0.000800\n",
            "Training Epoch: 196 [4736/50000]\tLoss: 0.0787\tLR: 0.000800\n",
            "Training Epoch: 196 [4864/50000]\tLoss: 0.1467\tLR: 0.000800\n",
            "Training Epoch: 196 [4992/50000]\tLoss: 0.1208\tLR: 0.000800\n",
            "Training Epoch: 196 [5120/50000]\tLoss: 0.0959\tLR: 0.000800\n",
            "Training Epoch: 196 [5248/50000]\tLoss: 0.1750\tLR: 0.000800\n",
            "Training Epoch: 196 [5376/50000]\tLoss: 0.1003\tLR: 0.000800\n",
            "Training Epoch: 196 [5504/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 196 [5632/50000]\tLoss: 0.1417\tLR: 0.000800\n",
            "Training Epoch: 196 [5760/50000]\tLoss: 0.1377\tLR: 0.000800\n",
            "Training Epoch: 196 [5888/50000]\tLoss: 0.1605\tLR: 0.000800\n",
            "Training Epoch: 196 [6016/50000]\tLoss: 0.0872\tLR: 0.000800\n",
            "Training Epoch: 196 [6144/50000]\tLoss: 0.1276\tLR: 0.000800\n",
            "Training Epoch: 196 [6272/50000]\tLoss: 0.1528\tLR: 0.000800\n",
            "Training Epoch: 196 [6400/50000]\tLoss: 0.1355\tLR: 0.000800\n",
            "Training Epoch: 196 [6528/50000]\tLoss: 0.1780\tLR: 0.000800\n",
            "Training Epoch: 196 [6656/50000]\tLoss: 0.1889\tLR: 0.000800\n",
            "Training Epoch: 196 [6784/50000]\tLoss: 0.1307\tLR: 0.000800\n",
            "Training Epoch: 196 [6912/50000]\tLoss: 0.1672\tLR: 0.000800\n",
            "Training Epoch: 196 [7040/50000]\tLoss: 0.1278\tLR: 0.000800\n",
            "Training Epoch: 196 [7168/50000]\tLoss: 0.1026\tLR: 0.000800\n",
            "Training Epoch: 196 [7296/50000]\tLoss: 0.1559\tLR: 0.000800\n",
            "Training Epoch: 196 [7424/50000]\tLoss: 0.1190\tLR: 0.000800\n",
            "Training Epoch: 196 [7552/50000]\tLoss: 0.1113\tLR: 0.000800\n",
            "Training Epoch: 196 [7680/50000]\tLoss: 0.1339\tLR: 0.000800\n",
            "Training Epoch: 196 [7808/50000]\tLoss: 0.1370\tLR: 0.000800\n",
            "Training Epoch: 196 [7936/50000]\tLoss: 0.1770\tLR: 0.000800\n",
            "Training Epoch: 196 [8064/50000]\tLoss: 0.1640\tLR: 0.000800\n",
            "Training Epoch: 196 [8192/50000]\tLoss: 0.1276\tLR: 0.000800\n",
            "Training Epoch: 196 [8320/50000]\tLoss: 0.1387\tLR: 0.000800\n",
            "Training Epoch: 196 [8448/50000]\tLoss: 0.1011\tLR: 0.000800\n",
            "Training Epoch: 196 [8576/50000]\tLoss: 0.1502\tLR: 0.000800\n",
            "Training Epoch: 196 [8704/50000]\tLoss: 0.1723\tLR: 0.000800\n",
            "Training Epoch: 196 [8832/50000]\tLoss: 0.0971\tLR: 0.000800\n",
            "Training Epoch: 196 [8960/50000]\tLoss: 0.1174\tLR: 0.000800\n",
            "Training Epoch: 196 [9088/50000]\tLoss: 0.1257\tLR: 0.000800\n",
            "Training Epoch: 196 [9216/50000]\tLoss: 0.1427\tLR: 0.000800\n",
            "Training Epoch: 196 [9344/50000]\tLoss: 0.1587\tLR: 0.000800\n",
            "Training Epoch: 196 [9472/50000]\tLoss: 0.0733\tLR: 0.000800\n",
            "Training Epoch: 196 [9600/50000]\tLoss: 0.0839\tLR: 0.000800\n",
            "Training Epoch: 196 [9728/50000]\tLoss: 0.0913\tLR: 0.000800\n",
            "Training Epoch: 196 [9856/50000]\tLoss: 0.1428\tLR: 0.000800\n",
            "Training Epoch: 196 [9984/50000]\tLoss: 0.1232\tLR: 0.000800\n",
            "Training Epoch: 196 [10112/50000]\tLoss: 0.1356\tLR: 0.000800\n",
            "Training Epoch: 196 [10240/50000]\tLoss: 0.1210\tLR: 0.000800\n",
            "Training Epoch: 196 [10368/50000]\tLoss: 0.1202\tLR: 0.000800\n",
            "Training Epoch: 196 [10496/50000]\tLoss: 0.1344\tLR: 0.000800\n",
            "Training Epoch: 196 [10624/50000]\tLoss: 0.1221\tLR: 0.000800\n",
            "Training Epoch: 196 [10752/50000]\tLoss: 0.2057\tLR: 0.000800\n",
            "Training Epoch: 196 [10880/50000]\tLoss: 0.1185\tLR: 0.000800\n",
            "Training Epoch: 196 [11008/50000]\tLoss: 0.1062\tLR: 0.000800\n",
            "Training Epoch: 196 [11136/50000]\tLoss: 0.1284\tLR: 0.000800\n",
            "Training Epoch: 196 [11264/50000]\tLoss: 0.1808\tLR: 0.000800\n",
            "Training Epoch: 196 [11392/50000]\tLoss: 0.2270\tLR: 0.000800\n",
            "Training Epoch: 196 [11520/50000]\tLoss: 0.1448\tLR: 0.000800\n",
            "Training Epoch: 196 [11648/50000]\tLoss: 0.1297\tLR: 0.000800\n",
            "Training Epoch: 196 [11776/50000]\tLoss: 0.1153\tLR: 0.000800\n",
            "Training Epoch: 196 [11904/50000]\tLoss: 0.1775\tLR: 0.000800\n",
            "Training Epoch: 196 [12032/50000]\tLoss: 0.1454\tLR: 0.000800\n",
            "Training Epoch: 196 [12160/50000]\tLoss: 0.0890\tLR: 0.000800\n",
            "Training Epoch: 196 [12288/50000]\tLoss: 0.1111\tLR: 0.000800\n",
            "Training Epoch: 196 [12416/50000]\tLoss: 0.1331\tLR: 0.000800\n",
            "Training Epoch: 196 [12544/50000]\tLoss: 0.0955\tLR: 0.000800\n",
            "Training Epoch: 196 [12672/50000]\tLoss: 0.1368\tLR: 0.000800\n",
            "Training Epoch: 196 [12800/50000]\tLoss: 0.1926\tLR: 0.000800\n",
            "Training Epoch: 196 [12928/50000]\tLoss: 0.1517\tLR: 0.000800\n",
            "Training Epoch: 196 [13056/50000]\tLoss: 0.1507\tLR: 0.000800\n",
            "Training Epoch: 196 [13184/50000]\tLoss: 0.1926\tLR: 0.000800\n",
            "Training Epoch: 196 [13312/50000]\tLoss: 0.1097\tLR: 0.000800\n",
            "Training Epoch: 196 [13440/50000]\tLoss: 0.1091\tLR: 0.000800\n",
            "Training Epoch: 196 [13568/50000]\tLoss: 0.1360\tLR: 0.000800\n",
            "Training Epoch: 196 [13696/50000]\tLoss: 0.1689\tLR: 0.000800\n",
            "Training Epoch: 196 [13824/50000]\tLoss: 0.0919\tLR: 0.000800\n",
            "Training Epoch: 196 [13952/50000]\tLoss: 0.0736\tLR: 0.000800\n",
            "Training Epoch: 196 [14080/50000]\tLoss: 0.1699\tLR: 0.000800\n",
            "Training Epoch: 196 [14208/50000]\tLoss: 0.1896\tLR: 0.000800\n",
            "Training Epoch: 196 [14336/50000]\tLoss: 0.1747\tLR: 0.000800\n",
            "Training Epoch: 196 [14464/50000]\tLoss: 0.1585\tLR: 0.000800\n",
            "Training Epoch: 196 [14592/50000]\tLoss: 0.1890\tLR: 0.000800\n",
            "Training Epoch: 196 [14720/50000]\tLoss: 0.1801\tLR: 0.000800\n",
            "Training Epoch: 196 [14848/50000]\tLoss: 0.1151\tLR: 0.000800\n",
            "Training Epoch: 196 [14976/50000]\tLoss: 0.2632\tLR: 0.000800\n",
            "Training Epoch: 196 [15104/50000]\tLoss: 0.1659\tLR: 0.000800\n",
            "Training Epoch: 196 [15232/50000]\tLoss: 0.1257\tLR: 0.000800\n",
            "Training Epoch: 196 [15360/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 196 [15488/50000]\tLoss: 0.1687\tLR: 0.000800\n",
            "Training Epoch: 196 [15616/50000]\tLoss: 0.1033\tLR: 0.000800\n",
            "Training Epoch: 196 [15744/50000]\tLoss: 0.1505\tLR: 0.000800\n",
            "Training Epoch: 196 [15872/50000]\tLoss: 0.1011\tLR: 0.000800\n",
            "Training Epoch: 196 [16000/50000]\tLoss: 0.1565\tLR: 0.000800\n",
            "Training Epoch: 196 [16128/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 196 [16256/50000]\tLoss: 0.1246\tLR: 0.000800\n",
            "Training Epoch: 196 [16384/50000]\tLoss: 0.2021\tLR: 0.000800\n",
            "Training Epoch: 196 [16512/50000]\tLoss: 0.1638\tLR: 0.000800\n",
            "Training Epoch: 196 [16640/50000]\tLoss: 0.1059\tLR: 0.000800\n",
            "Training Epoch: 196 [16768/50000]\tLoss: 0.0943\tLR: 0.000800\n",
            "Training Epoch: 196 [16896/50000]\tLoss: 0.1104\tLR: 0.000800\n",
            "Training Epoch: 196 [17024/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 196 [17152/50000]\tLoss: 0.1398\tLR: 0.000800\n",
            "Training Epoch: 196 [17280/50000]\tLoss: 0.1346\tLR: 0.000800\n",
            "Training Epoch: 196 [17408/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 196 [17536/50000]\tLoss: 0.1562\tLR: 0.000800\n",
            "Training Epoch: 196 [17664/50000]\tLoss: 0.2165\tLR: 0.000800\n",
            "Training Epoch: 196 [17792/50000]\tLoss: 0.1256\tLR: 0.000800\n",
            "Training Epoch: 196 [17920/50000]\tLoss: 0.1315\tLR: 0.000800\n",
            "Training Epoch: 196 [18048/50000]\tLoss: 0.1019\tLR: 0.000800\n",
            "Training Epoch: 196 [18176/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 196 [18304/50000]\tLoss: 0.2195\tLR: 0.000800\n",
            "Training Epoch: 196 [18432/50000]\tLoss: 0.0989\tLR: 0.000800\n",
            "Training Epoch: 196 [18560/50000]\tLoss: 0.1575\tLR: 0.000800\n",
            "Training Epoch: 196 [18688/50000]\tLoss: 0.1656\tLR: 0.000800\n",
            "Training Epoch: 196 [18816/50000]\tLoss: 0.1903\tLR: 0.000800\n",
            "Training Epoch: 196 [18944/50000]\tLoss: 0.1563\tLR: 0.000800\n",
            "Training Epoch: 196 [19072/50000]\tLoss: 0.1520\tLR: 0.000800\n",
            "Training Epoch: 196 [19200/50000]\tLoss: 0.2030\tLR: 0.000800\n",
            "Training Epoch: 196 [19328/50000]\tLoss: 0.1309\tLR: 0.000800\n",
            "Training Epoch: 196 [19456/50000]\tLoss: 0.2145\tLR: 0.000800\n",
            "Training Epoch: 196 [19584/50000]\tLoss: 0.1442\tLR: 0.000800\n",
            "Training Epoch: 196 [19712/50000]\tLoss: 0.2447\tLR: 0.000800\n",
            "Training Epoch: 196 [19840/50000]\tLoss: 0.1247\tLR: 0.000800\n",
            "Training Epoch: 196 [19968/50000]\tLoss: 0.1625\tLR: 0.000800\n",
            "Training Epoch: 196 [20096/50000]\tLoss: 0.1572\tLR: 0.000800\n",
            "Training Epoch: 196 [20224/50000]\tLoss: 0.1597\tLR: 0.000800\n",
            "Training Epoch: 196 [20352/50000]\tLoss: 0.1386\tLR: 0.000800\n",
            "Training Epoch: 196 [20480/50000]\tLoss: 0.1504\tLR: 0.000800\n",
            "Training Epoch: 196 [20608/50000]\tLoss: 0.0910\tLR: 0.000800\n",
            "Training Epoch: 196 [20736/50000]\tLoss: 0.1705\tLR: 0.000800\n",
            "Training Epoch: 196 [20864/50000]\tLoss: 0.1080\tLR: 0.000800\n",
            "Training Epoch: 196 [20992/50000]\tLoss: 0.1050\tLR: 0.000800\n",
            "Training Epoch: 196 [21120/50000]\tLoss: 0.1944\tLR: 0.000800\n",
            "Training Epoch: 196 [21248/50000]\tLoss: 0.1161\tLR: 0.000800\n",
            "Training Epoch: 196 [21376/50000]\tLoss: 0.1009\tLR: 0.000800\n",
            "Training Epoch: 196 [21504/50000]\tLoss: 0.1397\tLR: 0.000800\n",
            "Training Epoch: 196 [21632/50000]\tLoss: 0.1507\tLR: 0.000800\n",
            "Training Epoch: 196 [21760/50000]\tLoss: 0.0954\tLR: 0.000800\n",
            "Training Epoch: 196 [21888/50000]\tLoss: 0.1666\tLR: 0.000800\n",
            "Training Epoch: 196 [22016/50000]\tLoss: 0.1303\tLR: 0.000800\n",
            "Training Epoch: 196 [22144/50000]\tLoss: 0.1307\tLR: 0.000800\n",
            "Training Epoch: 196 [22272/50000]\tLoss: 0.1877\tLR: 0.000800\n",
            "Training Epoch: 196 [22400/50000]\tLoss: 0.0707\tLR: 0.000800\n",
            "Training Epoch: 196 [22528/50000]\tLoss: 0.1093\tLR: 0.000800\n",
            "Training Epoch: 196 [22656/50000]\tLoss: 0.1202\tLR: 0.000800\n",
            "Training Epoch: 196 [22784/50000]\tLoss: 0.1289\tLR: 0.000800\n",
            "Training Epoch: 196 [22912/50000]\tLoss: 0.1745\tLR: 0.000800\n",
            "Training Epoch: 196 [23040/50000]\tLoss: 0.1752\tLR: 0.000800\n",
            "Training Epoch: 196 [23168/50000]\tLoss: 0.1176\tLR: 0.000800\n",
            "Training Epoch: 196 [23296/50000]\tLoss: 0.1390\tLR: 0.000800\n",
            "Training Epoch: 196 [23424/50000]\tLoss: 0.1386\tLR: 0.000800\n",
            "Training Epoch: 196 [23552/50000]\tLoss: 0.1123\tLR: 0.000800\n",
            "Training Epoch: 196 [23680/50000]\tLoss: 0.1288\tLR: 0.000800\n",
            "Training Epoch: 196 [23808/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 196 [23936/50000]\tLoss: 0.0941\tLR: 0.000800\n",
            "Training Epoch: 196 [24064/50000]\tLoss: 0.1293\tLR: 0.000800\n",
            "Training Epoch: 196 [24192/50000]\tLoss: 0.1454\tLR: 0.000800\n",
            "Training Epoch: 196 [24320/50000]\tLoss: 0.1744\tLR: 0.000800\n",
            "Training Epoch: 196 [24448/50000]\tLoss: 0.1466\tLR: 0.000800\n",
            "Training Epoch: 196 [24576/50000]\tLoss: 0.1655\tLR: 0.000800\n",
            "Training Epoch: 196 [24704/50000]\tLoss: 0.2029\tLR: 0.000800\n",
            "Training Epoch: 196 [24832/50000]\tLoss: 0.1291\tLR: 0.000800\n",
            "Training Epoch: 196 [24960/50000]\tLoss: 0.1164\tLR: 0.000800\n",
            "Training Epoch: 196 [25088/50000]\tLoss: 0.1679\tLR: 0.000800\n",
            "Training Epoch: 196 [25216/50000]\tLoss: 0.2017\tLR: 0.000800\n",
            "Training Epoch: 196 [25344/50000]\tLoss: 0.1435\tLR: 0.000800\n",
            "Training Epoch: 196 [25472/50000]\tLoss: 0.1272\tLR: 0.000800\n",
            "Training Epoch: 196 [25600/50000]\tLoss: 0.1532\tLR: 0.000800\n",
            "Training Epoch: 196 [25728/50000]\tLoss: 0.0801\tLR: 0.000800\n",
            "Training Epoch: 196 [25856/50000]\tLoss: 0.1273\tLR: 0.000800\n",
            "Training Epoch: 196 [25984/50000]\tLoss: 0.1560\tLR: 0.000800\n",
            "Training Epoch: 196 [26112/50000]\tLoss: 0.1173\tLR: 0.000800\n",
            "Training Epoch: 196 [26240/50000]\tLoss: 0.1828\tLR: 0.000800\n",
            "Training Epoch: 196 [26368/50000]\tLoss: 0.1541\tLR: 0.000800\n",
            "Training Epoch: 196 [26496/50000]\tLoss: 0.1165\tLR: 0.000800\n",
            "Training Epoch: 196 [26624/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 196 [26752/50000]\tLoss: 0.1291\tLR: 0.000800\n",
            "Training Epoch: 196 [26880/50000]\tLoss: 0.0926\tLR: 0.000800\n",
            "Training Epoch: 196 [27008/50000]\tLoss: 0.1093\tLR: 0.000800\n",
            "Training Epoch: 196 [27136/50000]\tLoss: 0.1061\tLR: 0.000800\n",
            "Training Epoch: 196 [27264/50000]\tLoss: 0.1473\tLR: 0.000800\n",
            "Training Epoch: 196 [27392/50000]\tLoss: 0.2130\tLR: 0.000800\n",
            "Training Epoch: 196 [27520/50000]\tLoss: 0.1511\tLR: 0.000800\n",
            "Training Epoch: 196 [27648/50000]\tLoss: 0.1159\tLR: 0.000800\n",
            "Training Epoch: 196 [27776/50000]\tLoss: 0.0926\tLR: 0.000800\n",
            "Training Epoch: 196 [27904/50000]\tLoss: 0.1500\tLR: 0.000800\n",
            "Training Epoch: 196 [28032/50000]\tLoss: 0.1776\tLR: 0.000800\n",
            "Training Epoch: 196 [28160/50000]\tLoss: 0.1006\tLR: 0.000800\n",
            "Training Epoch: 196 [28288/50000]\tLoss: 0.1152\tLR: 0.000800\n",
            "Training Epoch: 196 [28416/50000]\tLoss: 0.1301\tLR: 0.000800\n",
            "Training Epoch: 196 [28544/50000]\tLoss: 0.1143\tLR: 0.000800\n",
            "Training Epoch: 196 [28672/50000]\tLoss: 0.1358\tLR: 0.000800\n",
            "Training Epoch: 196 [28800/50000]\tLoss: 0.1029\tLR: 0.000800\n",
            "Training Epoch: 196 [28928/50000]\tLoss: 0.1198\tLR: 0.000800\n",
            "Training Epoch: 196 [29056/50000]\tLoss: 0.1545\tLR: 0.000800\n",
            "Training Epoch: 196 [29184/50000]\tLoss: 0.1726\tLR: 0.000800\n",
            "Training Epoch: 196 [29312/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 196 [29440/50000]\tLoss: 0.1561\tLR: 0.000800\n",
            "Training Epoch: 196 [29568/50000]\tLoss: 0.1238\tLR: 0.000800\n",
            "Training Epoch: 196 [29696/50000]\tLoss: 0.1307\tLR: 0.000800\n",
            "Training Epoch: 196 [29824/50000]\tLoss: 0.1596\tLR: 0.000800\n",
            "Training Epoch: 196 [29952/50000]\tLoss: 0.2086\tLR: 0.000800\n",
            "Training Epoch: 196 [30080/50000]\tLoss: 0.1237\tLR: 0.000800\n",
            "Training Epoch: 196 [30208/50000]\tLoss: 0.1232\tLR: 0.000800\n",
            "Training Epoch: 196 [30336/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 196 [30464/50000]\tLoss: 0.1519\tLR: 0.000800\n",
            "Training Epoch: 196 [30592/50000]\tLoss: 0.1908\tLR: 0.000800\n",
            "Training Epoch: 196 [30720/50000]\tLoss: 0.1555\tLR: 0.000800\n",
            "Training Epoch: 196 [30848/50000]\tLoss: 0.1971\tLR: 0.000800\n",
            "Training Epoch: 196 [30976/50000]\tLoss: 0.1204\tLR: 0.000800\n",
            "Training Epoch: 196 [31104/50000]\tLoss: 0.2088\tLR: 0.000800\n",
            "Training Epoch: 196 [31232/50000]\tLoss: 0.2077\tLR: 0.000800\n",
            "Training Epoch: 196 [31360/50000]\tLoss: 0.0949\tLR: 0.000800\n",
            "Training Epoch: 196 [31488/50000]\tLoss: 0.1061\tLR: 0.000800\n",
            "Training Epoch: 196 [31616/50000]\tLoss: 0.1315\tLR: 0.000800\n",
            "Training Epoch: 196 [31744/50000]\tLoss: 0.1337\tLR: 0.000800\n",
            "Training Epoch: 196 [31872/50000]\tLoss: 0.1242\tLR: 0.000800\n",
            "Training Epoch: 196 [32000/50000]\tLoss: 0.1685\tLR: 0.000800\n",
            "Training Epoch: 196 [32128/50000]\tLoss: 0.1859\tLR: 0.000800\n",
            "Training Epoch: 196 [32256/50000]\tLoss: 0.1370\tLR: 0.000800\n",
            "Training Epoch: 196 [32384/50000]\tLoss: 0.0985\tLR: 0.000800\n",
            "Training Epoch: 196 [32512/50000]\tLoss: 0.1285\tLR: 0.000800\n",
            "Training Epoch: 196 [32640/50000]\tLoss: 0.0874\tLR: 0.000800\n",
            "Training Epoch: 196 [32768/50000]\tLoss: 0.1069\tLR: 0.000800\n",
            "Training Epoch: 196 [32896/50000]\tLoss: 0.1204\tLR: 0.000800\n",
            "Training Epoch: 196 [33024/50000]\tLoss: 0.1441\tLR: 0.000800\n",
            "Training Epoch: 196 [33152/50000]\tLoss: 0.1833\tLR: 0.000800\n",
            "Training Epoch: 196 [33280/50000]\tLoss: 0.1311\tLR: 0.000800\n",
            "Training Epoch: 196 [33408/50000]\tLoss: 0.1435\tLR: 0.000800\n",
            "Training Epoch: 196 [33536/50000]\tLoss: 0.1525\tLR: 0.000800\n",
            "Training Epoch: 196 [33664/50000]\tLoss: 0.1089\tLR: 0.000800\n",
            "Training Epoch: 196 [33792/50000]\tLoss: 0.1746\tLR: 0.000800\n",
            "Training Epoch: 196 [33920/50000]\tLoss: 0.1133\tLR: 0.000800\n",
            "Training Epoch: 196 [34048/50000]\tLoss: 0.1463\tLR: 0.000800\n",
            "Training Epoch: 196 [34176/50000]\tLoss: 0.1366\tLR: 0.000800\n",
            "Training Epoch: 196 [34304/50000]\tLoss: 0.1534\tLR: 0.000800\n",
            "Training Epoch: 196 [34432/50000]\tLoss: 0.1654\tLR: 0.000800\n",
            "Training Epoch: 196 [34560/50000]\tLoss: 0.1772\tLR: 0.000800\n",
            "Training Epoch: 196 [34688/50000]\tLoss: 0.1310\tLR: 0.000800\n",
            "Training Epoch: 196 [34816/50000]\tLoss: 0.2069\tLR: 0.000800\n",
            "Training Epoch: 196 [34944/50000]\tLoss: 0.1425\tLR: 0.000800\n",
            "Training Epoch: 196 [35072/50000]\tLoss: 0.1606\tLR: 0.000800\n",
            "Training Epoch: 196 [35200/50000]\tLoss: 0.1164\tLR: 0.000800\n",
            "Training Epoch: 196 [35328/50000]\tLoss: 0.0839\tLR: 0.000800\n",
            "Training Epoch: 196 [35456/50000]\tLoss: 0.1334\tLR: 0.000800\n",
            "Training Epoch: 196 [35584/50000]\tLoss: 0.1150\tLR: 0.000800\n",
            "Training Epoch: 196 [35712/50000]\tLoss: 0.1309\tLR: 0.000800\n",
            "Training Epoch: 196 [35840/50000]\tLoss: 0.1311\tLR: 0.000800\n",
            "Training Epoch: 196 [35968/50000]\tLoss: 0.1439\tLR: 0.000800\n",
            "Training Epoch: 196 [36096/50000]\tLoss: 0.1111\tLR: 0.000800\n",
            "Training Epoch: 196 [36224/50000]\tLoss: 0.1429\tLR: 0.000800\n",
            "Training Epoch: 196 [36352/50000]\tLoss: 0.1748\tLR: 0.000800\n",
            "Training Epoch: 196 [36480/50000]\tLoss: 0.1491\tLR: 0.000800\n",
            "Training Epoch: 196 [36608/50000]\tLoss: 0.1329\tLR: 0.000800\n",
            "Training Epoch: 196 [36736/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 196 [36864/50000]\tLoss: 0.1850\tLR: 0.000800\n",
            "Training Epoch: 196 [36992/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 196 [37120/50000]\tLoss: 0.1898\tLR: 0.000800\n",
            "Training Epoch: 196 [37248/50000]\tLoss: 0.1179\tLR: 0.000800\n",
            "Training Epoch: 196 [37376/50000]\tLoss: 0.1449\tLR: 0.000800\n",
            "Training Epoch: 196 [37504/50000]\tLoss: 0.1485\tLR: 0.000800\n",
            "Training Epoch: 196 [37632/50000]\tLoss: 0.1183\tLR: 0.000800\n",
            "Training Epoch: 196 [37760/50000]\tLoss: 0.1216\tLR: 0.000800\n",
            "Training Epoch: 196 [37888/50000]\tLoss: 0.1656\tLR: 0.000800\n",
            "Training Epoch: 196 [38016/50000]\tLoss: 0.1404\tLR: 0.000800\n",
            "Training Epoch: 196 [38144/50000]\tLoss: 0.1422\tLR: 0.000800\n",
            "Training Epoch: 196 [38272/50000]\tLoss: 0.2067\tLR: 0.000800\n",
            "Training Epoch: 196 [38400/50000]\tLoss: 0.1408\tLR: 0.000800\n",
            "Training Epoch: 196 [38528/50000]\tLoss: 0.1176\tLR: 0.000800\n",
            "Training Epoch: 196 [38656/50000]\tLoss: 0.1599\tLR: 0.000800\n",
            "Training Epoch: 196 [38784/50000]\tLoss: 0.1783\tLR: 0.000800\n",
            "Training Epoch: 196 [38912/50000]\tLoss: 0.1128\tLR: 0.000800\n",
            "Training Epoch: 196 [39040/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 196 [39168/50000]\tLoss: 0.1318\tLR: 0.000800\n",
            "Training Epoch: 196 [39296/50000]\tLoss: 0.1426\tLR: 0.000800\n",
            "Training Epoch: 196 [39424/50000]\tLoss: 0.1550\tLR: 0.000800\n",
            "Training Epoch: 196 [39552/50000]\tLoss: 0.1303\tLR: 0.000800\n",
            "Training Epoch: 196 [39680/50000]\tLoss: 0.1472\tLR: 0.000800\n",
            "Training Epoch: 196 [39808/50000]\tLoss: 0.1192\tLR: 0.000800\n",
            "Training Epoch: 196 [39936/50000]\tLoss: 0.1247\tLR: 0.000800\n",
            "Training Epoch: 196 [40064/50000]\tLoss: 0.1520\tLR: 0.000800\n",
            "Training Epoch: 196 [40192/50000]\tLoss: 0.1637\tLR: 0.000800\n",
            "Training Epoch: 196 [40320/50000]\tLoss: 0.1820\tLR: 0.000800\n",
            "Training Epoch: 196 [40448/50000]\tLoss: 0.1219\tLR: 0.000800\n",
            "Training Epoch: 196 [40576/50000]\tLoss: 0.0976\tLR: 0.000800\n",
            "Training Epoch: 196 [40704/50000]\tLoss: 0.1741\tLR: 0.000800\n",
            "Training Epoch: 196 [40832/50000]\tLoss: 0.1078\tLR: 0.000800\n",
            "Training Epoch: 196 [40960/50000]\tLoss: 0.1405\tLR: 0.000800\n",
            "Training Epoch: 196 [41088/50000]\tLoss: 0.1455\tLR: 0.000800\n",
            "Training Epoch: 196 [41216/50000]\tLoss: 0.1969\tLR: 0.000800\n",
            "Training Epoch: 196 [41344/50000]\tLoss: 0.1639\tLR: 0.000800\n",
            "Training Epoch: 196 [41472/50000]\tLoss: 0.2180\tLR: 0.000800\n",
            "Training Epoch: 196 [41600/50000]\tLoss: 0.1632\tLR: 0.000800\n",
            "Training Epoch: 196 [41728/50000]\tLoss: 0.1320\tLR: 0.000800\n",
            "Training Epoch: 196 [41856/50000]\tLoss: 0.1514\tLR: 0.000800\n",
            "Training Epoch: 196 [41984/50000]\tLoss: 0.2139\tLR: 0.000800\n",
            "Training Epoch: 196 [42112/50000]\tLoss: 0.1431\tLR: 0.000800\n",
            "Training Epoch: 196 [42240/50000]\tLoss: 0.1367\tLR: 0.000800\n",
            "Training Epoch: 196 [42368/50000]\tLoss: 0.1959\tLR: 0.000800\n",
            "Training Epoch: 196 [42496/50000]\tLoss: 0.1397\tLR: 0.000800\n",
            "Training Epoch: 196 [42624/50000]\tLoss: 0.1288\tLR: 0.000800\n",
            "Training Epoch: 196 [42752/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 196 [42880/50000]\tLoss: 0.1465\tLR: 0.000800\n",
            "Training Epoch: 196 [43008/50000]\tLoss: 0.1327\tLR: 0.000800\n",
            "Training Epoch: 196 [43136/50000]\tLoss: 0.1737\tLR: 0.000800\n",
            "Training Epoch: 196 [43264/50000]\tLoss: 0.1188\tLR: 0.000800\n",
            "Training Epoch: 196 [43392/50000]\tLoss: 0.1510\tLR: 0.000800\n",
            "Training Epoch: 196 [43520/50000]\tLoss: 0.1459\tLR: 0.000800\n",
            "Training Epoch: 196 [43648/50000]\tLoss: 0.1669\tLR: 0.000800\n",
            "Training Epoch: 196 [43776/50000]\tLoss: 0.0960\tLR: 0.000800\n",
            "Training Epoch: 196 [43904/50000]\tLoss: 0.1861\tLR: 0.000800\n",
            "Training Epoch: 196 [44032/50000]\tLoss: 0.1241\tLR: 0.000800\n",
            "Training Epoch: 196 [44160/50000]\tLoss: 0.1275\tLR: 0.000800\n",
            "Training Epoch: 196 [44288/50000]\tLoss: 0.0917\tLR: 0.000800\n",
            "Training Epoch: 196 [44416/50000]\tLoss: 0.1540\tLR: 0.000800\n",
            "Training Epoch: 196 [44544/50000]\tLoss: 0.1748\tLR: 0.000800\n",
            "Training Epoch: 196 [44672/50000]\tLoss: 0.1547\tLR: 0.000800\n",
            "Training Epoch: 196 [44800/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 196 [44928/50000]\tLoss: 0.1595\tLR: 0.000800\n",
            "Training Epoch: 196 [45056/50000]\tLoss: 0.1084\tLR: 0.000800\n",
            "Training Epoch: 196 [45184/50000]\tLoss: 0.0986\tLR: 0.000800\n",
            "Training Epoch: 196 [45312/50000]\tLoss: 0.1811\tLR: 0.000800\n",
            "Training Epoch: 196 [45440/50000]\tLoss: 0.1312\tLR: 0.000800\n",
            "Training Epoch: 196 [45568/50000]\tLoss: 0.1460\tLR: 0.000800\n",
            "Training Epoch: 196 [45696/50000]\tLoss: 0.0835\tLR: 0.000800\n",
            "Training Epoch: 196 [45824/50000]\tLoss: 0.0928\tLR: 0.000800\n",
            "Training Epoch: 196 [45952/50000]\tLoss: 0.0857\tLR: 0.000800\n",
            "Training Epoch: 196 [46080/50000]\tLoss: 0.1491\tLR: 0.000800\n",
            "Training Epoch: 196 [46208/50000]\tLoss: 0.1006\tLR: 0.000800\n",
            "Training Epoch: 196 [46336/50000]\tLoss: 0.1710\tLR: 0.000800\n",
            "Training Epoch: 196 [46464/50000]\tLoss: 0.1316\tLR: 0.000800\n",
            "Training Epoch: 196 [46592/50000]\tLoss: 0.1465\tLR: 0.000800\n",
            "Training Epoch: 196 [46720/50000]\tLoss: 0.1786\tLR: 0.000800\n",
            "Training Epoch: 196 [46848/50000]\tLoss: 0.1705\tLR: 0.000800\n",
            "Training Epoch: 196 [46976/50000]\tLoss: 0.1249\tLR: 0.000800\n",
            "Training Epoch: 196 [47104/50000]\tLoss: 0.1788\tLR: 0.000800\n",
            "Training Epoch: 196 [47232/50000]\tLoss: 0.1538\tLR: 0.000800\n",
            "Training Epoch: 196 [47360/50000]\tLoss: 0.1626\tLR: 0.000800\n",
            "Training Epoch: 196 [47488/50000]\tLoss: 0.1191\tLR: 0.000800\n",
            "Training Epoch: 196 [47616/50000]\tLoss: 0.1246\tLR: 0.000800\n",
            "Training Epoch: 196 [47744/50000]\tLoss: 0.1699\tLR: 0.000800\n",
            "Training Epoch: 196 [47872/50000]\tLoss: 0.1311\tLR: 0.000800\n",
            "Training Epoch: 196 [48000/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 196 [48128/50000]\tLoss: 0.1413\tLR: 0.000800\n",
            "Training Epoch: 196 [48256/50000]\tLoss: 0.1127\tLR: 0.000800\n",
            "Training Epoch: 196 [48384/50000]\tLoss: 0.1663\tLR: 0.000800\n",
            "Training Epoch: 196 [48512/50000]\tLoss: 0.1614\tLR: 0.000800\n",
            "Training Epoch: 196 [48640/50000]\tLoss: 0.2029\tLR: 0.000800\n",
            "Training Epoch: 196 [48768/50000]\tLoss: 0.2103\tLR: 0.000800\n",
            "Training Epoch: 196 [48896/50000]\tLoss: 0.1537\tLR: 0.000800\n",
            "Training Epoch: 196 [49024/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 196 [49152/50000]\tLoss: 0.0851\tLR: 0.000800\n",
            "Training Epoch: 196 [49280/50000]\tLoss: 0.1306\tLR: 0.000800\n",
            "Training Epoch: 196 [49408/50000]\tLoss: 0.1578\tLR: 0.000800\n",
            "Training Epoch: 196 [49536/50000]\tLoss: 0.1183\tLR: 0.000800\n",
            "Training Epoch: 196 [49664/50000]\tLoss: 0.0944\tLR: 0.000800\n",
            "Training Epoch: 196 [49792/50000]\tLoss: 0.1219\tLR: 0.000800\n",
            "Training Epoch: 196 [49920/50000]\tLoss: 0.1310\tLR: 0.000800\n",
            "Training Epoch: 196 [50000/50000]\tLoss: 0.1404\tLR: 0.000800\n",
            "epoch 196 training time consumed: 28.52s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  88204 GiB |  88204 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  86525 GiB |  86525 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1679 GiB |   1679 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  88204 GiB |  88204 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  86525 GiB |  86525 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1679 GiB |   1679 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  87942 GiB |  87942 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  86264 GiB |  86264 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1677 GiB |   1677 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  53963 GiB |  53963 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  52229 GiB |  52229 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1733 GiB |   1733 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   34177 K  |   34177 K  |\n",
            "|       from large pool |       8    |      61    |   11465 K  |   11465 K  |\n",
            "|       from small pool |     373    |     464    |   22711 K  |   22711 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   34177 K  |   34177 K  |\n",
            "|       from large pool |       8    |      61    |   11465 K  |   11465 K  |\n",
            "|       from small pool |     373    |     464    |   22711 K  |   22711 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   11236 K  |   11236 K  |\n",
            "|       from large pool |       5    |      14    |    5466 K  |    5466 K  |\n",
            "|       from small pool |      10    |      16    |    5769 K  |    5769 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 196, Average loss: 0.0113, Accuracy: 0.6690, Time consumed:3.99s\n",
            "\n",
            "Training Epoch: 197 [128/50000]\tLoss: 0.1021\tLR: 0.000800\n",
            "Training Epoch: 197 [256/50000]\tLoss: 0.1820\tLR: 0.000800\n",
            "Training Epoch: 197 [384/50000]\tLoss: 0.1869\tLR: 0.000800\n",
            "Training Epoch: 197 [512/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 197 [640/50000]\tLoss: 0.0970\tLR: 0.000800\n",
            "Training Epoch: 197 [768/50000]\tLoss: 0.1017\tLR: 0.000800\n",
            "Training Epoch: 197 [896/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 197 [1024/50000]\tLoss: 0.1669\tLR: 0.000800\n",
            "Training Epoch: 197 [1152/50000]\tLoss: 0.1753\tLR: 0.000800\n",
            "Training Epoch: 197 [1280/50000]\tLoss: 0.1520\tLR: 0.000800\n",
            "Training Epoch: 197 [1408/50000]\tLoss: 0.1540\tLR: 0.000800\n",
            "Training Epoch: 197 [1536/50000]\tLoss: 0.0855\tLR: 0.000800\n",
            "Training Epoch: 197 [1664/50000]\tLoss: 0.2383\tLR: 0.000800\n",
            "Training Epoch: 197 [1792/50000]\tLoss: 0.1167\tLR: 0.000800\n",
            "Training Epoch: 197 [1920/50000]\tLoss: 0.0760\tLR: 0.000800\n",
            "Training Epoch: 197 [2048/50000]\tLoss: 0.1418\tLR: 0.000800\n",
            "Training Epoch: 197 [2176/50000]\tLoss: 0.1204\tLR: 0.000800\n",
            "Training Epoch: 197 [2304/50000]\tLoss: 0.2376\tLR: 0.000800\n",
            "Training Epoch: 197 [2432/50000]\tLoss: 0.1483\tLR: 0.000800\n",
            "Training Epoch: 197 [2560/50000]\tLoss: 0.1534\tLR: 0.000800\n",
            "Training Epoch: 197 [2688/50000]\tLoss: 0.1213\tLR: 0.000800\n",
            "Training Epoch: 197 [2816/50000]\tLoss: 0.1408\tLR: 0.000800\n",
            "Training Epoch: 197 [2944/50000]\tLoss: 0.1345\tLR: 0.000800\n",
            "Training Epoch: 197 [3072/50000]\tLoss: 0.1657\tLR: 0.000800\n",
            "Training Epoch: 197 [3200/50000]\tLoss: 0.1346\tLR: 0.000800\n",
            "Training Epoch: 197 [3328/50000]\tLoss: 0.1063\tLR: 0.000800\n",
            "Training Epoch: 197 [3456/50000]\tLoss: 0.1022\tLR: 0.000800\n",
            "Training Epoch: 197 [3584/50000]\tLoss: 0.0938\tLR: 0.000800\n",
            "Training Epoch: 197 [3712/50000]\tLoss: 0.2138\tLR: 0.000800\n",
            "Training Epoch: 197 [3840/50000]\tLoss: 0.1110\tLR: 0.000800\n",
            "Training Epoch: 197 [3968/50000]\tLoss: 0.1003\tLR: 0.000800\n",
            "Training Epoch: 197 [4096/50000]\tLoss: 0.1565\tLR: 0.000800\n",
            "Training Epoch: 197 [4224/50000]\tLoss: 0.1359\tLR: 0.000800\n",
            "Training Epoch: 197 [4352/50000]\tLoss: 0.1379\tLR: 0.000800\n",
            "Training Epoch: 197 [4480/50000]\tLoss: 0.1350\tLR: 0.000800\n",
            "Training Epoch: 197 [4608/50000]\tLoss: 0.1326\tLR: 0.000800\n",
            "Training Epoch: 197 [4736/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 197 [4864/50000]\tLoss: 0.1427\tLR: 0.000800\n",
            "Training Epoch: 197 [4992/50000]\tLoss: 0.1088\tLR: 0.000800\n",
            "Training Epoch: 197 [5120/50000]\tLoss: 0.0697\tLR: 0.000800\n",
            "Training Epoch: 197 [5248/50000]\tLoss: 0.0901\tLR: 0.000800\n",
            "Training Epoch: 197 [5376/50000]\tLoss: 0.1542\tLR: 0.000800\n",
            "Training Epoch: 197 [5504/50000]\tLoss: 0.1449\tLR: 0.000800\n",
            "Training Epoch: 197 [5632/50000]\tLoss: 0.1952\tLR: 0.000800\n",
            "Training Epoch: 197 [5760/50000]\tLoss: 0.2006\tLR: 0.000800\n",
            "Training Epoch: 197 [5888/50000]\tLoss: 0.1827\tLR: 0.000800\n",
            "Training Epoch: 197 [6016/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 197 [6144/50000]\tLoss: 0.1343\tLR: 0.000800\n",
            "Training Epoch: 197 [6272/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 197 [6400/50000]\tLoss: 0.1924\tLR: 0.000800\n",
            "Training Epoch: 197 [6528/50000]\tLoss: 0.1189\tLR: 0.000800\n",
            "Training Epoch: 197 [6656/50000]\tLoss: 0.1421\tLR: 0.000800\n",
            "Training Epoch: 197 [6784/50000]\tLoss: 0.1484\tLR: 0.000800\n",
            "Training Epoch: 197 [6912/50000]\tLoss: 0.1010\tLR: 0.000800\n",
            "Training Epoch: 197 [7040/50000]\tLoss: 0.1053\tLR: 0.000800\n",
            "Training Epoch: 197 [7168/50000]\tLoss: 0.1247\tLR: 0.000800\n",
            "Training Epoch: 197 [7296/50000]\tLoss: 0.1745\tLR: 0.000800\n",
            "Training Epoch: 197 [7424/50000]\tLoss: 0.1503\tLR: 0.000800\n",
            "Training Epoch: 197 [7552/50000]\tLoss: 0.1385\tLR: 0.000800\n",
            "Training Epoch: 197 [7680/50000]\tLoss: 0.1007\tLR: 0.000800\n",
            "Training Epoch: 197 [7808/50000]\tLoss: 0.1403\tLR: 0.000800\n",
            "Training Epoch: 197 [7936/50000]\tLoss: 0.1903\tLR: 0.000800\n",
            "Training Epoch: 197 [8064/50000]\tLoss: 0.0808\tLR: 0.000800\n",
            "Training Epoch: 197 [8192/50000]\tLoss: 0.1060\tLR: 0.000800\n",
            "Training Epoch: 197 [8320/50000]\tLoss: 0.1194\tLR: 0.000800\n",
            "Training Epoch: 197 [8448/50000]\tLoss: 0.1825\tLR: 0.000800\n",
            "Training Epoch: 197 [8576/50000]\tLoss: 0.0921\tLR: 0.000800\n",
            "Training Epoch: 197 [8704/50000]\tLoss: 0.1701\tLR: 0.000800\n",
            "Training Epoch: 197 [8832/50000]\tLoss: 0.1596\tLR: 0.000800\n",
            "Training Epoch: 197 [8960/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 197 [9088/50000]\tLoss: 0.1514\tLR: 0.000800\n",
            "Training Epoch: 197 [9216/50000]\tLoss: 0.0893\tLR: 0.000800\n",
            "Training Epoch: 197 [9344/50000]\tLoss: 0.1874\tLR: 0.000800\n",
            "Training Epoch: 197 [9472/50000]\tLoss: 0.2184\tLR: 0.000800\n",
            "Training Epoch: 197 [9600/50000]\tLoss: 0.1156\tLR: 0.000800\n",
            "Training Epoch: 197 [9728/50000]\tLoss: 0.2701\tLR: 0.000800\n",
            "Training Epoch: 197 [9856/50000]\tLoss: 0.1309\tLR: 0.000800\n",
            "Training Epoch: 197 [9984/50000]\tLoss: 0.1278\tLR: 0.000800\n",
            "Training Epoch: 197 [10112/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 197 [10240/50000]\tLoss: 0.0951\tLR: 0.000800\n",
            "Training Epoch: 197 [10368/50000]\tLoss: 0.1946\tLR: 0.000800\n",
            "Training Epoch: 197 [10496/50000]\tLoss: 0.1278\tLR: 0.000800\n",
            "Training Epoch: 197 [10624/50000]\tLoss: 0.1874\tLR: 0.000800\n",
            "Training Epoch: 197 [10752/50000]\tLoss: 0.0585\tLR: 0.000800\n",
            "Training Epoch: 197 [10880/50000]\tLoss: 0.1096\tLR: 0.000800\n",
            "Training Epoch: 197 [11008/50000]\tLoss: 0.1518\tLR: 0.000800\n",
            "Training Epoch: 197 [11136/50000]\tLoss: 0.1447\tLR: 0.000800\n",
            "Training Epoch: 197 [11264/50000]\tLoss: 0.1032\tLR: 0.000800\n",
            "Training Epoch: 197 [11392/50000]\tLoss: 0.1036\tLR: 0.000800\n",
            "Training Epoch: 197 [11520/50000]\tLoss: 0.2033\tLR: 0.000800\n",
            "Training Epoch: 197 [11648/50000]\tLoss: 0.1417\tLR: 0.000800\n",
            "Training Epoch: 197 [11776/50000]\tLoss: 0.1528\tLR: 0.000800\n",
            "Training Epoch: 197 [11904/50000]\tLoss: 0.1586\tLR: 0.000800\n",
            "Training Epoch: 197 [12032/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 197 [12160/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 197 [12288/50000]\tLoss: 0.1147\tLR: 0.000800\n",
            "Training Epoch: 197 [12416/50000]\tLoss: 0.1241\tLR: 0.000800\n",
            "Training Epoch: 197 [12544/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 197 [12672/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 197 [12800/50000]\tLoss: 0.0807\tLR: 0.000800\n",
            "Training Epoch: 197 [12928/50000]\tLoss: 0.1737\tLR: 0.000800\n",
            "Training Epoch: 197 [13056/50000]\tLoss: 0.1292\tLR: 0.000800\n",
            "Training Epoch: 197 [13184/50000]\tLoss: 0.1003\tLR: 0.000800\n",
            "Training Epoch: 197 [13312/50000]\tLoss: 0.1324\tLR: 0.000800\n",
            "Training Epoch: 197 [13440/50000]\tLoss: 0.1224\tLR: 0.000800\n",
            "Training Epoch: 197 [13568/50000]\tLoss: 0.1147\tLR: 0.000800\n",
            "Training Epoch: 197 [13696/50000]\tLoss: 0.0828\tLR: 0.000800\n",
            "Training Epoch: 197 [13824/50000]\tLoss: 0.1578\tLR: 0.000800\n",
            "Training Epoch: 197 [13952/50000]\tLoss: 0.1026\tLR: 0.000800\n",
            "Training Epoch: 197 [14080/50000]\tLoss: 0.1647\tLR: 0.000800\n",
            "Training Epoch: 197 [14208/50000]\tLoss: 0.1221\tLR: 0.000800\n",
            "Training Epoch: 197 [14336/50000]\tLoss: 0.1666\tLR: 0.000800\n",
            "Training Epoch: 197 [14464/50000]\tLoss: 0.1352\tLR: 0.000800\n",
            "Training Epoch: 197 [14592/50000]\tLoss: 0.1470\tLR: 0.000800\n",
            "Training Epoch: 197 [14720/50000]\tLoss: 0.2133\tLR: 0.000800\n",
            "Training Epoch: 197 [14848/50000]\tLoss: 0.1017\tLR: 0.000800\n",
            "Training Epoch: 197 [14976/50000]\tLoss: 0.1195\tLR: 0.000800\n",
            "Training Epoch: 197 [15104/50000]\tLoss: 0.1252\tLR: 0.000800\n",
            "Training Epoch: 197 [15232/50000]\tLoss: 0.1396\tLR: 0.000800\n",
            "Training Epoch: 197 [15360/50000]\tLoss: 0.1353\tLR: 0.000800\n",
            "Training Epoch: 197 [15488/50000]\tLoss: 0.1692\tLR: 0.000800\n",
            "Training Epoch: 197 [15616/50000]\tLoss: 0.1241\tLR: 0.000800\n",
            "Training Epoch: 197 [15744/50000]\tLoss: 0.1527\tLR: 0.000800\n",
            "Training Epoch: 197 [15872/50000]\tLoss: 0.1984\tLR: 0.000800\n",
            "Training Epoch: 197 [16000/50000]\tLoss: 0.1359\tLR: 0.000800\n",
            "Training Epoch: 197 [16128/50000]\tLoss: 0.1641\tLR: 0.000800\n",
            "Training Epoch: 197 [16256/50000]\tLoss: 0.1728\tLR: 0.000800\n",
            "Training Epoch: 197 [16384/50000]\tLoss: 0.1136\tLR: 0.000800\n",
            "Training Epoch: 197 [16512/50000]\tLoss: 0.1152\tLR: 0.000800\n",
            "Training Epoch: 197 [16640/50000]\tLoss: 0.1743\tLR: 0.000800\n",
            "Training Epoch: 197 [16768/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 197 [16896/50000]\tLoss: 0.1253\tLR: 0.000800\n",
            "Training Epoch: 197 [17024/50000]\tLoss: 0.1187\tLR: 0.000800\n",
            "Training Epoch: 197 [17152/50000]\tLoss: 0.0995\tLR: 0.000800\n",
            "Training Epoch: 197 [17280/50000]\tLoss: 0.1999\tLR: 0.000800\n",
            "Training Epoch: 197 [17408/50000]\tLoss: 0.1746\tLR: 0.000800\n",
            "Training Epoch: 197 [17536/50000]\tLoss: 0.0979\tLR: 0.000800\n",
            "Training Epoch: 197 [17664/50000]\tLoss: 0.1355\tLR: 0.000800\n",
            "Training Epoch: 197 [17792/50000]\tLoss: 0.1474\tLR: 0.000800\n",
            "Training Epoch: 197 [17920/50000]\tLoss: 0.2063\tLR: 0.000800\n",
            "Training Epoch: 197 [18048/50000]\tLoss: 0.1194\tLR: 0.000800\n",
            "Training Epoch: 197 [18176/50000]\tLoss: 0.1743\tLR: 0.000800\n",
            "Training Epoch: 197 [18304/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 197 [18432/50000]\tLoss: 0.1739\tLR: 0.000800\n",
            "Training Epoch: 197 [18560/50000]\tLoss: 0.1091\tLR: 0.000800\n",
            "Training Epoch: 197 [18688/50000]\tLoss: 0.1388\tLR: 0.000800\n",
            "Training Epoch: 197 [18816/50000]\tLoss: 0.1176\tLR: 0.000800\n",
            "Training Epoch: 197 [18944/50000]\tLoss: 0.1225\tLR: 0.000800\n",
            "Training Epoch: 197 [19072/50000]\tLoss: 0.1643\tLR: 0.000800\n",
            "Training Epoch: 197 [19200/50000]\tLoss: 0.0995\tLR: 0.000800\n",
            "Training Epoch: 197 [19328/50000]\tLoss: 0.1387\tLR: 0.000800\n",
            "Training Epoch: 197 [19456/50000]\tLoss: 0.1420\tLR: 0.000800\n",
            "Training Epoch: 197 [19584/50000]\tLoss: 0.1189\tLR: 0.000800\n",
            "Training Epoch: 197 [19712/50000]\tLoss: 0.1628\tLR: 0.000800\n",
            "Training Epoch: 197 [19840/50000]\tLoss: 0.1266\tLR: 0.000800\n",
            "Training Epoch: 197 [19968/50000]\tLoss: 0.1900\tLR: 0.000800\n",
            "Training Epoch: 197 [20096/50000]\tLoss: 0.1801\tLR: 0.000800\n",
            "Training Epoch: 197 [20224/50000]\tLoss: 0.2012\tLR: 0.000800\n",
            "Training Epoch: 197 [20352/50000]\tLoss: 0.1590\tLR: 0.000800\n",
            "Training Epoch: 197 [20480/50000]\tLoss: 0.1190\tLR: 0.000800\n",
            "Training Epoch: 197 [20608/50000]\tLoss: 0.1504\tLR: 0.000800\n",
            "Training Epoch: 197 [20736/50000]\tLoss: 0.1129\tLR: 0.000800\n",
            "Training Epoch: 197 [20864/50000]\tLoss: 0.1222\tLR: 0.000800\n",
            "Training Epoch: 197 [20992/50000]\tLoss: 0.1659\tLR: 0.000800\n",
            "Training Epoch: 197 [21120/50000]\tLoss: 0.2126\tLR: 0.000800\n",
            "Training Epoch: 197 [21248/50000]\tLoss: 0.0946\tLR: 0.000800\n",
            "Training Epoch: 197 [21376/50000]\tLoss: 0.1573\tLR: 0.000800\n",
            "Training Epoch: 197 [21504/50000]\tLoss: 0.0901\tLR: 0.000800\n",
            "Training Epoch: 197 [21632/50000]\tLoss: 0.1050\tLR: 0.000800\n",
            "Training Epoch: 197 [21760/50000]\tLoss: 0.0946\tLR: 0.000800\n",
            "Training Epoch: 197 [21888/50000]\tLoss: 0.1458\tLR: 0.000800\n",
            "Training Epoch: 197 [22016/50000]\tLoss: 0.1204\tLR: 0.000800\n",
            "Training Epoch: 197 [22144/50000]\tLoss: 0.1341\tLR: 0.000800\n",
            "Training Epoch: 197 [22272/50000]\tLoss: 0.1533\tLR: 0.000800\n",
            "Training Epoch: 197 [22400/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 197 [22528/50000]\tLoss: 0.1822\tLR: 0.000800\n",
            "Training Epoch: 197 [22656/50000]\tLoss: 0.1387\tLR: 0.000800\n",
            "Training Epoch: 197 [22784/50000]\tLoss: 0.1616\tLR: 0.000800\n",
            "Training Epoch: 197 [22912/50000]\tLoss: 0.1760\tLR: 0.000800\n",
            "Training Epoch: 197 [23040/50000]\tLoss: 0.0930\tLR: 0.000800\n",
            "Training Epoch: 197 [23168/50000]\tLoss: 0.1678\tLR: 0.000800\n",
            "Training Epoch: 197 [23296/50000]\tLoss: 0.1600\tLR: 0.000800\n",
            "Training Epoch: 197 [23424/50000]\tLoss: 0.1554\tLR: 0.000800\n",
            "Training Epoch: 197 [23552/50000]\tLoss: 0.1601\tLR: 0.000800\n",
            "Training Epoch: 197 [23680/50000]\tLoss: 0.2015\tLR: 0.000800\n",
            "Training Epoch: 197 [23808/50000]\tLoss: 0.1369\tLR: 0.000800\n",
            "Training Epoch: 197 [23936/50000]\tLoss: 0.1366\tLR: 0.000800\n",
            "Training Epoch: 197 [24064/50000]\tLoss: 0.1620\tLR: 0.000800\n",
            "Training Epoch: 197 [24192/50000]\tLoss: 0.1320\tLR: 0.000800\n",
            "Training Epoch: 197 [24320/50000]\tLoss: 0.1330\tLR: 0.000800\n",
            "Training Epoch: 197 [24448/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 197 [24576/50000]\tLoss: 0.2004\tLR: 0.000800\n",
            "Training Epoch: 197 [24704/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 197 [24832/50000]\tLoss: 0.1613\tLR: 0.000800\n",
            "Training Epoch: 197 [24960/50000]\tLoss: 0.1527\tLR: 0.000800\n",
            "Training Epoch: 197 [25088/50000]\tLoss: 0.2063\tLR: 0.000800\n",
            "Training Epoch: 197 [25216/50000]\tLoss: 0.1238\tLR: 0.000800\n",
            "Training Epoch: 197 [25344/50000]\tLoss: 0.2099\tLR: 0.000800\n",
            "Training Epoch: 197 [25472/50000]\tLoss: 0.1451\tLR: 0.000800\n",
            "Training Epoch: 197 [25600/50000]\tLoss: 0.0824\tLR: 0.000800\n",
            "Training Epoch: 197 [25728/50000]\tLoss: 0.0977\tLR: 0.000800\n",
            "Training Epoch: 197 [25856/50000]\tLoss: 0.1717\tLR: 0.000800\n",
            "Training Epoch: 197 [25984/50000]\tLoss: 0.1018\tLR: 0.000800\n",
            "Training Epoch: 197 [26112/50000]\tLoss: 0.0753\tLR: 0.000800\n",
            "Training Epoch: 197 [26240/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 197 [26368/50000]\tLoss: 0.1137\tLR: 0.000800\n",
            "Training Epoch: 197 [26496/50000]\tLoss: 0.1226\tLR: 0.000800\n",
            "Training Epoch: 197 [26624/50000]\tLoss: 0.1469\tLR: 0.000800\n",
            "Training Epoch: 197 [26752/50000]\tLoss: 0.1442\tLR: 0.000800\n",
            "Training Epoch: 197 [26880/50000]\tLoss: 0.0768\tLR: 0.000800\n",
            "Training Epoch: 197 [27008/50000]\tLoss: 0.1517\tLR: 0.000800\n",
            "Training Epoch: 197 [27136/50000]\tLoss: 0.1752\tLR: 0.000800\n",
            "Training Epoch: 197 [27264/50000]\tLoss: 0.1370\tLR: 0.000800\n",
            "Training Epoch: 197 [27392/50000]\tLoss: 0.1162\tLR: 0.000800\n",
            "Training Epoch: 197 [27520/50000]\tLoss: 0.1352\tLR: 0.000800\n",
            "Training Epoch: 197 [27648/50000]\tLoss: 0.1259\tLR: 0.000800\n",
            "Training Epoch: 197 [27776/50000]\tLoss: 0.0968\tLR: 0.000800\n",
            "Training Epoch: 197 [27904/50000]\tLoss: 0.1452\tLR: 0.000800\n",
            "Training Epoch: 197 [28032/50000]\tLoss: 0.1258\tLR: 0.000800\n",
            "Training Epoch: 197 [28160/50000]\tLoss: 0.1685\tLR: 0.000800\n",
            "Training Epoch: 197 [28288/50000]\tLoss: 0.1051\tLR: 0.000800\n",
            "Training Epoch: 197 [28416/50000]\tLoss: 0.1276\tLR: 0.000800\n",
            "Training Epoch: 197 [28544/50000]\tLoss: 0.1303\tLR: 0.000800\n",
            "Training Epoch: 197 [28672/50000]\tLoss: 0.1306\tLR: 0.000800\n",
            "Training Epoch: 197 [28800/50000]\tLoss: 0.1852\tLR: 0.000800\n",
            "Training Epoch: 197 [28928/50000]\tLoss: 0.0999\tLR: 0.000800\n",
            "Training Epoch: 197 [29056/50000]\tLoss: 0.1472\tLR: 0.000800\n",
            "Training Epoch: 197 [29184/50000]\tLoss: 0.0923\tLR: 0.000800\n",
            "Training Epoch: 197 [29312/50000]\tLoss: 0.1774\tLR: 0.000800\n",
            "Training Epoch: 197 [29440/50000]\tLoss: 0.1819\tLR: 0.000800\n",
            "Training Epoch: 197 [29568/50000]\tLoss: 0.0963\tLR: 0.000800\n",
            "Training Epoch: 197 [29696/50000]\tLoss: 0.1473\tLR: 0.000800\n",
            "Training Epoch: 197 [29824/50000]\tLoss: 0.0944\tLR: 0.000800\n",
            "Training Epoch: 197 [29952/50000]\tLoss: 0.1731\tLR: 0.000800\n",
            "Training Epoch: 197 [30080/50000]\tLoss: 0.1491\tLR: 0.000800\n",
            "Training Epoch: 197 [30208/50000]\tLoss: 0.1349\tLR: 0.000800\n",
            "Training Epoch: 197 [30336/50000]\tLoss: 0.1453\tLR: 0.000800\n",
            "Training Epoch: 197 [30464/50000]\tLoss: 0.1106\tLR: 0.000800\n",
            "Training Epoch: 197 [30592/50000]\tLoss: 0.0926\tLR: 0.000800\n",
            "Training Epoch: 197 [30720/50000]\tLoss: 0.1620\tLR: 0.000800\n",
            "Training Epoch: 197 [30848/50000]\tLoss: 0.1409\tLR: 0.000800\n",
            "Training Epoch: 197 [30976/50000]\tLoss: 0.0891\tLR: 0.000800\n",
            "Training Epoch: 197 [31104/50000]\tLoss: 0.2036\tLR: 0.000800\n",
            "Training Epoch: 197 [31232/50000]\tLoss: 0.1075\tLR: 0.000800\n",
            "Training Epoch: 197 [31360/50000]\tLoss: 0.1178\tLR: 0.000800\n",
            "Training Epoch: 197 [31488/50000]\tLoss: 0.0971\tLR: 0.000800\n",
            "Training Epoch: 197 [31616/50000]\tLoss: 0.1472\tLR: 0.000800\n",
            "Training Epoch: 197 [31744/50000]\tLoss: 0.0980\tLR: 0.000800\n",
            "Training Epoch: 197 [31872/50000]\tLoss: 0.1507\tLR: 0.000800\n",
            "Training Epoch: 197 [32000/50000]\tLoss: 0.0886\tLR: 0.000800\n",
            "Training Epoch: 197 [32128/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 197 [32256/50000]\tLoss: 0.1258\tLR: 0.000800\n",
            "Training Epoch: 197 [32384/50000]\tLoss: 0.1793\tLR: 0.000800\n",
            "Training Epoch: 197 [32512/50000]\tLoss: 0.1473\tLR: 0.000800\n",
            "Training Epoch: 197 [32640/50000]\tLoss: 0.1555\tLR: 0.000800\n",
            "Training Epoch: 197 [32768/50000]\tLoss: 0.1363\tLR: 0.000800\n",
            "Training Epoch: 197 [32896/50000]\tLoss: 0.1384\tLR: 0.000800\n",
            "Training Epoch: 197 [33024/50000]\tLoss: 0.1438\tLR: 0.000800\n",
            "Training Epoch: 197 [33152/50000]\tLoss: 0.1598\tLR: 0.000800\n",
            "Training Epoch: 197 [33280/50000]\tLoss: 0.1272\tLR: 0.000800\n",
            "Training Epoch: 197 [33408/50000]\tLoss: 0.1485\tLR: 0.000800\n",
            "Training Epoch: 197 [33536/50000]\tLoss: 0.1939\tLR: 0.000800\n",
            "Training Epoch: 197 [33664/50000]\tLoss: 0.1238\tLR: 0.000800\n",
            "Training Epoch: 197 [33792/50000]\tLoss: 0.1792\tLR: 0.000800\n",
            "Training Epoch: 197 [33920/50000]\tLoss: 0.1100\tLR: 0.000800\n",
            "Training Epoch: 197 [34048/50000]\tLoss: 0.1662\tLR: 0.000800\n",
            "Training Epoch: 197 [34176/50000]\tLoss: 0.0938\tLR: 0.000800\n",
            "Training Epoch: 197 [34304/50000]\tLoss: 0.1599\tLR: 0.000800\n",
            "Training Epoch: 197 [34432/50000]\tLoss: 0.2206\tLR: 0.000800\n",
            "Training Epoch: 197 [34560/50000]\tLoss: 0.1459\tLR: 0.000800\n",
            "Training Epoch: 197 [34688/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 197 [34816/50000]\tLoss: 0.1900\tLR: 0.000800\n",
            "Training Epoch: 197 [34944/50000]\tLoss: 0.1333\tLR: 0.000800\n",
            "Training Epoch: 197 [35072/50000]\tLoss: 0.1385\tLR: 0.000800\n",
            "Training Epoch: 197 [35200/50000]\tLoss: 0.2090\tLR: 0.000800\n",
            "Training Epoch: 197 [35328/50000]\tLoss: 0.1378\tLR: 0.000800\n",
            "Training Epoch: 197 [35456/50000]\tLoss: 0.0695\tLR: 0.000800\n",
            "Training Epoch: 197 [35584/50000]\tLoss: 0.1172\tLR: 0.000800\n",
            "Training Epoch: 197 [35712/50000]\tLoss: 0.1491\tLR: 0.000800\n",
            "Training Epoch: 197 [35840/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 197 [35968/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 197 [36096/50000]\tLoss: 0.1577\tLR: 0.000800\n",
            "Training Epoch: 197 [36224/50000]\tLoss: 0.1448\tLR: 0.000800\n",
            "Training Epoch: 197 [36352/50000]\tLoss: 0.0747\tLR: 0.000800\n",
            "Training Epoch: 197 [36480/50000]\tLoss: 0.1174\tLR: 0.000800\n",
            "Training Epoch: 197 [36608/50000]\tLoss: 0.1642\tLR: 0.000800\n",
            "Training Epoch: 197 [36736/50000]\tLoss: 0.0945\tLR: 0.000800\n",
            "Training Epoch: 197 [36864/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 197 [36992/50000]\tLoss: 0.1038\tLR: 0.000800\n",
            "Training Epoch: 197 [37120/50000]\tLoss: 0.1719\tLR: 0.000800\n",
            "Training Epoch: 197 [37248/50000]\tLoss: 0.1182\tLR: 0.000800\n",
            "Training Epoch: 197 [37376/50000]\tLoss: 0.1809\tLR: 0.000800\n",
            "Training Epoch: 197 [37504/50000]\tLoss: 0.1657\tLR: 0.000800\n",
            "Training Epoch: 197 [37632/50000]\tLoss: 0.1619\tLR: 0.000800\n",
            "Training Epoch: 197 [37760/50000]\tLoss: 0.1331\tLR: 0.000800\n",
            "Training Epoch: 197 [37888/50000]\tLoss: 0.1525\tLR: 0.000800\n",
            "Training Epoch: 197 [38016/50000]\tLoss: 0.2155\tLR: 0.000800\n",
            "Training Epoch: 197 [38144/50000]\tLoss: 0.1559\tLR: 0.000800\n",
            "Training Epoch: 197 [38272/50000]\tLoss: 0.2279\tLR: 0.000800\n",
            "Training Epoch: 197 [38400/50000]\tLoss: 0.1133\tLR: 0.000800\n",
            "Training Epoch: 197 [38528/50000]\tLoss: 0.1395\tLR: 0.000800\n",
            "Training Epoch: 197 [38656/50000]\tLoss: 0.1552\tLR: 0.000800\n",
            "Training Epoch: 197 [38784/50000]\tLoss: 0.1212\tLR: 0.000800\n",
            "Training Epoch: 197 [38912/50000]\tLoss: 0.1516\tLR: 0.000800\n",
            "Training Epoch: 197 [39040/50000]\tLoss: 0.1032\tLR: 0.000800\n",
            "Training Epoch: 197 [39168/50000]\tLoss: 0.1841\tLR: 0.000800\n",
            "Training Epoch: 197 [39296/50000]\tLoss: 0.1305\tLR: 0.000800\n",
            "Training Epoch: 197 [39424/50000]\tLoss: 0.1815\tLR: 0.000800\n",
            "Training Epoch: 197 [39552/50000]\tLoss: 0.1648\tLR: 0.000800\n",
            "Training Epoch: 197 [39680/50000]\tLoss: 0.1791\tLR: 0.000800\n",
            "Training Epoch: 197 [39808/50000]\tLoss: 0.2006\tLR: 0.000800\n",
            "Training Epoch: 197 [39936/50000]\tLoss: 0.1304\tLR: 0.000800\n",
            "Training Epoch: 197 [40064/50000]\tLoss: 0.1718\tLR: 0.000800\n",
            "Training Epoch: 197 [40192/50000]\tLoss: 0.1168\tLR: 0.000800\n",
            "Training Epoch: 197 [40320/50000]\tLoss: 0.2019\tLR: 0.000800\n",
            "Training Epoch: 197 [40448/50000]\tLoss: 0.1631\tLR: 0.000800\n",
            "Training Epoch: 197 [40576/50000]\tLoss: 0.1908\tLR: 0.000800\n",
            "Training Epoch: 197 [40704/50000]\tLoss: 0.1681\tLR: 0.000800\n",
            "Training Epoch: 197 [40832/50000]\tLoss: 0.1016\tLR: 0.000800\n",
            "Training Epoch: 197 [40960/50000]\tLoss: 0.1276\tLR: 0.000800\n",
            "Training Epoch: 197 [41088/50000]\tLoss: 0.2125\tLR: 0.000800\n",
            "Training Epoch: 197 [41216/50000]\tLoss: 0.1900\tLR: 0.000800\n",
            "Training Epoch: 197 [41344/50000]\tLoss: 0.1556\tLR: 0.000800\n",
            "Training Epoch: 197 [41472/50000]\tLoss: 0.1366\tLR: 0.000800\n",
            "Training Epoch: 197 [41600/50000]\tLoss: 0.0932\tLR: 0.000800\n",
            "Training Epoch: 197 [41728/50000]\tLoss: 0.1690\tLR: 0.000800\n",
            "Training Epoch: 197 [41856/50000]\tLoss: 0.1266\tLR: 0.000800\n",
            "Training Epoch: 197 [41984/50000]\tLoss: 0.1071\tLR: 0.000800\n",
            "Training Epoch: 197 [42112/50000]\tLoss: 0.1410\tLR: 0.000800\n",
            "Training Epoch: 197 [42240/50000]\tLoss: 0.1213\tLR: 0.000800\n",
            "Training Epoch: 197 [42368/50000]\tLoss: 0.1181\tLR: 0.000800\n",
            "Training Epoch: 197 [42496/50000]\tLoss: 0.2467\tLR: 0.000800\n",
            "Training Epoch: 197 [42624/50000]\tLoss: 0.1518\tLR: 0.000800\n",
            "Training Epoch: 197 [42752/50000]\tLoss: 0.1002\tLR: 0.000800\n",
            "Training Epoch: 197 [42880/50000]\tLoss: 0.1226\tLR: 0.000800\n",
            "Training Epoch: 197 [43008/50000]\tLoss: 0.1604\tLR: 0.000800\n",
            "Training Epoch: 197 [43136/50000]\tLoss: 0.1141\tLR: 0.000800\n",
            "Training Epoch: 197 [43264/50000]\tLoss: 0.1719\tLR: 0.000800\n",
            "Training Epoch: 197 [43392/50000]\tLoss: 0.1142\tLR: 0.000800\n",
            "Training Epoch: 197 [43520/50000]\tLoss: 0.1993\tLR: 0.000800\n",
            "Training Epoch: 197 [43648/50000]\tLoss: 0.1013\tLR: 0.000800\n",
            "Training Epoch: 197 [43776/50000]\tLoss: 0.0995\tLR: 0.000800\n",
            "Training Epoch: 197 [43904/50000]\tLoss: 0.1544\tLR: 0.000800\n",
            "Training Epoch: 197 [44032/50000]\tLoss: 0.1545\tLR: 0.000800\n",
            "Training Epoch: 197 [44160/50000]\tLoss: 0.1240\tLR: 0.000800\n",
            "Training Epoch: 197 [44288/50000]\tLoss: 0.0889\tLR: 0.000800\n",
            "Training Epoch: 197 [44416/50000]\tLoss: 0.1647\tLR: 0.000800\n",
            "Training Epoch: 197 [44544/50000]\tLoss: 0.1589\tLR: 0.000800\n",
            "Training Epoch: 197 [44672/50000]\tLoss: 0.2070\tLR: 0.000800\n",
            "Training Epoch: 197 [44800/50000]\tLoss: 0.0934\tLR: 0.000800\n",
            "Training Epoch: 197 [44928/50000]\tLoss: 0.1982\tLR: 0.000800\n",
            "Training Epoch: 197 [45056/50000]\tLoss: 0.1650\tLR: 0.000800\n",
            "Training Epoch: 197 [45184/50000]\tLoss: 0.2058\tLR: 0.000800\n",
            "Training Epoch: 197 [45312/50000]\tLoss: 0.1743\tLR: 0.000800\n",
            "Training Epoch: 197 [45440/50000]\tLoss: 0.1430\tLR: 0.000800\n",
            "Training Epoch: 197 [45568/50000]\tLoss: 0.1210\tLR: 0.000800\n",
            "Training Epoch: 197 [45696/50000]\tLoss: 0.1992\tLR: 0.000800\n",
            "Training Epoch: 197 [45824/50000]\tLoss: 0.1593\tLR: 0.000800\n",
            "Training Epoch: 197 [45952/50000]\tLoss: 0.1635\tLR: 0.000800\n",
            "Training Epoch: 197 [46080/50000]\tLoss: 0.1543\tLR: 0.000800\n",
            "Training Epoch: 197 [46208/50000]\tLoss: 0.1836\tLR: 0.000800\n",
            "Training Epoch: 197 [46336/50000]\tLoss: 0.1790\tLR: 0.000800\n",
            "Training Epoch: 197 [46464/50000]\tLoss: 0.1328\tLR: 0.000800\n",
            "Training Epoch: 197 [46592/50000]\tLoss: 0.1947\tLR: 0.000800\n",
            "Training Epoch: 197 [46720/50000]\tLoss: 0.1349\tLR: 0.000800\n",
            "Training Epoch: 197 [46848/50000]\tLoss: 0.1615\tLR: 0.000800\n",
            "Training Epoch: 197 [46976/50000]\tLoss: 0.2168\tLR: 0.000800\n",
            "Training Epoch: 197 [47104/50000]\tLoss: 0.1009\tLR: 0.000800\n",
            "Training Epoch: 197 [47232/50000]\tLoss: 0.1730\tLR: 0.000800\n",
            "Training Epoch: 197 [47360/50000]\tLoss: 0.1599\tLR: 0.000800\n",
            "Training Epoch: 197 [47488/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 197 [47616/50000]\tLoss: 0.1037\tLR: 0.000800\n",
            "Training Epoch: 197 [47744/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 197 [47872/50000]\tLoss: 0.1885\tLR: 0.000800\n",
            "Training Epoch: 197 [48000/50000]\tLoss: 0.1039\tLR: 0.000800\n",
            "Training Epoch: 197 [48128/50000]\tLoss: 0.0822\tLR: 0.000800\n",
            "Training Epoch: 197 [48256/50000]\tLoss: 0.1786\tLR: 0.000800\n",
            "Training Epoch: 197 [48384/50000]\tLoss: 0.1573\tLR: 0.000800\n",
            "Training Epoch: 197 [48512/50000]\tLoss: 0.1446\tLR: 0.000800\n",
            "Training Epoch: 197 [48640/50000]\tLoss: 0.1113\tLR: 0.000800\n",
            "Training Epoch: 197 [48768/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 197 [48896/50000]\tLoss: 0.1686\tLR: 0.000800\n",
            "Training Epoch: 197 [49024/50000]\tLoss: 0.1796\tLR: 0.000800\n",
            "Training Epoch: 197 [49152/50000]\tLoss: 0.1901\tLR: 0.000800\n",
            "Training Epoch: 197 [49280/50000]\tLoss: 0.1607\tLR: 0.000800\n",
            "Training Epoch: 197 [49408/50000]\tLoss: 0.1445\tLR: 0.000800\n",
            "Training Epoch: 197 [49536/50000]\tLoss: 0.1715\tLR: 0.000800\n",
            "Training Epoch: 197 [49664/50000]\tLoss: 0.1484\tLR: 0.000800\n",
            "Training Epoch: 197 [49792/50000]\tLoss: 0.1189\tLR: 0.000800\n",
            "Training Epoch: 197 [49920/50000]\tLoss: 0.1143\tLR: 0.000800\n",
            "Training Epoch: 197 [50000/50000]\tLoss: 0.1526\tLR: 0.000800\n",
            "epoch 197 training time consumed: 28.66s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  88654 GiB |  88654 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  86966 GiB |  86966 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1687 GiB |   1687 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  88654 GiB |  88654 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  86966 GiB |  86966 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1687 GiB |   1687 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  88391 GiB |  88391 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  86704 GiB |  86704 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1686 GiB |   1686 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  54238 GiB |  54238 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  52495 GiB |  52495 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1742 GiB |   1742 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   34351 K  |   34351 K  |\n",
            "|       from large pool |       8    |      61    |   11524 K  |   11524 K  |\n",
            "|       from small pool |     373    |     464    |   22827 K  |   22827 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   34351 K  |   34351 K  |\n",
            "|       from large pool |       8    |      61    |   11524 K  |   11524 K  |\n",
            "|       from small pool |     373    |     464    |   22827 K  |   22827 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   11292 K  |   11292 K  |\n",
            "|       from large pool |       5    |      14    |    5494 K  |    5494 K  |\n",
            "|       from small pool |      10    |      16    |    5798 K  |    5798 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 197, Average loss: 0.0113, Accuracy: 0.6672, Time consumed:2.74s\n",
            "\n",
            "Training Epoch: 198 [128/50000]\tLoss: 0.1203\tLR: 0.000800\n",
            "Training Epoch: 198 [256/50000]\tLoss: 0.1526\tLR: 0.000800\n",
            "Training Epoch: 198 [384/50000]\tLoss: 0.1094\tLR: 0.000800\n",
            "Training Epoch: 198 [512/50000]\tLoss: 0.1267\tLR: 0.000800\n",
            "Training Epoch: 198 [640/50000]\tLoss: 0.2073\tLR: 0.000800\n",
            "Training Epoch: 198 [768/50000]\tLoss: 0.2153\tLR: 0.000800\n",
            "Training Epoch: 198 [896/50000]\tLoss: 0.1771\tLR: 0.000800\n",
            "Training Epoch: 198 [1024/50000]\tLoss: 0.1257\tLR: 0.000800\n",
            "Training Epoch: 198 [1152/50000]\tLoss: 0.1112\tLR: 0.000800\n",
            "Training Epoch: 198 [1280/50000]\tLoss: 0.1260\tLR: 0.000800\n",
            "Training Epoch: 198 [1408/50000]\tLoss: 0.1408\tLR: 0.000800\n",
            "Training Epoch: 198 [1536/50000]\tLoss: 0.1119\tLR: 0.000800\n",
            "Training Epoch: 198 [1664/50000]\tLoss: 0.1482\tLR: 0.000800\n",
            "Training Epoch: 198 [1792/50000]\tLoss: 0.1720\tLR: 0.000800\n",
            "Training Epoch: 198 [1920/50000]\tLoss: 0.1504\tLR: 0.000800\n",
            "Training Epoch: 198 [2048/50000]\tLoss: 0.1235\tLR: 0.000800\n",
            "Training Epoch: 198 [2176/50000]\tLoss: 0.1324\tLR: 0.000800\n",
            "Training Epoch: 198 [2304/50000]\tLoss: 0.1402\tLR: 0.000800\n",
            "Training Epoch: 198 [2432/50000]\tLoss: 0.1144\tLR: 0.000800\n",
            "Training Epoch: 198 [2560/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 198 [2688/50000]\tLoss: 0.1260\tLR: 0.000800\n",
            "Training Epoch: 198 [2816/50000]\tLoss: 0.1027\tLR: 0.000800\n",
            "Training Epoch: 198 [2944/50000]\tLoss: 0.1566\tLR: 0.000800\n",
            "Training Epoch: 198 [3072/50000]\tLoss: 0.1443\tLR: 0.000800\n",
            "Training Epoch: 198 [3200/50000]\tLoss: 0.1614\tLR: 0.000800\n",
            "Training Epoch: 198 [3328/50000]\tLoss: 0.1775\tLR: 0.000800\n",
            "Training Epoch: 198 [3456/50000]\tLoss: 0.1576\tLR: 0.000800\n",
            "Training Epoch: 198 [3584/50000]\tLoss: 0.1500\tLR: 0.000800\n",
            "Training Epoch: 198 [3712/50000]\tLoss: 0.1698\tLR: 0.000800\n",
            "Training Epoch: 198 [3840/50000]\tLoss: 0.1449\tLR: 0.000800\n",
            "Training Epoch: 198 [3968/50000]\tLoss: 0.1066\tLR: 0.000800\n",
            "Training Epoch: 198 [4096/50000]\tLoss: 0.0965\tLR: 0.000800\n",
            "Training Epoch: 198 [4224/50000]\tLoss: 0.0952\tLR: 0.000800\n",
            "Training Epoch: 198 [4352/50000]\tLoss: 0.1494\tLR: 0.000800\n",
            "Training Epoch: 198 [4480/50000]\tLoss: 0.1686\tLR: 0.000800\n",
            "Training Epoch: 198 [4608/50000]\tLoss: 0.1700\tLR: 0.000800\n",
            "Training Epoch: 198 [4736/50000]\tLoss: 0.1069\tLR: 0.000800\n",
            "Training Epoch: 198 [4864/50000]\tLoss: 0.1593\tLR: 0.000800\n",
            "Training Epoch: 198 [4992/50000]\tLoss: 0.0799\tLR: 0.000800\n",
            "Training Epoch: 198 [5120/50000]\tLoss: 0.1491\tLR: 0.000800\n",
            "Training Epoch: 198 [5248/50000]\tLoss: 0.1170\tLR: 0.000800\n",
            "Training Epoch: 198 [5376/50000]\tLoss: 0.1250\tLR: 0.000800\n",
            "Training Epoch: 198 [5504/50000]\tLoss: 0.1470\tLR: 0.000800\n",
            "Training Epoch: 198 [5632/50000]\tLoss: 0.1309\tLR: 0.000800\n",
            "Training Epoch: 198 [5760/50000]\tLoss: 0.1716\tLR: 0.000800\n",
            "Training Epoch: 198 [5888/50000]\tLoss: 0.1139\tLR: 0.000800\n",
            "Training Epoch: 198 [6016/50000]\tLoss: 0.1633\tLR: 0.000800\n",
            "Training Epoch: 198 [6144/50000]\tLoss: 0.1106\tLR: 0.000800\n",
            "Training Epoch: 198 [6272/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 198 [6400/50000]\tLoss: 0.1021\tLR: 0.000800\n",
            "Training Epoch: 198 [6528/50000]\tLoss: 0.2101\tLR: 0.000800\n",
            "Training Epoch: 198 [6656/50000]\tLoss: 0.1374\tLR: 0.000800\n",
            "Training Epoch: 198 [6784/50000]\tLoss: 0.1169\tLR: 0.000800\n",
            "Training Epoch: 198 [6912/50000]\tLoss: 0.1020\tLR: 0.000800\n",
            "Training Epoch: 198 [7040/50000]\tLoss: 0.1004\tLR: 0.000800\n",
            "Training Epoch: 198 [7168/50000]\tLoss: 0.1829\tLR: 0.000800\n",
            "Training Epoch: 198 [7296/50000]\tLoss: 0.1590\tLR: 0.000800\n",
            "Training Epoch: 198 [7424/50000]\tLoss: 0.1058\tLR: 0.000800\n",
            "Training Epoch: 198 [7552/50000]\tLoss: 0.1692\tLR: 0.000800\n",
            "Training Epoch: 198 [7680/50000]\tLoss: 0.2206\tLR: 0.000800\n",
            "Training Epoch: 198 [7808/50000]\tLoss: 0.1659\tLR: 0.000800\n",
            "Training Epoch: 198 [7936/50000]\tLoss: 0.1353\tLR: 0.000800\n",
            "Training Epoch: 198 [8064/50000]\tLoss: 0.1326\tLR: 0.000800\n",
            "Training Epoch: 198 [8192/50000]\tLoss: 0.1905\tLR: 0.000800\n",
            "Training Epoch: 198 [8320/50000]\tLoss: 0.1456\tLR: 0.000800\n",
            "Training Epoch: 198 [8448/50000]\tLoss: 0.1107\tLR: 0.000800\n",
            "Training Epoch: 198 [8576/50000]\tLoss: 0.1209\tLR: 0.000800\n",
            "Training Epoch: 198 [8704/50000]\tLoss: 0.1084\tLR: 0.000800\n",
            "Training Epoch: 198 [8832/50000]\tLoss: 0.1902\tLR: 0.000800\n",
            "Training Epoch: 198 [8960/50000]\tLoss: 0.1745\tLR: 0.000800\n",
            "Training Epoch: 198 [9088/50000]\tLoss: 0.1122\tLR: 0.000800\n",
            "Training Epoch: 198 [9216/50000]\tLoss: 0.1798\tLR: 0.000800\n",
            "Training Epoch: 198 [9344/50000]\tLoss: 0.1651\tLR: 0.000800\n",
            "Training Epoch: 198 [9472/50000]\tLoss: 0.1741\tLR: 0.000800\n",
            "Training Epoch: 198 [9600/50000]\tLoss: 0.1708\tLR: 0.000800\n",
            "Training Epoch: 198 [9728/50000]\tLoss: 0.1253\tLR: 0.000800\n",
            "Training Epoch: 198 [9856/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 198 [9984/50000]\tLoss: 0.1403\tLR: 0.000800\n",
            "Training Epoch: 198 [10112/50000]\tLoss: 0.1086\tLR: 0.000800\n",
            "Training Epoch: 198 [10240/50000]\tLoss: 0.1498\tLR: 0.000800\n",
            "Training Epoch: 198 [10368/50000]\tLoss: 0.1755\tLR: 0.000800\n",
            "Training Epoch: 198 [10496/50000]\tLoss: 0.1286\tLR: 0.000800\n",
            "Training Epoch: 198 [10624/50000]\tLoss: 0.1484\tLR: 0.000800\n",
            "Training Epoch: 198 [10752/50000]\tLoss: 0.1614\tLR: 0.000800\n",
            "Training Epoch: 198 [10880/50000]\tLoss: 0.2324\tLR: 0.000800\n",
            "Training Epoch: 198 [11008/50000]\tLoss: 0.1770\tLR: 0.000800\n",
            "Training Epoch: 198 [11136/50000]\tLoss: 0.0965\tLR: 0.000800\n",
            "Training Epoch: 198 [11264/50000]\tLoss: 0.1512\tLR: 0.000800\n",
            "Training Epoch: 198 [11392/50000]\tLoss: 0.1078\tLR: 0.000800\n",
            "Training Epoch: 198 [11520/50000]\tLoss: 0.1538\tLR: 0.000800\n",
            "Training Epoch: 198 [11648/50000]\tLoss: 0.0823\tLR: 0.000800\n",
            "Training Epoch: 198 [11776/50000]\tLoss: 0.0762\tLR: 0.000800\n",
            "Training Epoch: 198 [11904/50000]\tLoss: 0.0932\tLR: 0.000800\n",
            "Training Epoch: 198 [12032/50000]\tLoss: 0.1087\tLR: 0.000800\n",
            "Training Epoch: 198 [12160/50000]\tLoss: 0.1702\tLR: 0.000800\n",
            "Training Epoch: 198 [12288/50000]\tLoss: 0.0679\tLR: 0.000800\n",
            "Training Epoch: 198 [12416/50000]\tLoss: 0.1294\tLR: 0.000800\n",
            "Training Epoch: 198 [12544/50000]\tLoss: 0.1078\tLR: 0.000800\n",
            "Training Epoch: 198 [12672/50000]\tLoss: 0.1720\tLR: 0.000800\n",
            "Training Epoch: 198 [12800/50000]\tLoss: 0.1299\tLR: 0.000800\n",
            "Training Epoch: 198 [12928/50000]\tLoss: 0.1590\tLR: 0.000800\n",
            "Training Epoch: 198 [13056/50000]\tLoss: 0.0986\tLR: 0.000800\n",
            "Training Epoch: 198 [13184/50000]\tLoss: 0.0682\tLR: 0.000800\n",
            "Training Epoch: 198 [13312/50000]\tLoss: 0.2078\tLR: 0.000800\n",
            "Training Epoch: 198 [13440/50000]\tLoss: 0.1420\tLR: 0.000800\n",
            "Training Epoch: 198 [13568/50000]\tLoss: 0.1239\tLR: 0.000800\n",
            "Training Epoch: 198 [13696/50000]\tLoss: 0.0834\tLR: 0.000800\n",
            "Training Epoch: 198 [13824/50000]\tLoss: 0.1016\tLR: 0.000800\n",
            "Training Epoch: 198 [13952/50000]\tLoss: 0.1210\tLR: 0.000800\n",
            "Training Epoch: 198 [14080/50000]\tLoss: 0.1658\tLR: 0.000800\n",
            "Training Epoch: 198 [14208/50000]\tLoss: 0.1492\tLR: 0.000800\n",
            "Training Epoch: 198 [14336/50000]\tLoss: 0.1148\tLR: 0.000800\n",
            "Training Epoch: 198 [14464/50000]\tLoss: 0.1053\tLR: 0.000800\n",
            "Training Epoch: 198 [14592/50000]\tLoss: 0.1218\tLR: 0.000800\n",
            "Training Epoch: 198 [14720/50000]\tLoss: 0.1266\tLR: 0.000800\n",
            "Training Epoch: 198 [14848/50000]\tLoss: 0.1646\tLR: 0.000800\n",
            "Training Epoch: 198 [14976/50000]\tLoss: 0.1736\tLR: 0.000800\n",
            "Training Epoch: 198 [15104/50000]\tLoss: 0.1640\tLR: 0.000800\n",
            "Training Epoch: 198 [15232/50000]\tLoss: 0.1133\tLR: 0.000800\n",
            "Training Epoch: 198 [15360/50000]\tLoss: 0.1482\tLR: 0.000800\n",
            "Training Epoch: 198 [15488/50000]\tLoss: 0.0834\tLR: 0.000800\n",
            "Training Epoch: 198 [15616/50000]\tLoss: 0.1669\tLR: 0.000800\n",
            "Training Epoch: 198 [15744/50000]\tLoss: 0.1241\tLR: 0.000800\n",
            "Training Epoch: 198 [15872/50000]\tLoss: 0.1540\tLR: 0.000800\n",
            "Training Epoch: 198 [16000/50000]\tLoss: 0.1443\tLR: 0.000800\n",
            "Training Epoch: 198 [16128/50000]\tLoss: 0.0607\tLR: 0.000800\n",
            "Training Epoch: 198 [16256/50000]\tLoss: 0.1629\tLR: 0.000800\n",
            "Training Epoch: 198 [16384/50000]\tLoss: 0.0710\tLR: 0.000800\n",
            "Training Epoch: 198 [16512/50000]\tLoss: 0.1952\tLR: 0.000800\n",
            "Training Epoch: 198 [16640/50000]\tLoss: 0.1772\tLR: 0.000800\n",
            "Training Epoch: 198 [16768/50000]\tLoss: 0.1136\tLR: 0.000800\n",
            "Training Epoch: 198 [16896/50000]\tLoss: 0.1476\tLR: 0.000800\n",
            "Training Epoch: 198 [17024/50000]\tLoss: 0.1193\tLR: 0.000800\n",
            "Training Epoch: 198 [17152/50000]\tLoss: 0.1436\tLR: 0.000800\n",
            "Training Epoch: 198 [17280/50000]\tLoss: 0.1556\tLR: 0.000800\n",
            "Training Epoch: 198 [17408/50000]\tLoss: 0.1412\tLR: 0.000800\n",
            "Training Epoch: 198 [17536/50000]\tLoss: 0.1883\tLR: 0.000800\n",
            "Training Epoch: 198 [17664/50000]\tLoss: 0.0939\tLR: 0.000800\n",
            "Training Epoch: 198 [17792/50000]\tLoss: 0.0767\tLR: 0.000800\n",
            "Training Epoch: 198 [17920/50000]\tLoss: 0.1956\tLR: 0.000800\n",
            "Training Epoch: 198 [18048/50000]\tLoss: 0.1075\tLR: 0.000800\n",
            "Training Epoch: 198 [18176/50000]\tLoss: 0.1433\tLR: 0.000800\n",
            "Training Epoch: 198 [18304/50000]\tLoss: 0.1893\tLR: 0.000800\n",
            "Training Epoch: 198 [18432/50000]\tLoss: 0.1356\tLR: 0.000800\n",
            "Training Epoch: 198 [18560/50000]\tLoss: 0.1537\tLR: 0.000800\n",
            "Training Epoch: 198 [18688/50000]\tLoss: 0.1674\tLR: 0.000800\n",
            "Training Epoch: 198 [18816/50000]\tLoss: 0.1241\tLR: 0.000800\n",
            "Training Epoch: 198 [18944/50000]\tLoss: 0.1356\tLR: 0.000800\n",
            "Training Epoch: 198 [19072/50000]\tLoss: 0.0952\tLR: 0.000800\n",
            "Training Epoch: 198 [19200/50000]\tLoss: 0.1321\tLR: 0.000800\n",
            "Training Epoch: 198 [19328/50000]\tLoss: 0.1210\tLR: 0.000800\n",
            "Training Epoch: 198 [19456/50000]\tLoss: 0.1030\tLR: 0.000800\n",
            "Training Epoch: 198 [19584/50000]\tLoss: 0.1968\tLR: 0.000800\n",
            "Training Epoch: 198 [19712/50000]\tLoss: 0.1310\tLR: 0.000800\n",
            "Training Epoch: 198 [19840/50000]\tLoss: 0.1612\tLR: 0.000800\n",
            "Training Epoch: 198 [19968/50000]\tLoss: 0.1356\tLR: 0.000800\n",
            "Training Epoch: 198 [20096/50000]\tLoss: 0.2264\tLR: 0.000800\n",
            "Training Epoch: 198 [20224/50000]\tLoss: 0.1182\tLR: 0.000800\n",
            "Training Epoch: 198 [20352/50000]\tLoss: 0.2050\tLR: 0.000800\n",
            "Training Epoch: 198 [20480/50000]\tLoss: 0.1635\tLR: 0.000800\n",
            "Training Epoch: 198 [20608/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 198 [20736/50000]\tLoss: 0.0870\tLR: 0.000800\n",
            "Training Epoch: 198 [20864/50000]\tLoss: 0.0993\tLR: 0.000800\n",
            "Training Epoch: 198 [20992/50000]\tLoss: 0.1747\tLR: 0.000800\n",
            "Training Epoch: 198 [21120/50000]\tLoss: 0.1684\tLR: 0.000800\n",
            "Training Epoch: 198 [21248/50000]\tLoss: 0.1446\tLR: 0.000800\n",
            "Training Epoch: 198 [21376/50000]\tLoss: 0.0892\tLR: 0.000800\n",
            "Training Epoch: 198 [21504/50000]\tLoss: 0.1005\tLR: 0.000800\n",
            "Training Epoch: 198 [21632/50000]\tLoss: 0.1705\tLR: 0.000800\n",
            "Training Epoch: 198 [21760/50000]\tLoss: 0.1396\tLR: 0.000800\n",
            "Training Epoch: 198 [21888/50000]\tLoss: 0.0884\tLR: 0.000800\n",
            "Training Epoch: 198 [22016/50000]\tLoss: 0.0982\tLR: 0.000800\n",
            "Training Epoch: 198 [22144/50000]\tLoss: 0.2068\tLR: 0.000800\n",
            "Training Epoch: 198 [22272/50000]\tLoss: 0.1718\tLR: 0.000800\n",
            "Training Epoch: 198 [22400/50000]\tLoss: 0.1133\tLR: 0.000800\n",
            "Training Epoch: 198 [22528/50000]\tLoss: 0.0958\tLR: 0.000800\n",
            "Training Epoch: 198 [22656/50000]\tLoss: 0.2102\tLR: 0.000800\n",
            "Training Epoch: 198 [22784/50000]\tLoss: 0.1073\tLR: 0.000800\n",
            "Training Epoch: 198 [22912/50000]\tLoss: 0.1746\tLR: 0.000800\n",
            "Training Epoch: 198 [23040/50000]\tLoss: 0.1179\tLR: 0.000800\n",
            "Training Epoch: 198 [23168/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 198 [23296/50000]\tLoss: 0.0985\tLR: 0.000800\n",
            "Training Epoch: 198 [23424/50000]\tLoss: 0.1273\tLR: 0.000800\n",
            "Training Epoch: 198 [23552/50000]\tLoss: 0.0882\tLR: 0.000800\n",
            "Training Epoch: 198 [23680/50000]\tLoss: 0.1475\tLR: 0.000800\n",
            "Training Epoch: 198 [23808/50000]\tLoss: 0.1141\tLR: 0.000800\n",
            "Training Epoch: 198 [23936/50000]\tLoss: 0.2221\tLR: 0.000800\n",
            "Training Epoch: 198 [24064/50000]\tLoss: 0.1796\tLR: 0.000800\n",
            "Training Epoch: 198 [24192/50000]\tLoss: 0.1548\tLR: 0.000800\n",
            "Training Epoch: 198 [24320/50000]\tLoss: 0.1234\tLR: 0.000800\n",
            "Training Epoch: 198 [24448/50000]\tLoss: 0.1034\tLR: 0.000800\n",
            "Training Epoch: 198 [24576/50000]\tLoss: 0.0966\tLR: 0.000800\n",
            "Training Epoch: 198 [24704/50000]\tLoss: 0.1025\tLR: 0.000800\n",
            "Training Epoch: 198 [24832/50000]\tLoss: 0.0939\tLR: 0.000800\n",
            "Training Epoch: 198 [24960/50000]\tLoss: 0.1505\tLR: 0.000800\n",
            "Training Epoch: 198 [25088/50000]\tLoss: 0.1646\tLR: 0.000800\n",
            "Training Epoch: 198 [25216/50000]\tLoss: 0.2367\tLR: 0.000800\n",
            "Training Epoch: 198 [25344/50000]\tLoss: 0.1605\tLR: 0.000800\n",
            "Training Epoch: 198 [25472/50000]\tLoss: 0.1182\tLR: 0.000800\n",
            "Training Epoch: 198 [25600/50000]\tLoss: 0.1500\tLR: 0.000800\n",
            "Training Epoch: 198 [25728/50000]\tLoss: 0.1334\tLR: 0.000800\n",
            "Training Epoch: 198 [25856/50000]\tLoss: 0.1613\tLR: 0.000800\n",
            "Training Epoch: 198 [25984/50000]\tLoss: 0.1185\tLR: 0.000800\n",
            "Training Epoch: 198 [26112/50000]\tLoss: 0.1951\tLR: 0.000800\n",
            "Training Epoch: 198 [26240/50000]\tLoss: 0.1883\tLR: 0.000800\n",
            "Training Epoch: 198 [26368/50000]\tLoss: 0.1074\tLR: 0.000800\n",
            "Training Epoch: 198 [26496/50000]\tLoss: 0.1191\tLR: 0.000800\n",
            "Training Epoch: 198 [26624/50000]\tLoss: 0.1421\tLR: 0.000800\n",
            "Training Epoch: 198 [26752/50000]\tLoss: 0.1450\tLR: 0.000800\n",
            "Training Epoch: 198 [26880/50000]\tLoss: 0.1583\tLR: 0.000800\n",
            "Training Epoch: 198 [27008/50000]\tLoss: 0.1166\tLR: 0.000800\n",
            "Training Epoch: 198 [27136/50000]\tLoss: 0.1638\tLR: 0.000800\n",
            "Training Epoch: 198 [27264/50000]\tLoss: 0.1257\tLR: 0.000800\n",
            "Training Epoch: 198 [27392/50000]\tLoss: 0.1770\tLR: 0.000800\n",
            "Training Epoch: 198 [27520/50000]\tLoss: 0.1188\tLR: 0.000800\n",
            "Training Epoch: 198 [27648/50000]\tLoss: 0.1185\tLR: 0.000800\n",
            "Training Epoch: 198 [27776/50000]\tLoss: 0.1385\tLR: 0.000800\n",
            "Training Epoch: 198 [27904/50000]\tLoss: 0.1513\tLR: 0.000800\n",
            "Training Epoch: 198 [28032/50000]\tLoss: 0.1793\tLR: 0.000800\n",
            "Training Epoch: 198 [28160/50000]\tLoss: 0.1713\tLR: 0.000800\n",
            "Training Epoch: 198 [28288/50000]\tLoss: 0.1413\tLR: 0.000800\n",
            "Training Epoch: 198 [28416/50000]\tLoss: 0.1079\tLR: 0.000800\n",
            "Training Epoch: 198 [28544/50000]\tLoss: 0.1644\tLR: 0.000800\n",
            "Training Epoch: 198 [28672/50000]\tLoss: 0.1878\tLR: 0.000800\n",
            "Training Epoch: 198 [28800/50000]\tLoss: 0.1146\tLR: 0.000800\n",
            "Training Epoch: 198 [28928/50000]\tLoss: 0.1400\tLR: 0.000800\n",
            "Training Epoch: 198 [29056/50000]\tLoss: 0.1389\tLR: 0.000800\n",
            "Training Epoch: 198 [29184/50000]\tLoss: 0.2152\tLR: 0.000800\n",
            "Training Epoch: 198 [29312/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 198 [29440/50000]\tLoss: 0.1223\tLR: 0.000800\n",
            "Training Epoch: 198 [29568/50000]\tLoss: 0.1461\tLR: 0.000800\n",
            "Training Epoch: 198 [29696/50000]\tLoss: 0.1407\tLR: 0.000800\n",
            "Training Epoch: 198 [29824/50000]\tLoss: 0.1546\tLR: 0.000800\n",
            "Training Epoch: 198 [29952/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 198 [30080/50000]\tLoss: 0.2580\tLR: 0.000800\n",
            "Training Epoch: 198 [30208/50000]\tLoss: 0.1081\tLR: 0.000800\n",
            "Training Epoch: 198 [30336/50000]\tLoss: 0.1025\tLR: 0.000800\n",
            "Training Epoch: 198 [30464/50000]\tLoss: 0.1352\tLR: 0.000800\n",
            "Training Epoch: 198 [30592/50000]\tLoss: 0.1859\tLR: 0.000800\n",
            "Training Epoch: 198 [30720/50000]\tLoss: 0.1630\tLR: 0.000800\n",
            "Training Epoch: 198 [30848/50000]\tLoss: 0.1725\tLR: 0.000800\n",
            "Training Epoch: 198 [30976/50000]\tLoss: 0.1354\tLR: 0.000800\n",
            "Training Epoch: 198 [31104/50000]\tLoss: 0.1789\tLR: 0.000800\n",
            "Training Epoch: 198 [31232/50000]\tLoss: 0.1442\tLR: 0.000800\n",
            "Training Epoch: 198 [31360/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 198 [31488/50000]\tLoss: 0.0837\tLR: 0.000800\n",
            "Training Epoch: 198 [31616/50000]\tLoss: 0.2230\tLR: 0.000800\n",
            "Training Epoch: 198 [31744/50000]\tLoss: 0.1651\tLR: 0.000800\n",
            "Training Epoch: 198 [31872/50000]\tLoss: 0.1057\tLR: 0.000800\n",
            "Training Epoch: 198 [32000/50000]\tLoss: 0.1405\tLR: 0.000800\n",
            "Training Epoch: 198 [32128/50000]\tLoss: 0.1251\tLR: 0.000800\n",
            "Training Epoch: 198 [32256/50000]\tLoss: 0.1111\tLR: 0.000800\n",
            "Training Epoch: 198 [32384/50000]\tLoss: 0.1621\tLR: 0.000800\n",
            "Training Epoch: 198 [32512/50000]\tLoss: 0.1361\tLR: 0.000800\n",
            "Training Epoch: 198 [32640/50000]\tLoss: 0.1167\tLR: 0.000800\n",
            "Training Epoch: 198 [32768/50000]\tLoss: 0.1165\tLR: 0.000800\n",
            "Training Epoch: 198 [32896/50000]\tLoss: 0.1958\tLR: 0.000800\n",
            "Training Epoch: 198 [33024/50000]\tLoss: 0.1001\tLR: 0.000800\n",
            "Training Epoch: 198 [33152/50000]\tLoss: 0.1286\tLR: 0.000800\n",
            "Training Epoch: 198 [33280/50000]\tLoss: 0.0869\tLR: 0.000800\n",
            "Training Epoch: 198 [33408/50000]\tLoss: 0.1474\tLR: 0.000800\n",
            "Training Epoch: 198 [33536/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 198 [33664/50000]\tLoss: 0.0532\tLR: 0.000800\n",
            "Training Epoch: 198 [33792/50000]\tLoss: 0.1217\tLR: 0.000800\n",
            "Training Epoch: 198 [33920/50000]\tLoss: 0.1234\tLR: 0.000800\n",
            "Training Epoch: 198 [34048/50000]\tLoss: 0.0828\tLR: 0.000800\n",
            "Training Epoch: 198 [34176/50000]\tLoss: 0.0700\tLR: 0.000800\n",
            "Training Epoch: 198 [34304/50000]\tLoss: 0.1629\tLR: 0.000800\n",
            "Training Epoch: 198 [34432/50000]\tLoss: 0.1234\tLR: 0.000800\n",
            "Training Epoch: 198 [34560/50000]\tLoss: 0.1300\tLR: 0.000800\n",
            "Training Epoch: 198 [34688/50000]\tLoss: 0.1337\tLR: 0.000800\n",
            "Training Epoch: 198 [34816/50000]\tLoss: 0.1622\tLR: 0.000800\n",
            "Training Epoch: 198 [34944/50000]\tLoss: 0.0888\tLR: 0.000800\n",
            "Training Epoch: 198 [35072/50000]\tLoss: 0.1274\tLR: 0.000800\n",
            "Training Epoch: 198 [35200/50000]\tLoss: 0.0793\tLR: 0.000800\n",
            "Training Epoch: 198 [35328/50000]\tLoss: 0.0909\tLR: 0.000800\n",
            "Training Epoch: 198 [35456/50000]\tLoss: 0.1799\tLR: 0.000800\n",
            "Training Epoch: 198 [35584/50000]\tLoss: 0.1114\tLR: 0.000800\n",
            "Training Epoch: 198 [35712/50000]\tLoss: 0.1556\tLR: 0.000800\n",
            "Training Epoch: 198 [35840/50000]\tLoss: 0.1132\tLR: 0.000800\n",
            "Training Epoch: 198 [35968/50000]\tLoss: 0.1322\tLR: 0.000800\n",
            "Training Epoch: 198 [36096/50000]\tLoss: 0.1227\tLR: 0.000800\n",
            "Training Epoch: 198 [36224/50000]\tLoss: 0.0600\tLR: 0.000800\n",
            "Training Epoch: 198 [36352/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 198 [36480/50000]\tLoss: 0.1334\tLR: 0.000800\n",
            "Training Epoch: 198 [36608/50000]\tLoss: 0.1219\tLR: 0.000800\n",
            "Training Epoch: 198 [36736/50000]\tLoss: 0.1142\tLR: 0.000800\n",
            "Training Epoch: 198 [36864/50000]\tLoss: 0.1461\tLR: 0.000800\n",
            "Training Epoch: 198 [36992/50000]\tLoss: 0.1450\tLR: 0.000800\n",
            "Training Epoch: 198 [37120/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 198 [37248/50000]\tLoss: 0.1932\tLR: 0.000800\n",
            "Training Epoch: 198 [37376/50000]\tLoss: 0.1123\tLR: 0.000800\n",
            "Training Epoch: 198 [37504/50000]\tLoss: 0.1080\tLR: 0.000800\n",
            "Training Epoch: 198 [37632/50000]\tLoss: 0.0954\tLR: 0.000800\n",
            "Training Epoch: 198 [37760/50000]\tLoss: 0.1188\tLR: 0.000800\n",
            "Training Epoch: 198 [37888/50000]\tLoss: 0.1780\tLR: 0.000800\n",
            "Training Epoch: 198 [38016/50000]\tLoss: 0.1344\tLR: 0.000800\n",
            "Training Epoch: 198 [38144/50000]\tLoss: 0.1274\tLR: 0.000800\n",
            "Training Epoch: 198 [38272/50000]\tLoss: 0.1132\tLR: 0.000800\n",
            "Training Epoch: 198 [38400/50000]\tLoss: 0.0750\tLR: 0.000800\n",
            "Training Epoch: 198 [38528/50000]\tLoss: 0.1634\tLR: 0.000800\n",
            "Training Epoch: 198 [38656/50000]\tLoss: 0.1075\tLR: 0.000800\n",
            "Training Epoch: 198 [38784/50000]\tLoss: 0.0951\tLR: 0.000800\n",
            "Training Epoch: 198 [38912/50000]\tLoss: 0.1163\tLR: 0.000800\n",
            "Training Epoch: 198 [39040/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 198 [39168/50000]\tLoss: 0.1510\tLR: 0.000800\n",
            "Training Epoch: 198 [39296/50000]\tLoss: 0.1632\tLR: 0.000800\n",
            "Training Epoch: 198 [39424/50000]\tLoss: 0.1452\tLR: 0.000800\n",
            "Training Epoch: 198 [39552/50000]\tLoss: 0.1386\tLR: 0.000800\n",
            "Training Epoch: 198 [39680/50000]\tLoss: 0.1459\tLR: 0.000800\n",
            "Training Epoch: 198 [39808/50000]\tLoss: 0.1588\tLR: 0.000800\n",
            "Training Epoch: 198 [39936/50000]\tLoss: 0.1197\tLR: 0.000800\n",
            "Training Epoch: 198 [40064/50000]\tLoss: 0.1631\tLR: 0.000800\n",
            "Training Epoch: 198 [40192/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 198 [40320/50000]\tLoss: 0.1486\tLR: 0.000800\n",
            "Training Epoch: 198 [40448/50000]\tLoss: 0.1620\tLR: 0.000800\n",
            "Training Epoch: 198 [40576/50000]\tLoss: 0.1827\tLR: 0.000800\n",
            "Training Epoch: 198 [40704/50000]\tLoss: 0.1646\tLR: 0.000800\n",
            "Training Epoch: 198 [40832/50000]\tLoss: 0.1665\tLR: 0.000800\n",
            "Training Epoch: 198 [40960/50000]\tLoss: 0.0895\tLR: 0.000800\n",
            "Training Epoch: 198 [41088/50000]\tLoss: 0.0803\tLR: 0.000800\n",
            "Training Epoch: 198 [41216/50000]\tLoss: 0.1693\tLR: 0.000800\n",
            "Training Epoch: 198 [41344/50000]\tLoss: 0.1334\tLR: 0.000800\n",
            "Training Epoch: 198 [41472/50000]\tLoss: 0.1835\tLR: 0.000800\n",
            "Training Epoch: 198 [41600/50000]\tLoss: 0.1755\tLR: 0.000800\n",
            "Training Epoch: 198 [41728/50000]\tLoss: 0.1463\tLR: 0.000800\n",
            "Training Epoch: 198 [41856/50000]\tLoss: 0.1700\tLR: 0.000800\n",
            "Training Epoch: 198 [41984/50000]\tLoss: 0.1797\tLR: 0.000800\n",
            "Training Epoch: 198 [42112/50000]\tLoss: 0.1168\tLR: 0.000800\n",
            "Training Epoch: 198 [42240/50000]\tLoss: 0.1706\tLR: 0.000800\n",
            "Training Epoch: 198 [42368/50000]\tLoss: 0.0783\tLR: 0.000800\n",
            "Training Epoch: 198 [42496/50000]\tLoss: 0.1184\tLR: 0.000800\n",
            "Training Epoch: 198 [42624/50000]\tLoss: 0.0770\tLR: 0.000800\n",
            "Training Epoch: 198 [42752/50000]\tLoss: 0.2161\tLR: 0.000800\n",
            "Training Epoch: 198 [42880/50000]\tLoss: 0.1181\tLR: 0.000800\n",
            "Training Epoch: 198 [43008/50000]\tLoss: 0.1111\tLR: 0.000800\n",
            "Training Epoch: 198 [43136/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 198 [43264/50000]\tLoss: 0.1454\tLR: 0.000800\n",
            "Training Epoch: 198 [43392/50000]\tLoss: 0.1598\tLR: 0.000800\n",
            "Training Epoch: 198 [43520/50000]\tLoss: 0.1106\tLR: 0.000800\n",
            "Training Epoch: 198 [43648/50000]\tLoss: 0.1341\tLR: 0.000800\n",
            "Training Epoch: 198 [43776/50000]\tLoss: 0.1881\tLR: 0.000800\n",
            "Training Epoch: 198 [43904/50000]\tLoss: 0.1402\tLR: 0.000800\n",
            "Training Epoch: 198 [44032/50000]\tLoss: 0.1667\tLR: 0.000800\n",
            "Training Epoch: 198 [44160/50000]\tLoss: 0.2529\tLR: 0.000800\n",
            "Training Epoch: 198 [44288/50000]\tLoss: 0.1364\tLR: 0.000800\n",
            "Training Epoch: 198 [44416/50000]\tLoss: 0.1257\tLR: 0.000800\n",
            "Training Epoch: 198 [44544/50000]\tLoss: 0.1651\tLR: 0.000800\n",
            "Training Epoch: 198 [44672/50000]\tLoss: 0.1279\tLR: 0.000800\n",
            "Training Epoch: 198 [44800/50000]\tLoss: 0.1456\tLR: 0.000800\n",
            "Training Epoch: 198 [44928/50000]\tLoss: 0.1400\tLR: 0.000800\n",
            "Training Epoch: 198 [45056/50000]\tLoss: 0.1962\tLR: 0.000800\n",
            "Training Epoch: 198 [45184/50000]\tLoss: 0.1550\tLR: 0.000800\n",
            "Training Epoch: 198 [45312/50000]\tLoss: 0.1752\tLR: 0.000800\n",
            "Training Epoch: 198 [45440/50000]\tLoss: 0.1200\tLR: 0.000800\n",
            "Training Epoch: 198 [45568/50000]\tLoss: 0.1794\tLR: 0.000800\n",
            "Training Epoch: 198 [45696/50000]\tLoss: 0.1134\tLR: 0.000800\n",
            "Training Epoch: 198 [45824/50000]\tLoss: 0.1215\tLR: 0.000800\n",
            "Training Epoch: 198 [45952/50000]\tLoss: 0.1282\tLR: 0.000800\n",
            "Training Epoch: 198 [46080/50000]\tLoss: 0.1503\tLR: 0.000800\n",
            "Training Epoch: 198 [46208/50000]\tLoss: 0.0813\tLR: 0.000800\n",
            "Training Epoch: 198 [46336/50000]\tLoss: 0.1336\tLR: 0.000800\n",
            "Training Epoch: 198 [46464/50000]\tLoss: 0.1368\tLR: 0.000800\n",
            "Training Epoch: 198 [46592/50000]\tLoss: 0.1318\tLR: 0.000800\n",
            "Training Epoch: 198 [46720/50000]\tLoss: 0.1698\tLR: 0.000800\n",
            "Training Epoch: 198 [46848/50000]\tLoss: 0.1445\tLR: 0.000800\n",
            "Training Epoch: 198 [46976/50000]\tLoss: 0.1555\tLR: 0.000800\n",
            "Training Epoch: 198 [47104/50000]\tLoss: 0.1060\tLR: 0.000800\n",
            "Training Epoch: 198 [47232/50000]\tLoss: 0.2047\tLR: 0.000800\n",
            "Training Epoch: 198 [47360/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 198 [47488/50000]\tLoss: 0.2440\tLR: 0.000800\n",
            "Training Epoch: 198 [47616/50000]\tLoss: 0.1225\tLR: 0.000800\n",
            "Training Epoch: 198 [47744/50000]\tLoss: 0.1616\tLR: 0.000800\n",
            "Training Epoch: 198 [47872/50000]\tLoss: 0.1130\tLR: 0.000800\n",
            "Training Epoch: 198 [48000/50000]\tLoss: 0.1357\tLR: 0.000800\n",
            "Training Epoch: 198 [48128/50000]\tLoss: 0.0706\tLR: 0.000800\n",
            "Training Epoch: 198 [48256/50000]\tLoss: 0.1509\tLR: 0.000800\n",
            "Training Epoch: 198 [48384/50000]\tLoss: 0.1521\tLR: 0.000800\n",
            "Training Epoch: 198 [48512/50000]\tLoss: 0.1205\tLR: 0.000800\n",
            "Training Epoch: 198 [48640/50000]\tLoss: 0.1495\tLR: 0.000800\n",
            "Training Epoch: 198 [48768/50000]\tLoss: 0.1409\tLR: 0.000800\n",
            "Training Epoch: 198 [48896/50000]\tLoss: 0.0971\tLR: 0.000800\n",
            "Training Epoch: 198 [49024/50000]\tLoss: 0.1404\tLR: 0.000800\n",
            "Training Epoch: 198 [49152/50000]\tLoss: 0.1483\tLR: 0.000800\n",
            "Training Epoch: 198 [49280/50000]\tLoss: 0.1169\tLR: 0.000800\n",
            "Training Epoch: 198 [49408/50000]\tLoss: 0.0942\tLR: 0.000800\n",
            "Training Epoch: 198 [49536/50000]\tLoss: 0.1181\tLR: 0.000800\n",
            "Training Epoch: 198 [49664/50000]\tLoss: 0.1266\tLR: 0.000800\n",
            "Training Epoch: 198 [49792/50000]\tLoss: 0.0869\tLR: 0.000800\n",
            "Training Epoch: 198 [49920/50000]\tLoss: 0.1021\tLR: 0.000800\n",
            "Training Epoch: 198 [50000/50000]\tLoss: 0.1971\tLR: 0.000800\n",
            "epoch 198 training time consumed: 29.51s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  89104 GiB |  89104 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  87408 GiB |  87408 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1696 GiB |   1696 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  89104 GiB |  89104 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  87408 GiB |  87408 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1696 GiB |   1696 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  88839 GiB |  88839 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  87145 GiB |  87145 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1694 GiB |   1694 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  54513 GiB |  54513 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  52762 GiB |  52762 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1750 GiB |   1750 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   34526 K  |   34525 K  |\n",
            "|       from large pool |       8    |      61    |   11582 K  |   11582 K  |\n",
            "|       from small pool |     373    |     464    |   22943 K  |   22943 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   34526 K  |   34525 K  |\n",
            "|       from large pool |       8    |      61    |   11582 K  |   11582 K  |\n",
            "|       from small pool |     373    |     464    |   22943 K  |   22943 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   11350 K  |   11350 K  |\n",
            "|       from large pool |       5    |      14    |    5522 K  |    5522 K  |\n",
            "|       from small pool |      10    |      16    |    5828 K  |    5828 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 198, Average loss: 0.0113, Accuracy: 0.6662, Time consumed:3.71s\n",
            "\n",
            "Training Epoch: 199 [128/50000]\tLoss: 0.1215\tLR: 0.000800\n",
            "Training Epoch: 199 [256/50000]\tLoss: 0.0838\tLR: 0.000800\n",
            "Training Epoch: 199 [384/50000]\tLoss: 0.1534\tLR: 0.000800\n",
            "Training Epoch: 199 [512/50000]\tLoss: 0.1025\tLR: 0.000800\n",
            "Training Epoch: 199 [640/50000]\tLoss: 0.1354\tLR: 0.000800\n",
            "Training Epoch: 199 [768/50000]\tLoss: 0.1026\tLR: 0.000800\n",
            "Training Epoch: 199 [896/50000]\tLoss: 0.1492\tLR: 0.000800\n",
            "Training Epoch: 199 [1024/50000]\tLoss: 0.1064\tLR: 0.000800\n",
            "Training Epoch: 199 [1152/50000]\tLoss: 0.0932\tLR: 0.000800\n",
            "Training Epoch: 199 [1280/50000]\tLoss: 0.1639\tLR: 0.000800\n",
            "Training Epoch: 199 [1408/50000]\tLoss: 0.1274\tLR: 0.000800\n",
            "Training Epoch: 199 [1536/50000]\tLoss: 0.1456\tLR: 0.000800\n",
            "Training Epoch: 199 [1664/50000]\tLoss: 0.1524\tLR: 0.000800\n",
            "Training Epoch: 199 [1792/50000]\tLoss: 0.1417\tLR: 0.000800\n",
            "Training Epoch: 199 [1920/50000]\tLoss: 0.1350\tLR: 0.000800\n",
            "Training Epoch: 199 [2048/50000]\tLoss: 0.0673\tLR: 0.000800\n",
            "Training Epoch: 199 [2176/50000]\tLoss: 0.1179\tLR: 0.000800\n",
            "Training Epoch: 199 [2304/50000]\tLoss: 0.0826\tLR: 0.000800\n",
            "Training Epoch: 199 [2432/50000]\tLoss: 0.1536\tLR: 0.000800\n",
            "Training Epoch: 199 [2560/50000]\tLoss: 0.1318\tLR: 0.000800\n",
            "Training Epoch: 199 [2688/50000]\tLoss: 0.1600\tLR: 0.000800\n",
            "Training Epoch: 199 [2816/50000]\tLoss: 0.2198\tLR: 0.000800\n",
            "Training Epoch: 199 [2944/50000]\tLoss: 0.1652\tLR: 0.000800\n",
            "Training Epoch: 199 [3072/50000]\tLoss: 0.1384\tLR: 0.000800\n",
            "Training Epoch: 199 [3200/50000]\tLoss: 0.0886\tLR: 0.000800\n",
            "Training Epoch: 199 [3328/50000]\tLoss: 0.0699\tLR: 0.000800\n",
            "Training Epoch: 199 [3456/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 199 [3584/50000]\tLoss: 0.1041\tLR: 0.000800\n",
            "Training Epoch: 199 [3712/50000]\tLoss: 0.1398\tLR: 0.000800\n",
            "Training Epoch: 199 [3840/50000]\tLoss: 0.1784\tLR: 0.000800\n",
            "Training Epoch: 199 [3968/50000]\tLoss: 0.1044\tLR: 0.000800\n",
            "Training Epoch: 199 [4096/50000]\tLoss: 0.0940\tLR: 0.000800\n",
            "Training Epoch: 199 [4224/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 199 [4352/50000]\tLoss: 0.1077\tLR: 0.000800\n",
            "Training Epoch: 199 [4480/50000]\tLoss: 0.1771\tLR: 0.000800\n",
            "Training Epoch: 199 [4608/50000]\tLoss: 0.0849\tLR: 0.000800\n",
            "Training Epoch: 199 [4736/50000]\tLoss: 0.1736\tLR: 0.000800\n",
            "Training Epoch: 199 [4864/50000]\tLoss: 0.1171\tLR: 0.000800\n",
            "Training Epoch: 199 [4992/50000]\tLoss: 0.1253\tLR: 0.000800\n",
            "Training Epoch: 199 [5120/50000]\tLoss: 0.1122\tLR: 0.000800\n",
            "Training Epoch: 199 [5248/50000]\tLoss: 0.1280\tLR: 0.000800\n",
            "Training Epoch: 199 [5376/50000]\tLoss: 0.1415\tLR: 0.000800\n",
            "Training Epoch: 199 [5504/50000]\tLoss: 0.1414\tLR: 0.000800\n",
            "Training Epoch: 199 [5632/50000]\tLoss: 0.1480\tLR: 0.000800\n",
            "Training Epoch: 199 [5760/50000]\tLoss: 0.1422\tLR: 0.000800\n",
            "Training Epoch: 199 [5888/50000]\tLoss: 0.1590\tLR: 0.000800\n",
            "Training Epoch: 199 [6016/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 199 [6144/50000]\tLoss: 0.1008\tLR: 0.000800\n",
            "Training Epoch: 199 [6272/50000]\tLoss: 0.1382\tLR: 0.000800\n",
            "Training Epoch: 199 [6400/50000]\tLoss: 0.0953\tLR: 0.000800\n",
            "Training Epoch: 199 [6528/50000]\tLoss: 0.1655\tLR: 0.000800\n",
            "Training Epoch: 199 [6656/50000]\tLoss: 0.1040\tLR: 0.000800\n",
            "Training Epoch: 199 [6784/50000]\tLoss: 0.0747\tLR: 0.000800\n",
            "Training Epoch: 199 [6912/50000]\tLoss: 0.1512\tLR: 0.000800\n",
            "Training Epoch: 199 [7040/50000]\tLoss: 0.1302\tLR: 0.000800\n",
            "Training Epoch: 199 [7168/50000]\tLoss: 0.1378\tLR: 0.000800\n",
            "Training Epoch: 199 [7296/50000]\tLoss: 0.2646\tLR: 0.000800\n",
            "Training Epoch: 199 [7424/50000]\tLoss: 0.1240\tLR: 0.000800\n",
            "Training Epoch: 199 [7552/50000]\tLoss: 0.1659\tLR: 0.000800\n",
            "Training Epoch: 199 [7680/50000]\tLoss: 0.0920\tLR: 0.000800\n",
            "Training Epoch: 199 [7808/50000]\tLoss: 0.1555\tLR: 0.000800\n",
            "Training Epoch: 199 [7936/50000]\tLoss: 0.1867\tLR: 0.000800\n",
            "Training Epoch: 199 [8064/50000]\tLoss: 0.0759\tLR: 0.000800\n",
            "Training Epoch: 199 [8192/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 199 [8320/50000]\tLoss: 0.1150\tLR: 0.000800\n",
            "Training Epoch: 199 [8448/50000]\tLoss: 0.1017\tLR: 0.000800\n",
            "Training Epoch: 199 [8576/50000]\tLoss: 0.1057\tLR: 0.000800\n",
            "Training Epoch: 199 [8704/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 199 [8832/50000]\tLoss: 0.1496\tLR: 0.000800\n",
            "Training Epoch: 199 [8960/50000]\tLoss: 0.1220\tLR: 0.000800\n",
            "Training Epoch: 199 [9088/50000]\tLoss: 0.0821\tLR: 0.000800\n",
            "Training Epoch: 199 [9216/50000]\tLoss: 0.0892\tLR: 0.000800\n",
            "Training Epoch: 199 [9344/50000]\tLoss: 0.1403\tLR: 0.000800\n",
            "Training Epoch: 199 [9472/50000]\tLoss: 0.1115\tLR: 0.000800\n",
            "Training Epoch: 199 [9600/50000]\tLoss: 0.1086\tLR: 0.000800\n",
            "Training Epoch: 199 [9728/50000]\tLoss: 0.1316\tLR: 0.000800\n",
            "Training Epoch: 199 [9856/50000]\tLoss: 0.1343\tLR: 0.000800\n",
            "Training Epoch: 199 [9984/50000]\tLoss: 0.1025\tLR: 0.000800\n",
            "Training Epoch: 199 [10112/50000]\tLoss: 0.1545\tLR: 0.000800\n",
            "Training Epoch: 199 [10240/50000]\tLoss: 0.1499\tLR: 0.000800\n",
            "Training Epoch: 199 [10368/50000]\tLoss: 0.1168\tLR: 0.000800\n",
            "Training Epoch: 199 [10496/50000]\tLoss: 0.0941\tLR: 0.000800\n",
            "Training Epoch: 199 [10624/50000]\tLoss: 0.1271\tLR: 0.000800\n",
            "Training Epoch: 199 [10752/50000]\tLoss: 0.1305\tLR: 0.000800\n",
            "Training Epoch: 199 [10880/50000]\tLoss: 0.1377\tLR: 0.000800\n",
            "Training Epoch: 199 [11008/50000]\tLoss: 0.1478\tLR: 0.000800\n",
            "Training Epoch: 199 [11136/50000]\tLoss: 0.1445\tLR: 0.000800\n",
            "Training Epoch: 199 [11264/50000]\tLoss: 0.1454\tLR: 0.000800\n",
            "Training Epoch: 199 [11392/50000]\tLoss: 0.1214\tLR: 0.000800\n",
            "Training Epoch: 199 [11520/50000]\tLoss: 0.1038\tLR: 0.000800\n",
            "Training Epoch: 199 [11648/50000]\tLoss: 0.0879\tLR: 0.000800\n",
            "Training Epoch: 199 [11776/50000]\tLoss: 0.1637\tLR: 0.000800\n",
            "Training Epoch: 199 [11904/50000]\tLoss: 0.1234\tLR: 0.000800\n",
            "Training Epoch: 199 [12032/50000]\tLoss: 0.0923\tLR: 0.000800\n",
            "Training Epoch: 199 [12160/50000]\tLoss: 0.1525\tLR: 0.000800\n",
            "Training Epoch: 199 [12288/50000]\tLoss: 0.1622\tLR: 0.000800\n",
            "Training Epoch: 199 [12416/50000]\tLoss: 0.0694\tLR: 0.000800\n",
            "Training Epoch: 199 [12544/50000]\tLoss: 0.1082\tLR: 0.000800\n",
            "Training Epoch: 199 [12672/50000]\tLoss: 0.1639\tLR: 0.000800\n",
            "Training Epoch: 199 [12800/50000]\tLoss: 0.1470\tLR: 0.000800\n",
            "Training Epoch: 199 [12928/50000]\tLoss: 0.2085\tLR: 0.000800\n",
            "Training Epoch: 199 [13056/50000]\tLoss: 0.1976\tLR: 0.000800\n",
            "Training Epoch: 199 [13184/50000]\tLoss: 0.2041\tLR: 0.000800\n",
            "Training Epoch: 199 [13312/50000]\tLoss: 0.1035\tLR: 0.000800\n",
            "Training Epoch: 199 [13440/50000]\tLoss: 0.1145\tLR: 0.000800\n",
            "Training Epoch: 199 [13568/50000]\tLoss: 0.1333\tLR: 0.000800\n",
            "Training Epoch: 199 [13696/50000]\tLoss: 0.1677\tLR: 0.000800\n",
            "Training Epoch: 199 [13824/50000]\tLoss: 0.1250\tLR: 0.000800\n",
            "Training Epoch: 199 [13952/50000]\tLoss: 0.1352\tLR: 0.000800\n",
            "Training Epoch: 199 [14080/50000]\tLoss: 0.1139\tLR: 0.000800\n",
            "Training Epoch: 199 [14208/50000]\tLoss: 0.1319\tLR: 0.000800\n",
            "Training Epoch: 199 [14336/50000]\tLoss: 0.1266\tLR: 0.000800\n",
            "Training Epoch: 199 [14464/50000]\tLoss: 0.1608\tLR: 0.000800\n",
            "Training Epoch: 199 [14592/50000]\tLoss: 0.1658\tLR: 0.000800\n",
            "Training Epoch: 199 [14720/50000]\tLoss: 0.1737\tLR: 0.000800\n",
            "Training Epoch: 199 [14848/50000]\tLoss: 0.1233\tLR: 0.000800\n",
            "Training Epoch: 199 [14976/50000]\tLoss: 0.1417\tLR: 0.000800\n",
            "Training Epoch: 199 [15104/50000]\tLoss: 0.0898\tLR: 0.000800\n",
            "Training Epoch: 199 [15232/50000]\tLoss: 0.1557\tLR: 0.000800\n",
            "Training Epoch: 199 [15360/50000]\tLoss: 0.0787\tLR: 0.000800\n",
            "Training Epoch: 199 [15488/50000]\tLoss: 0.1562\tLR: 0.000800\n",
            "Training Epoch: 199 [15616/50000]\tLoss: 0.1655\tLR: 0.000800\n",
            "Training Epoch: 199 [15744/50000]\tLoss: 0.1837\tLR: 0.000800\n",
            "Training Epoch: 199 [15872/50000]\tLoss: 0.1297\tLR: 0.000800\n",
            "Training Epoch: 199 [16000/50000]\tLoss: 0.1300\tLR: 0.000800\n",
            "Training Epoch: 199 [16128/50000]\tLoss: 0.1516\tLR: 0.000800\n",
            "Training Epoch: 199 [16256/50000]\tLoss: 0.1830\tLR: 0.000800\n",
            "Training Epoch: 199 [16384/50000]\tLoss: 0.0897\tLR: 0.000800\n",
            "Training Epoch: 199 [16512/50000]\tLoss: 0.0956\tLR: 0.000800\n",
            "Training Epoch: 199 [16640/50000]\tLoss: 0.1208\tLR: 0.000800\n",
            "Training Epoch: 199 [16768/50000]\tLoss: 0.1791\tLR: 0.000800\n",
            "Training Epoch: 199 [16896/50000]\tLoss: 0.1461\tLR: 0.000800\n",
            "Training Epoch: 199 [17024/50000]\tLoss: 0.1823\tLR: 0.000800\n",
            "Training Epoch: 199 [17152/50000]\tLoss: 0.1381\tLR: 0.000800\n",
            "Training Epoch: 199 [17280/50000]\tLoss: 0.1038\tLR: 0.000800\n",
            "Training Epoch: 199 [17408/50000]\tLoss: 0.1330\tLR: 0.000800\n",
            "Training Epoch: 199 [17536/50000]\tLoss: 0.1108\tLR: 0.000800\n",
            "Training Epoch: 199 [17664/50000]\tLoss: 0.1019\tLR: 0.000800\n",
            "Training Epoch: 199 [17792/50000]\tLoss: 0.1472\tLR: 0.000800\n",
            "Training Epoch: 199 [17920/50000]\tLoss: 0.1019\tLR: 0.000800\n",
            "Training Epoch: 199 [18048/50000]\tLoss: 0.1190\tLR: 0.000800\n",
            "Training Epoch: 199 [18176/50000]\tLoss: 0.1433\tLR: 0.000800\n",
            "Training Epoch: 199 [18304/50000]\tLoss: 0.1763\tLR: 0.000800\n",
            "Training Epoch: 199 [18432/50000]\tLoss: 0.1398\tLR: 0.000800\n",
            "Training Epoch: 199 [18560/50000]\tLoss: 0.1250\tLR: 0.000800\n",
            "Training Epoch: 199 [18688/50000]\tLoss: 0.1219\tLR: 0.000800\n",
            "Training Epoch: 199 [18816/50000]\tLoss: 0.0928\tLR: 0.000800\n",
            "Training Epoch: 199 [18944/50000]\tLoss: 0.1441\tLR: 0.000800\n",
            "Training Epoch: 199 [19072/50000]\tLoss: 0.1575\tLR: 0.000800\n",
            "Training Epoch: 199 [19200/50000]\tLoss: 0.1657\tLR: 0.000800\n",
            "Training Epoch: 199 [19328/50000]\tLoss: 0.1063\tLR: 0.000800\n",
            "Training Epoch: 199 [19456/50000]\tLoss: 0.1567\tLR: 0.000800\n",
            "Training Epoch: 199 [19584/50000]\tLoss: 0.1761\tLR: 0.000800\n",
            "Training Epoch: 199 [19712/50000]\tLoss: 0.1882\tLR: 0.000800\n",
            "Training Epoch: 199 [19840/50000]\tLoss: 0.1525\tLR: 0.000800\n",
            "Training Epoch: 199 [19968/50000]\tLoss: 0.1105\tLR: 0.000800\n",
            "Training Epoch: 199 [20096/50000]\tLoss: 0.1009\tLR: 0.000800\n",
            "Training Epoch: 199 [20224/50000]\tLoss: 0.2195\tLR: 0.000800\n",
            "Training Epoch: 199 [20352/50000]\tLoss: 0.1520\tLR: 0.000800\n",
            "Training Epoch: 199 [20480/50000]\tLoss: 0.1089\tLR: 0.000800\n",
            "Training Epoch: 199 [20608/50000]\tLoss: 0.1971\tLR: 0.000800\n",
            "Training Epoch: 199 [20736/50000]\tLoss: 0.1427\tLR: 0.000800\n",
            "Training Epoch: 199 [20864/50000]\tLoss: 0.1350\tLR: 0.000800\n",
            "Training Epoch: 199 [20992/50000]\tLoss: 0.0798\tLR: 0.000800\n",
            "Training Epoch: 199 [21120/50000]\tLoss: 0.1373\tLR: 0.000800\n",
            "Training Epoch: 199 [21248/50000]\tLoss: 0.1578\tLR: 0.000800\n",
            "Training Epoch: 199 [21376/50000]\tLoss: 0.1530\tLR: 0.000800\n",
            "Training Epoch: 199 [21504/50000]\tLoss: 0.0857\tLR: 0.000800\n",
            "Training Epoch: 199 [21632/50000]\tLoss: 0.1317\tLR: 0.000800\n",
            "Training Epoch: 199 [21760/50000]\tLoss: 0.1759\tLR: 0.000800\n",
            "Training Epoch: 199 [21888/50000]\tLoss: 0.2165\tLR: 0.000800\n",
            "Training Epoch: 199 [22016/50000]\tLoss: 0.1539\tLR: 0.000800\n",
            "Training Epoch: 199 [22144/50000]\tLoss: 0.1501\tLR: 0.000800\n",
            "Training Epoch: 199 [22272/50000]\tLoss: 0.1104\tLR: 0.000800\n",
            "Training Epoch: 199 [22400/50000]\tLoss: 0.1761\tLR: 0.000800\n",
            "Training Epoch: 199 [22528/50000]\tLoss: 0.1572\tLR: 0.000800\n",
            "Training Epoch: 199 [22656/50000]\tLoss: 0.1451\tLR: 0.000800\n",
            "Training Epoch: 199 [22784/50000]\tLoss: 0.1722\tLR: 0.000800\n",
            "Training Epoch: 199 [22912/50000]\tLoss: 0.0951\tLR: 0.000800\n",
            "Training Epoch: 199 [23040/50000]\tLoss: 0.1185\tLR: 0.000800\n",
            "Training Epoch: 199 [23168/50000]\tLoss: 0.2146\tLR: 0.000800\n",
            "Training Epoch: 199 [23296/50000]\tLoss: 0.1752\tLR: 0.000800\n",
            "Training Epoch: 199 [23424/50000]\tLoss: 0.1562\tLR: 0.000800\n",
            "Training Epoch: 199 [23552/50000]\tLoss: 0.1422\tLR: 0.000800\n",
            "Training Epoch: 199 [23680/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 199 [23808/50000]\tLoss: 0.1355\tLR: 0.000800\n",
            "Training Epoch: 199 [23936/50000]\tLoss: 0.1127\tLR: 0.000800\n",
            "Training Epoch: 199 [24064/50000]\tLoss: 0.1161\tLR: 0.000800\n",
            "Training Epoch: 199 [24192/50000]\tLoss: 0.0898\tLR: 0.000800\n",
            "Training Epoch: 199 [24320/50000]\tLoss: 0.1875\tLR: 0.000800\n",
            "Training Epoch: 199 [24448/50000]\tLoss: 0.1279\tLR: 0.000800\n",
            "Training Epoch: 199 [24576/50000]\tLoss: 0.1827\tLR: 0.000800\n",
            "Training Epoch: 199 [24704/50000]\tLoss: 0.1238\tLR: 0.000800\n",
            "Training Epoch: 199 [24832/50000]\tLoss: 0.1655\tLR: 0.000800\n",
            "Training Epoch: 199 [24960/50000]\tLoss: 0.1488\tLR: 0.000800\n",
            "Training Epoch: 199 [25088/50000]\tLoss: 0.1095\tLR: 0.000800\n",
            "Training Epoch: 199 [25216/50000]\tLoss: 0.1302\tLR: 0.000800\n",
            "Training Epoch: 199 [25344/50000]\tLoss: 0.1452\tLR: 0.000800\n",
            "Training Epoch: 199 [25472/50000]\tLoss: 0.1619\tLR: 0.000800\n",
            "Training Epoch: 199 [25600/50000]\tLoss: 0.1248\tLR: 0.000800\n",
            "Training Epoch: 199 [25728/50000]\tLoss: 0.1227\tLR: 0.000800\n",
            "Training Epoch: 199 [25856/50000]\tLoss: 0.1077\tLR: 0.000800\n",
            "Training Epoch: 199 [25984/50000]\tLoss: 0.1310\tLR: 0.000800\n",
            "Training Epoch: 199 [26112/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 199 [26240/50000]\tLoss: 0.1634\tLR: 0.000800\n",
            "Training Epoch: 199 [26368/50000]\tLoss: 0.1024\tLR: 0.000800\n",
            "Training Epoch: 199 [26496/50000]\tLoss: 0.1121\tLR: 0.000800\n",
            "Training Epoch: 199 [26624/50000]\tLoss: 0.0949\tLR: 0.000800\n",
            "Training Epoch: 199 [26752/50000]\tLoss: 0.0972\tLR: 0.000800\n",
            "Training Epoch: 199 [26880/50000]\tLoss: 0.1399\tLR: 0.000800\n",
            "Training Epoch: 199 [27008/50000]\tLoss: 0.1656\tLR: 0.000800\n",
            "Training Epoch: 199 [27136/50000]\tLoss: 0.1220\tLR: 0.000800\n",
            "Training Epoch: 199 [27264/50000]\tLoss: 0.1707\tLR: 0.000800\n",
            "Training Epoch: 199 [27392/50000]\tLoss: 0.1501\tLR: 0.000800\n",
            "Training Epoch: 199 [27520/50000]\tLoss: 0.1336\tLR: 0.000800\n",
            "Training Epoch: 199 [27648/50000]\tLoss: 0.2334\tLR: 0.000800\n",
            "Training Epoch: 199 [27776/50000]\tLoss: 0.1459\tLR: 0.000800\n",
            "Training Epoch: 199 [27904/50000]\tLoss: 0.1492\tLR: 0.000800\n",
            "Training Epoch: 199 [28032/50000]\tLoss: 0.1276\tLR: 0.000800\n",
            "Training Epoch: 199 [28160/50000]\tLoss: 0.1238\tLR: 0.000800\n",
            "Training Epoch: 199 [28288/50000]\tLoss: 0.1879\tLR: 0.000800\n",
            "Training Epoch: 199 [28416/50000]\tLoss: 0.1313\tLR: 0.000800\n",
            "Training Epoch: 199 [28544/50000]\tLoss: 0.1112\tLR: 0.000800\n",
            "Training Epoch: 199 [28672/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 199 [28800/50000]\tLoss: 0.1580\tLR: 0.000800\n",
            "Training Epoch: 199 [28928/50000]\tLoss: 0.1955\tLR: 0.000800\n",
            "Training Epoch: 199 [29056/50000]\tLoss: 0.1035\tLR: 0.000800\n",
            "Training Epoch: 199 [29184/50000]\tLoss: 0.1707\tLR: 0.000800\n",
            "Training Epoch: 199 [29312/50000]\tLoss: 0.1620\tLR: 0.000800\n",
            "Training Epoch: 199 [29440/50000]\tLoss: 0.0860\tLR: 0.000800\n",
            "Training Epoch: 199 [29568/50000]\tLoss: 0.1200\tLR: 0.000800\n",
            "Training Epoch: 199 [29696/50000]\tLoss: 0.1802\tLR: 0.000800\n",
            "Training Epoch: 199 [29824/50000]\tLoss: 0.0976\tLR: 0.000800\n",
            "Training Epoch: 199 [29952/50000]\tLoss: 0.1348\tLR: 0.000800\n",
            "Training Epoch: 199 [30080/50000]\tLoss: 0.1115\tLR: 0.000800\n",
            "Training Epoch: 199 [30208/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 199 [30336/50000]\tLoss: 0.1100\tLR: 0.000800\n",
            "Training Epoch: 199 [30464/50000]\tLoss: 0.1863\tLR: 0.000800\n",
            "Training Epoch: 199 [30592/50000]\tLoss: 0.1668\tLR: 0.000800\n",
            "Training Epoch: 199 [30720/50000]\tLoss: 0.1157\tLR: 0.000800\n",
            "Training Epoch: 199 [30848/50000]\tLoss: 0.1053\tLR: 0.000800\n",
            "Training Epoch: 199 [30976/50000]\tLoss: 0.1416\tLR: 0.000800\n",
            "Training Epoch: 199 [31104/50000]\tLoss: 0.1646\tLR: 0.000800\n",
            "Training Epoch: 199 [31232/50000]\tLoss: 0.1097\tLR: 0.000800\n",
            "Training Epoch: 199 [31360/50000]\tLoss: 0.1439\tLR: 0.000800\n",
            "Training Epoch: 199 [31488/50000]\tLoss: 0.1804\tLR: 0.000800\n",
            "Training Epoch: 199 [31616/50000]\tLoss: 0.1820\tLR: 0.000800\n",
            "Training Epoch: 199 [31744/50000]\tLoss: 0.0810\tLR: 0.000800\n",
            "Training Epoch: 199 [31872/50000]\tLoss: 0.1862\tLR: 0.000800\n",
            "Training Epoch: 199 [32000/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 199 [32128/50000]\tLoss: 0.1547\tLR: 0.000800\n",
            "Training Epoch: 199 [32256/50000]\tLoss: 0.1856\tLR: 0.000800\n",
            "Training Epoch: 199 [32384/50000]\tLoss: 0.1715\tLR: 0.000800\n",
            "Training Epoch: 199 [32512/50000]\tLoss: 0.1784\tLR: 0.000800\n",
            "Training Epoch: 199 [32640/50000]\tLoss: 0.1228\tLR: 0.000800\n",
            "Training Epoch: 199 [32768/50000]\tLoss: 0.1317\tLR: 0.000800\n",
            "Training Epoch: 199 [32896/50000]\tLoss: 0.1107\tLR: 0.000800\n",
            "Training Epoch: 199 [33024/50000]\tLoss: 0.1462\tLR: 0.000800\n",
            "Training Epoch: 199 [33152/50000]\tLoss: 0.1296\tLR: 0.000800\n",
            "Training Epoch: 199 [33280/50000]\tLoss: 0.1437\tLR: 0.000800\n",
            "Training Epoch: 199 [33408/50000]\tLoss: 0.1407\tLR: 0.000800\n",
            "Training Epoch: 199 [33536/50000]\tLoss: 0.1578\tLR: 0.000800\n",
            "Training Epoch: 199 [33664/50000]\tLoss: 0.1650\tLR: 0.000800\n",
            "Training Epoch: 199 [33792/50000]\tLoss: 0.1545\tLR: 0.000800\n",
            "Training Epoch: 199 [33920/50000]\tLoss: 0.0890\tLR: 0.000800\n",
            "Training Epoch: 199 [34048/50000]\tLoss: 0.1148\tLR: 0.000800\n",
            "Training Epoch: 199 [34176/50000]\tLoss: 0.1354\tLR: 0.000800\n",
            "Training Epoch: 199 [34304/50000]\tLoss: 0.2205\tLR: 0.000800\n",
            "Training Epoch: 199 [34432/50000]\tLoss: 0.1166\tLR: 0.000800\n",
            "Training Epoch: 199 [34560/50000]\tLoss: 0.1281\tLR: 0.000800\n",
            "Training Epoch: 199 [34688/50000]\tLoss: 0.1109\tLR: 0.000800\n",
            "Training Epoch: 199 [34816/50000]\tLoss: 0.1435\tLR: 0.000800\n",
            "Training Epoch: 199 [34944/50000]\tLoss: 0.0860\tLR: 0.000800\n",
            "Training Epoch: 199 [35072/50000]\tLoss: 0.1274\tLR: 0.000800\n",
            "Training Epoch: 199 [35200/50000]\tLoss: 0.1299\tLR: 0.000800\n",
            "Training Epoch: 199 [35328/50000]\tLoss: 0.0843\tLR: 0.000800\n",
            "Training Epoch: 199 [35456/50000]\tLoss: 0.1310\tLR: 0.000800\n",
            "Training Epoch: 199 [35584/50000]\tLoss: 0.1027\tLR: 0.000800\n",
            "Training Epoch: 199 [35712/50000]\tLoss: 0.1512\tLR: 0.000800\n",
            "Training Epoch: 199 [35840/50000]\tLoss: 0.0945\tLR: 0.000800\n",
            "Training Epoch: 199 [35968/50000]\tLoss: 0.1348\tLR: 0.000800\n",
            "Training Epoch: 199 [36096/50000]\tLoss: 0.1128\tLR: 0.000800\n",
            "Training Epoch: 199 [36224/50000]\tLoss: 0.1066\tLR: 0.000800\n",
            "Training Epoch: 199 [36352/50000]\tLoss: 0.1575\tLR: 0.000800\n",
            "Training Epoch: 199 [36480/50000]\tLoss: 0.1112\tLR: 0.000800\n",
            "Training Epoch: 199 [36608/50000]\tLoss: 0.1834\tLR: 0.000800\n",
            "Training Epoch: 199 [36736/50000]\tLoss: 0.1304\tLR: 0.000800\n",
            "Training Epoch: 199 [36864/50000]\tLoss: 0.1870\tLR: 0.000800\n",
            "Training Epoch: 199 [36992/50000]\tLoss: 0.1428\tLR: 0.000800\n",
            "Training Epoch: 199 [37120/50000]\tLoss: 0.0871\tLR: 0.000800\n",
            "Training Epoch: 199 [37248/50000]\tLoss: 0.1513\tLR: 0.000800\n",
            "Training Epoch: 199 [37376/50000]\tLoss: 0.0962\tLR: 0.000800\n",
            "Training Epoch: 199 [37504/50000]\tLoss: 0.1691\tLR: 0.000800\n",
            "Training Epoch: 199 [37632/50000]\tLoss: 0.1238\tLR: 0.000800\n",
            "Training Epoch: 199 [37760/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 199 [37888/50000]\tLoss: 0.1882\tLR: 0.000800\n",
            "Training Epoch: 199 [38016/50000]\tLoss: 0.1302\tLR: 0.000800\n",
            "Training Epoch: 199 [38144/50000]\tLoss: 0.1563\tLR: 0.000800\n",
            "Training Epoch: 199 [38272/50000]\tLoss: 0.1469\tLR: 0.000800\n",
            "Training Epoch: 199 [38400/50000]\tLoss: 0.1403\tLR: 0.000800\n",
            "Training Epoch: 199 [38528/50000]\tLoss: 0.1522\tLR: 0.000800\n",
            "Training Epoch: 199 [38656/50000]\tLoss: 0.1667\tLR: 0.000800\n",
            "Training Epoch: 199 [38784/50000]\tLoss: 0.1287\tLR: 0.000800\n",
            "Training Epoch: 199 [38912/50000]\tLoss: 0.1268\tLR: 0.000800\n",
            "Training Epoch: 199 [39040/50000]\tLoss: 0.1339\tLR: 0.000800\n",
            "Training Epoch: 199 [39168/50000]\tLoss: 0.2021\tLR: 0.000800\n",
            "Training Epoch: 199 [39296/50000]\tLoss: 0.1259\tLR: 0.000800\n",
            "Training Epoch: 199 [39424/50000]\tLoss: 0.1683\tLR: 0.000800\n",
            "Training Epoch: 199 [39552/50000]\tLoss: 0.1565\tLR: 0.000800\n",
            "Training Epoch: 199 [39680/50000]\tLoss: 0.1517\tLR: 0.000800\n",
            "Training Epoch: 199 [39808/50000]\tLoss: 0.1014\tLR: 0.000800\n",
            "Training Epoch: 199 [39936/50000]\tLoss: 0.1179\tLR: 0.000800\n",
            "Training Epoch: 199 [40064/50000]\tLoss: 0.1670\tLR: 0.000800\n",
            "Training Epoch: 199 [40192/50000]\tLoss: 0.1124\tLR: 0.000800\n",
            "Training Epoch: 199 [40320/50000]\tLoss: 0.1611\tLR: 0.000800\n",
            "Training Epoch: 199 [40448/50000]\tLoss: 0.1562\tLR: 0.000800\n",
            "Training Epoch: 199 [40576/50000]\tLoss: 0.0916\tLR: 0.000800\n",
            "Training Epoch: 199 [40704/50000]\tLoss: 0.0946\tLR: 0.000800\n",
            "Training Epoch: 199 [40832/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 199 [40960/50000]\tLoss: 0.1274\tLR: 0.000800\n",
            "Training Epoch: 199 [41088/50000]\tLoss: 0.1496\tLR: 0.000800\n",
            "Training Epoch: 199 [41216/50000]\tLoss: 0.1656\tLR: 0.000800\n",
            "Training Epoch: 199 [41344/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 199 [41472/50000]\tLoss: 0.1812\tLR: 0.000800\n",
            "Training Epoch: 199 [41600/50000]\tLoss: 0.1204\tLR: 0.000800\n",
            "Training Epoch: 199 [41728/50000]\tLoss: 0.1693\tLR: 0.000800\n",
            "Training Epoch: 199 [41856/50000]\tLoss: 0.1211\tLR: 0.000800\n",
            "Training Epoch: 199 [41984/50000]\tLoss: 0.1694\tLR: 0.000800\n",
            "Training Epoch: 199 [42112/50000]\tLoss: 0.0910\tLR: 0.000800\n",
            "Training Epoch: 199 [42240/50000]\tLoss: 0.1095\tLR: 0.000800\n",
            "Training Epoch: 199 [42368/50000]\tLoss: 0.1526\tLR: 0.000800\n",
            "Training Epoch: 199 [42496/50000]\tLoss: 0.1233\tLR: 0.000800\n",
            "Training Epoch: 199 [42624/50000]\tLoss: 0.1119\tLR: 0.000800\n",
            "Training Epoch: 199 [42752/50000]\tLoss: 0.1679\tLR: 0.000800\n",
            "Training Epoch: 199 [42880/50000]\tLoss: 0.1361\tLR: 0.000800\n",
            "Training Epoch: 199 [43008/50000]\tLoss: 0.1776\tLR: 0.000800\n",
            "Training Epoch: 199 [43136/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 199 [43264/50000]\tLoss: 0.1208\tLR: 0.000800\n",
            "Training Epoch: 199 [43392/50000]\tLoss: 0.1219\tLR: 0.000800\n",
            "Training Epoch: 199 [43520/50000]\tLoss: 0.1641\tLR: 0.000800\n",
            "Training Epoch: 199 [43648/50000]\tLoss: 0.1034\tLR: 0.000800\n",
            "Training Epoch: 199 [43776/50000]\tLoss: 0.1271\tLR: 0.000800\n",
            "Training Epoch: 199 [43904/50000]\tLoss: 0.1546\tLR: 0.000800\n",
            "Training Epoch: 199 [44032/50000]\tLoss: 0.1434\tLR: 0.000800\n",
            "Training Epoch: 199 [44160/50000]\tLoss: 0.1366\tLR: 0.000800\n",
            "Training Epoch: 199 [44288/50000]\tLoss: 0.1955\tLR: 0.000800\n",
            "Training Epoch: 199 [44416/50000]\tLoss: 0.1664\tLR: 0.000800\n",
            "Training Epoch: 199 [44544/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 199 [44672/50000]\tLoss: 0.1442\tLR: 0.000800\n",
            "Training Epoch: 199 [44800/50000]\tLoss: 0.1168\tLR: 0.000800\n",
            "Training Epoch: 199 [44928/50000]\tLoss: 0.1295\tLR: 0.000800\n",
            "Training Epoch: 199 [45056/50000]\tLoss: 0.0903\tLR: 0.000800\n",
            "Training Epoch: 199 [45184/50000]\tLoss: 0.1386\tLR: 0.000800\n",
            "Training Epoch: 199 [45312/50000]\tLoss: 0.1264\tLR: 0.000800\n",
            "Training Epoch: 199 [45440/50000]\tLoss: 0.1530\tLR: 0.000800\n",
            "Training Epoch: 199 [45568/50000]\tLoss: 0.1226\tLR: 0.000800\n",
            "Training Epoch: 199 [45696/50000]\tLoss: 0.1441\tLR: 0.000800\n",
            "Training Epoch: 199 [45824/50000]\tLoss: 0.1460\tLR: 0.000800\n",
            "Training Epoch: 199 [45952/50000]\tLoss: 0.1194\tLR: 0.000800\n",
            "Training Epoch: 199 [46080/50000]\tLoss: 0.1576\tLR: 0.000800\n",
            "Training Epoch: 199 [46208/50000]\tLoss: 0.1453\tLR: 0.000800\n",
            "Training Epoch: 199 [46336/50000]\tLoss: 0.2272\tLR: 0.000800\n",
            "Training Epoch: 199 [46464/50000]\tLoss: 0.0870\tLR: 0.000800\n",
            "Training Epoch: 199 [46592/50000]\tLoss: 0.1250\tLR: 0.000800\n",
            "Training Epoch: 199 [46720/50000]\tLoss: 0.1152\tLR: 0.000800\n",
            "Training Epoch: 199 [46848/50000]\tLoss: 0.1064\tLR: 0.000800\n",
            "Training Epoch: 199 [46976/50000]\tLoss: 0.1706\tLR: 0.000800\n",
            "Training Epoch: 199 [47104/50000]\tLoss: 0.1860\tLR: 0.000800\n",
            "Training Epoch: 199 [47232/50000]\tLoss: 0.1143\tLR: 0.000800\n",
            "Training Epoch: 199 [47360/50000]\tLoss: 0.1816\tLR: 0.000800\n",
            "Training Epoch: 199 [47488/50000]\tLoss: 0.1807\tLR: 0.000800\n",
            "Training Epoch: 199 [47616/50000]\tLoss: 0.2329\tLR: 0.000800\n",
            "Training Epoch: 199 [47744/50000]\tLoss: 0.1552\tLR: 0.000800\n",
            "Training Epoch: 199 [47872/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 199 [48000/50000]\tLoss: 0.1428\tLR: 0.000800\n",
            "Training Epoch: 199 [48128/50000]\tLoss: 0.0840\tLR: 0.000800\n",
            "Training Epoch: 199 [48256/50000]\tLoss: 0.1211\tLR: 0.000800\n",
            "Training Epoch: 199 [48384/50000]\tLoss: 0.1770\tLR: 0.000800\n",
            "Training Epoch: 199 [48512/50000]\tLoss: 0.1656\tLR: 0.000800\n",
            "Training Epoch: 199 [48640/50000]\tLoss: 0.1190\tLR: 0.000800\n",
            "Training Epoch: 199 [48768/50000]\tLoss: 0.1109\tLR: 0.000800\n",
            "Training Epoch: 199 [48896/50000]\tLoss: 0.1120\tLR: 0.000800\n",
            "Training Epoch: 199 [49024/50000]\tLoss: 0.1195\tLR: 0.000800\n",
            "Training Epoch: 199 [49152/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 199 [49280/50000]\tLoss: 0.1358\tLR: 0.000800\n",
            "Training Epoch: 199 [49408/50000]\tLoss: 0.1385\tLR: 0.000800\n",
            "Training Epoch: 199 [49536/50000]\tLoss: 0.1119\tLR: 0.000800\n",
            "Training Epoch: 199 [49664/50000]\tLoss: 0.1375\tLR: 0.000800\n",
            "Training Epoch: 199 [49792/50000]\tLoss: 0.1320\tLR: 0.000800\n",
            "Training Epoch: 199 [49920/50000]\tLoss: 0.1520\tLR: 0.000800\n",
            "Training Epoch: 199 [50000/50000]\tLoss: 0.2127\tLR: 0.000800\n",
            "epoch 199 training time consumed: 28.81s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  89554 GiB |  89554 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  87849 GiB |  87849 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1704 GiB |   1704 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  89554 GiB |  89554 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  87849 GiB |  87849 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1704 GiB |   1704 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  89288 GiB |  89288 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  87585 GiB |  87585 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1703 GiB |   1703 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  54788 GiB |  54788 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  53028 GiB |  53028 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1759 GiB |   1759 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   34700 K  |   34700 K  |\n",
            "|       from large pool |       8    |      61    |   11641 K  |   11641 K  |\n",
            "|       from small pool |     373    |     464    |   23059 K  |   23059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   34700 K  |   34700 K  |\n",
            "|       from large pool |       8    |      61    |   11641 K  |   11641 K  |\n",
            "|       from small pool |     373    |     464    |   23059 K  |   23059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   11407 K  |   11407 K  |\n",
            "|       from large pool |       5    |      14    |    5550 K  |    5550 K  |\n",
            "|       from small pool |      10    |      16    |    5857 K  |    5857 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 199, Average loss: 0.0113, Accuracy: 0.6670, Time consumed:2.72s\n",
            "\n",
            "Training Epoch: 200 [128/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 200 [256/50000]\tLoss: 0.1745\tLR: 0.000800\n",
            "Training Epoch: 200 [384/50000]\tLoss: 0.1004\tLR: 0.000800\n",
            "Training Epoch: 200 [512/50000]\tLoss: 0.1340\tLR: 0.000800\n",
            "Training Epoch: 200 [640/50000]\tLoss: 0.1036\tLR: 0.000800\n",
            "Training Epoch: 200 [768/50000]\tLoss: 0.1381\tLR: 0.000800\n",
            "Training Epoch: 200 [896/50000]\tLoss: 0.1360\tLR: 0.000800\n",
            "Training Epoch: 200 [1024/50000]\tLoss: 0.1363\tLR: 0.000800\n",
            "Training Epoch: 200 [1152/50000]\tLoss: 0.1350\tLR: 0.000800\n",
            "Training Epoch: 200 [1280/50000]\tLoss: 0.0935\tLR: 0.000800\n",
            "Training Epoch: 200 [1408/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 200 [1536/50000]\tLoss: 0.1370\tLR: 0.000800\n",
            "Training Epoch: 200 [1664/50000]\tLoss: 0.0909\tLR: 0.000800\n",
            "Training Epoch: 200 [1792/50000]\tLoss: 0.1913\tLR: 0.000800\n",
            "Training Epoch: 200 [1920/50000]\tLoss: 0.1348\tLR: 0.000800\n",
            "Training Epoch: 200 [2048/50000]\tLoss: 0.1101\tLR: 0.000800\n",
            "Training Epoch: 200 [2176/50000]\tLoss: 0.1161\tLR: 0.000800\n",
            "Training Epoch: 200 [2304/50000]\tLoss: 0.0757\tLR: 0.000800\n",
            "Training Epoch: 200 [2432/50000]\tLoss: 0.1576\tLR: 0.000800\n",
            "Training Epoch: 200 [2560/50000]\tLoss: 0.1480\tLR: 0.000800\n",
            "Training Epoch: 200 [2688/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 200 [2816/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 200 [2944/50000]\tLoss: 0.1088\tLR: 0.000800\n",
            "Training Epoch: 200 [3072/50000]\tLoss: 0.1013\tLR: 0.000800\n",
            "Training Epoch: 200 [3200/50000]\tLoss: 0.1052\tLR: 0.000800\n",
            "Training Epoch: 200 [3328/50000]\tLoss: 0.1204\tLR: 0.000800\n",
            "Training Epoch: 200 [3456/50000]\tLoss: 0.0977\tLR: 0.000800\n",
            "Training Epoch: 200 [3584/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 200 [3712/50000]\tLoss: 0.1349\tLR: 0.000800\n",
            "Training Epoch: 200 [3840/50000]\tLoss: 0.1108\tLR: 0.000800\n",
            "Training Epoch: 200 [3968/50000]\tLoss: 0.1065\tLR: 0.000800\n",
            "Training Epoch: 200 [4096/50000]\tLoss: 0.1317\tLR: 0.000800\n",
            "Training Epoch: 200 [4224/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 200 [4352/50000]\tLoss: 0.0977\tLR: 0.000800\n",
            "Training Epoch: 200 [4480/50000]\tLoss: 0.1133\tLR: 0.000800\n",
            "Training Epoch: 200 [4608/50000]\tLoss: 0.2048\tLR: 0.000800\n",
            "Training Epoch: 200 [4736/50000]\tLoss: 0.0920\tLR: 0.000800\n",
            "Training Epoch: 200 [4864/50000]\tLoss: 0.1089\tLR: 0.000800\n",
            "Training Epoch: 200 [4992/50000]\tLoss: 0.1437\tLR: 0.000800\n",
            "Training Epoch: 200 [5120/50000]\tLoss: 0.1909\tLR: 0.000800\n",
            "Training Epoch: 200 [5248/50000]\tLoss: 0.0855\tLR: 0.000800\n",
            "Training Epoch: 200 [5376/50000]\tLoss: 0.0980\tLR: 0.000800\n",
            "Training Epoch: 200 [5504/50000]\tLoss: 0.1169\tLR: 0.000800\n",
            "Training Epoch: 200 [5632/50000]\tLoss: 0.1098\tLR: 0.000800\n",
            "Training Epoch: 200 [5760/50000]\tLoss: 0.1331\tLR: 0.000800\n",
            "Training Epoch: 200 [5888/50000]\tLoss: 0.1544\tLR: 0.000800\n",
            "Training Epoch: 200 [6016/50000]\tLoss: 0.1905\tLR: 0.000800\n",
            "Training Epoch: 200 [6144/50000]\tLoss: 0.1652\tLR: 0.000800\n",
            "Training Epoch: 200 [6272/50000]\tLoss: 0.1499\tLR: 0.000800\n",
            "Training Epoch: 200 [6400/50000]\tLoss: 0.1448\tLR: 0.000800\n",
            "Training Epoch: 200 [6528/50000]\tLoss: 0.1696\tLR: 0.000800\n",
            "Training Epoch: 200 [6656/50000]\tLoss: 0.0946\tLR: 0.000800\n",
            "Training Epoch: 200 [6784/50000]\tLoss: 0.1727\tLR: 0.000800\n",
            "Training Epoch: 200 [6912/50000]\tLoss: 0.1147\tLR: 0.000800\n",
            "Training Epoch: 200 [7040/50000]\tLoss: 0.1717\tLR: 0.000800\n",
            "Training Epoch: 200 [7168/50000]\tLoss: 0.1715\tLR: 0.000800\n",
            "Training Epoch: 200 [7296/50000]\tLoss: 0.1482\tLR: 0.000800\n",
            "Training Epoch: 200 [7424/50000]\tLoss: 0.0982\tLR: 0.000800\n",
            "Training Epoch: 200 [7552/50000]\tLoss: 0.0977\tLR: 0.000800\n",
            "Training Epoch: 200 [7680/50000]\tLoss: 0.1040\tLR: 0.000800\n",
            "Training Epoch: 200 [7808/50000]\tLoss: 0.1532\tLR: 0.000800\n",
            "Training Epoch: 200 [7936/50000]\tLoss: 0.1157\tLR: 0.000800\n",
            "Training Epoch: 200 [8064/50000]\tLoss: 0.1471\tLR: 0.000800\n",
            "Training Epoch: 200 [8192/50000]\tLoss: 0.1464\tLR: 0.000800\n",
            "Training Epoch: 200 [8320/50000]\tLoss: 0.1036\tLR: 0.000800\n",
            "Training Epoch: 200 [8448/50000]\tLoss: 0.1268\tLR: 0.000800\n",
            "Training Epoch: 200 [8576/50000]\tLoss: 0.1374\tLR: 0.000800\n",
            "Training Epoch: 200 [8704/50000]\tLoss: 0.1295\tLR: 0.000800\n",
            "Training Epoch: 200 [8832/50000]\tLoss: 0.1350\tLR: 0.000800\n",
            "Training Epoch: 200 [8960/50000]\tLoss: 0.2278\tLR: 0.000800\n",
            "Training Epoch: 200 [9088/50000]\tLoss: 0.1004\tLR: 0.000800\n",
            "Training Epoch: 200 [9216/50000]\tLoss: 0.1088\tLR: 0.000800\n",
            "Training Epoch: 200 [9344/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 200 [9472/50000]\tLoss: 0.1551\tLR: 0.000800\n",
            "Training Epoch: 200 [9600/50000]\tLoss: 0.0917\tLR: 0.000800\n",
            "Training Epoch: 200 [9728/50000]\tLoss: 0.1103\tLR: 0.000800\n",
            "Training Epoch: 200 [9856/50000]\tLoss: 0.1216\tLR: 0.000800\n",
            "Training Epoch: 200 [9984/50000]\tLoss: 0.1518\tLR: 0.000800\n",
            "Training Epoch: 200 [10112/50000]\tLoss: 0.1384\tLR: 0.000800\n",
            "Training Epoch: 200 [10240/50000]\tLoss: 0.2058\tLR: 0.000800\n",
            "Training Epoch: 200 [10368/50000]\tLoss: 0.1882\tLR: 0.000800\n",
            "Training Epoch: 200 [10496/50000]\tLoss: 0.1366\tLR: 0.000800\n",
            "Training Epoch: 200 [10624/50000]\tLoss: 0.0988\tLR: 0.000800\n",
            "Training Epoch: 200 [10752/50000]\tLoss: 0.1063\tLR: 0.000800\n",
            "Training Epoch: 200 [10880/50000]\tLoss: 0.0926\tLR: 0.000800\n",
            "Training Epoch: 200 [11008/50000]\tLoss: 0.0998\tLR: 0.000800\n",
            "Training Epoch: 200 [11136/50000]\tLoss: 0.1563\tLR: 0.000800\n",
            "Training Epoch: 200 [11264/50000]\tLoss: 0.1273\tLR: 0.000800\n",
            "Training Epoch: 200 [11392/50000]\tLoss: 0.1386\tLR: 0.000800\n",
            "Training Epoch: 200 [11520/50000]\tLoss: 0.1587\tLR: 0.000800\n",
            "Training Epoch: 200 [11648/50000]\tLoss: 0.1042\tLR: 0.000800\n",
            "Training Epoch: 200 [11776/50000]\tLoss: 0.1550\tLR: 0.000800\n",
            "Training Epoch: 200 [11904/50000]\tLoss: 0.1077\tLR: 0.000800\n",
            "Training Epoch: 200 [12032/50000]\tLoss: 0.1633\tLR: 0.000800\n",
            "Training Epoch: 200 [12160/50000]\tLoss: 0.1612\tLR: 0.000800\n",
            "Training Epoch: 200 [12288/50000]\tLoss: 0.1315\tLR: 0.000800\n",
            "Training Epoch: 200 [12416/50000]\tLoss: 0.1033\tLR: 0.000800\n",
            "Training Epoch: 200 [12544/50000]\tLoss: 0.1364\tLR: 0.000800\n",
            "Training Epoch: 200 [12672/50000]\tLoss: 0.1148\tLR: 0.000800\n",
            "Training Epoch: 200 [12800/50000]\tLoss: 0.1740\tLR: 0.000800\n",
            "Training Epoch: 200 [12928/50000]\tLoss: 0.1152\tLR: 0.000800\n",
            "Training Epoch: 200 [13056/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 200 [13184/50000]\tLoss: 0.1228\tLR: 0.000800\n",
            "Training Epoch: 200 [13312/50000]\tLoss: 0.1352\tLR: 0.000800\n",
            "Training Epoch: 200 [13440/50000]\tLoss: 0.1051\tLR: 0.000800\n",
            "Training Epoch: 200 [13568/50000]\tLoss: 0.0836\tLR: 0.000800\n",
            "Training Epoch: 200 [13696/50000]\tLoss: 0.0932\tLR: 0.000800\n",
            "Training Epoch: 200 [13824/50000]\tLoss: 0.1044\tLR: 0.000800\n",
            "Training Epoch: 200 [13952/50000]\tLoss: 0.0990\tLR: 0.000800\n",
            "Training Epoch: 200 [14080/50000]\tLoss: 0.1612\tLR: 0.000800\n",
            "Training Epoch: 200 [14208/50000]\tLoss: 0.1474\tLR: 0.000800\n",
            "Training Epoch: 200 [14336/50000]\tLoss: 0.1197\tLR: 0.000800\n",
            "Training Epoch: 200 [14464/50000]\tLoss: 0.1481\tLR: 0.000800\n",
            "Training Epoch: 200 [14592/50000]\tLoss: 0.1668\tLR: 0.000800\n",
            "Training Epoch: 200 [14720/50000]\tLoss: 0.1565\tLR: 0.000800\n",
            "Training Epoch: 200 [14848/50000]\tLoss: 0.0842\tLR: 0.000800\n",
            "Training Epoch: 200 [14976/50000]\tLoss: 0.1090\tLR: 0.000800\n",
            "Training Epoch: 200 [15104/50000]\tLoss: 0.2263\tLR: 0.000800\n",
            "Training Epoch: 200 [15232/50000]\tLoss: 0.1407\tLR: 0.000800\n",
            "Training Epoch: 200 [15360/50000]\tLoss: 0.1265\tLR: 0.000800\n",
            "Training Epoch: 200 [15488/50000]\tLoss: 0.1058\tLR: 0.000800\n",
            "Training Epoch: 200 [15616/50000]\tLoss: 0.1579\tLR: 0.000800\n",
            "Training Epoch: 200 [15744/50000]\tLoss: 0.1286\tLR: 0.000800\n",
            "Training Epoch: 200 [15872/50000]\tLoss: 0.1590\tLR: 0.000800\n",
            "Training Epoch: 200 [16000/50000]\tLoss: 0.1763\tLR: 0.000800\n",
            "Training Epoch: 200 [16128/50000]\tLoss: 0.1404\tLR: 0.000800\n",
            "Training Epoch: 200 [16256/50000]\tLoss: 0.1613\tLR: 0.000800\n",
            "Training Epoch: 200 [16384/50000]\tLoss: 0.0965\tLR: 0.000800\n",
            "Training Epoch: 200 [16512/50000]\tLoss: 0.2042\tLR: 0.000800\n",
            "Training Epoch: 200 [16640/50000]\tLoss: 0.1307\tLR: 0.000800\n",
            "Training Epoch: 200 [16768/50000]\tLoss: 0.1418\tLR: 0.000800\n",
            "Training Epoch: 200 [16896/50000]\tLoss: 0.1197\tLR: 0.000800\n",
            "Training Epoch: 200 [17024/50000]\tLoss: 0.0880\tLR: 0.000800\n",
            "Training Epoch: 200 [17152/50000]\tLoss: 0.1192\tLR: 0.000800\n",
            "Training Epoch: 200 [17280/50000]\tLoss: 0.1730\tLR: 0.000800\n",
            "Training Epoch: 200 [17408/50000]\tLoss: 0.0816\tLR: 0.000800\n",
            "Training Epoch: 200 [17536/50000]\tLoss: 0.0917\tLR: 0.000800\n",
            "Training Epoch: 200 [17664/50000]\tLoss: 0.1729\tLR: 0.000800\n",
            "Training Epoch: 200 [17792/50000]\tLoss: 0.1573\tLR: 0.000800\n",
            "Training Epoch: 200 [17920/50000]\tLoss: 0.1598\tLR: 0.000800\n",
            "Training Epoch: 200 [18048/50000]\tLoss: 0.0848\tLR: 0.000800\n",
            "Training Epoch: 200 [18176/50000]\tLoss: 0.1518\tLR: 0.000800\n",
            "Training Epoch: 200 [18304/50000]\tLoss: 0.1905\tLR: 0.000800\n",
            "Training Epoch: 200 [18432/50000]\tLoss: 0.1663\tLR: 0.000800\n",
            "Training Epoch: 200 [18560/50000]\tLoss: 0.1863\tLR: 0.000800\n",
            "Training Epoch: 200 [18688/50000]\tLoss: 0.2207\tLR: 0.000800\n",
            "Training Epoch: 200 [18816/50000]\tLoss: 0.1620\tLR: 0.000800\n",
            "Training Epoch: 200 [18944/50000]\tLoss: 0.0998\tLR: 0.000800\n",
            "Training Epoch: 200 [19072/50000]\tLoss: 0.1117\tLR: 0.000800\n",
            "Training Epoch: 200 [19200/50000]\tLoss: 0.1832\tLR: 0.000800\n",
            "Training Epoch: 200 [19328/50000]\tLoss: 0.1458\tLR: 0.000800\n",
            "Training Epoch: 200 [19456/50000]\tLoss: 0.1547\tLR: 0.000800\n",
            "Training Epoch: 200 [19584/50000]\tLoss: 0.1289\tLR: 0.000800\n",
            "Training Epoch: 200 [19712/50000]\tLoss: 0.0887\tLR: 0.000800\n",
            "Training Epoch: 200 [19840/50000]\tLoss: 0.1340\tLR: 0.000800\n",
            "Training Epoch: 200 [19968/50000]\tLoss: 0.1842\tLR: 0.000800\n",
            "Training Epoch: 200 [20096/50000]\tLoss: 0.1698\tLR: 0.000800\n",
            "Training Epoch: 200 [20224/50000]\tLoss: 0.1644\tLR: 0.000800\n",
            "Training Epoch: 200 [20352/50000]\tLoss: 0.1041\tLR: 0.000800\n",
            "Training Epoch: 200 [20480/50000]\tLoss: 0.1492\tLR: 0.000800\n",
            "Training Epoch: 200 [20608/50000]\tLoss: 0.1364\tLR: 0.000800\n",
            "Training Epoch: 200 [20736/50000]\tLoss: 0.0863\tLR: 0.000800\n",
            "Training Epoch: 200 [20864/50000]\tLoss: 0.1299\tLR: 0.000800\n",
            "Training Epoch: 200 [20992/50000]\tLoss: 0.0941\tLR: 0.000800\n",
            "Training Epoch: 200 [21120/50000]\tLoss: 0.1686\tLR: 0.000800\n",
            "Training Epoch: 200 [21248/50000]\tLoss: 0.0868\tLR: 0.000800\n",
            "Training Epoch: 200 [21376/50000]\tLoss: 0.1228\tLR: 0.000800\n",
            "Training Epoch: 200 [21504/50000]\tLoss: 0.1508\tLR: 0.000800\n",
            "Training Epoch: 200 [21632/50000]\tLoss: 0.1666\tLR: 0.000800\n",
            "Training Epoch: 200 [21760/50000]\tLoss: 0.1778\tLR: 0.000800\n",
            "Training Epoch: 200 [21888/50000]\tLoss: 0.1220\tLR: 0.000800\n",
            "Training Epoch: 200 [22016/50000]\tLoss: 0.0741\tLR: 0.000800\n",
            "Training Epoch: 200 [22144/50000]\tLoss: 0.0873\tLR: 0.000800\n",
            "Training Epoch: 200 [22272/50000]\tLoss: 0.0798\tLR: 0.000800\n",
            "Training Epoch: 200 [22400/50000]\tLoss: 0.1102\tLR: 0.000800\n",
            "Training Epoch: 200 [22528/50000]\tLoss: 0.1630\tLR: 0.000800\n",
            "Training Epoch: 200 [22656/50000]\tLoss: 0.1878\tLR: 0.000800\n",
            "Training Epoch: 200 [22784/50000]\tLoss: 0.1596\tLR: 0.000800\n",
            "Training Epoch: 200 [22912/50000]\tLoss: 0.1119\tLR: 0.000800\n",
            "Training Epoch: 200 [23040/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 200 [23168/50000]\tLoss: 0.1401\tLR: 0.000800\n",
            "Training Epoch: 200 [23296/50000]\tLoss: 0.1102\tLR: 0.000800\n",
            "Training Epoch: 200 [23424/50000]\tLoss: 0.1692\tLR: 0.000800\n",
            "Training Epoch: 200 [23552/50000]\tLoss: 0.1575\tLR: 0.000800\n",
            "Training Epoch: 200 [23680/50000]\tLoss: 0.1262\tLR: 0.000800\n",
            "Training Epoch: 200 [23808/50000]\tLoss: 0.2100\tLR: 0.000800\n",
            "Training Epoch: 200 [23936/50000]\tLoss: 0.1321\tLR: 0.000800\n",
            "Training Epoch: 200 [24064/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 200 [24192/50000]\tLoss: 0.1006\tLR: 0.000800\n",
            "Training Epoch: 200 [24320/50000]\tLoss: 0.2326\tLR: 0.000800\n",
            "Training Epoch: 200 [24448/50000]\tLoss: 0.0887\tLR: 0.000800\n",
            "Training Epoch: 200 [24576/50000]\tLoss: 0.1236\tLR: 0.000800\n",
            "Training Epoch: 200 [24704/50000]\tLoss: 0.0783\tLR: 0.000800\n",
            "Training Epoch: 200 [24832/50000]\tLoss: 0.1537\tLR: 0.000800\n",
            "Training Epoch: 200 [24960/50000]\tLoss: 0.1771\tLR: 0.000800\n",
            "Training Epoch: 200 [25088/50000]\tLoss: 0.2133\tLR: 0.000800\n",
            "Training Epoch: 200 [25216/50000]\tLoss: 0.1345\tLR: 0.000800\n",
            "Training Epoch: 200 [25344/50000]\tLoss: 0.0948\tLR: 0.000800\n",
            "Training Epoch: 200 [25472/50000]\tLoss: 0.1449\tLR: 0.000800\n",
            "Training Epoch: 200 [25600/50000]\tLoss: 0.0944\tLR: 0.000800\n",
            "Training Epoch: 200 [25728/50000]\tLoss: 0.1201\tLR: 0.000800\n",
            "Training Epoch: 200 [25856/50000]\tLoss: 0.0790\tLR: 0.000800\n",
            "Training Epoch: 200 [25984/50000]\tLoss: 0.1494\tLR: 0.000800\n",
            "Training Epoch: 200 [26112/50000]\tLoss: 0.0893\tLR: 0.000800\n",
            "Training Epoch: 200 [26240/50000]\tLoss: 0.0991\tLR: 0.000800\n",
            "Training Epoch: 200 [26368/50000]\tLoss: 0.0876\tLR: 0.000800\n",
            "Training Epoch: 200 [26496/50000]\tLoss: 0.1525\tLR: 0.000800\n",
            "Training Epoch: 200 [26624/50000]\tLoss: 0.1834\tLR: 0.000800\n",
            "Training Epoch: 200 [26752/50000]\tLoss: 0.1160\tLR: 0.000800\n",
            "Training Epoch: 200 [26880/50000]\tLoss: 0.1363\tLR: 0.000800\n",
            "Training Epoch: 200 [27008/50000]\tLoss: 0.1616\tLR: 0.000800\n",
            "Training Epoch: 200 [27136/50000]\tLoss: 0.1716\tLR: 0.000800\n",
            "Training Epoch: 200 [27264/50000]\tLoss: 0.1344\tLR: 0.000800\n",
            "Training Epoch: 200 [27392/50000]\tLoss: 0.1603\tLR: 0.000800\n",
            "Training Epoch: 200 [27520/50000]\tLoss: 0.0755\tLR: 0.000800\n",
            "Training Epoch: 200 [27648/50000]\tLoss: 0.2167\tLR: 0.000800\n",
            "Training Epoch: 200 [27776/50000]\tLoss: 0.2411\tLR: 0.000800\n",
            "Training Epoch: 200 [27904/50000]\tLoss: 0.1085\tLR: 0.000800\n",
            "Training Epoch: 200 [28032/50000]\tLoss: 0.0885\tLR: 0.000800\n",
            "Training Epoch: 200 [28160/50000]\tLoss: 0.1318\tLR: 0.000800\n",
            "Training Epoch: 200 [28288/50000]\tLoss: 0.1124\tLR: 0.000800\n",
            "Training Epoch: 200 [28416/50000]\tLoss: 0.1508\tLR: 0.000800\n",
            "Training Epoch: 200 [28544/50000]\tLoss: 0.1461\tLR: 0.000800\n",
            "Training Epoch: 200 [28672/50000]\tLoss: 0.1566\tLR: 0.000800\n",
            "Training Epoch: 200 [28800/50000]\tLoss: 0.1294\tLR: 0.000800\n",
            "Training Epoch: 200 [28928/50000]\tLoss: 0.1103\tLR: 0.000800\n",
            "Training Epoch: 200 [29056/50000]\tLoss: 0.1135\tLR: 0.000800\n",
            "Training Epoch: 200 [29184/50000]\tLoss: 0.1127\tLR: 0.000800\n",
            "Training Epoch: 200 [29312/50000]\tLoss: 0.1496\tLR: 0.000800\n",
            "Training Epoch: 200 [29440/50000]\tLoss: 0.1496\tLR: 0.000800\n",
            "Training Epoch: 200 [29568/50000]\tLoss: 0.1106\tLR: 0.000800\n",
            "Training Epoch: 200 [29696/50000]\tLoss: 0.1411\tLR: 0.000800\n",
            "Training Epoch: 200 [29824/50000]\tLoss: 0.1863\tLR: 0.000800\n",
            "Training Epoch: 200 [29952/50000]\tLoss: 0.1778\tLR: 0.000800\n",
            "Training Epoch: 200 [30080/50000]\tLoss: 0.1229\tLR: 0.000800\n",
            "Training Epoch: 200 [30208/50000]\tLoss: 0.1474\tLR: 0.000800\n",
            "Training Epoch: 200 [30336/50000]\tLoss: 0.1535\tLR: 0.000800\n",
            "Training Epoch: 200 [30464/50000]\tLoss: 0.1551\tLR: 0.000800\n",
            "Training Epoch: 200 [30592/50000]\tLoss: 0.1609\tLR: 0.000800\n",
            "Training Epoch: 200 [30720/50000]\tLoss: 0.2319\tLR: 0.000800\n",
            "Training Epoch: 200 [30848/50000]\tLoss: 0.1454\tLR: 0.000800\n",
            "Training Epoch: 200 [30976/50000]\tLoss: 0.1095\tLR: 0.000800\n",
            "Training Epoch: 200 [31104/50000]\tLoss: 0.1242\tLR: 0.000800\n",
            "Training Epoch: 200 [31232/50000]\tLoss: 0.1090\tLR: 0.000800\n",
            "Training Epoch: 200 [31360/50000]\tLoss: 0.1299\tLR: 0.000800\n",
            "Training Epoch: 200 [31488/50000]\tLoss: 0.1308\tLR: 0.000800\n",
            "Training Epoch: 200 [31616/50000]\tLoss: 0.1142\tLR: 0.000800\n",
            "Training Epoch: 200 [31744/50000]\tLoss: 0.2148\tLR: 0.000800\n",
            "Training Epoch: 200 [31872/50000]\tLoss: 0.1358\tLR: 0.000800\n",
            "Training Epoch: 200 [32000/50000]\tLoss: 0.1500\tLR: 0.000800\n",
            "Training Epoch: 200 [32128/50000]\tLoss: 0.0743\tLR: 0.000800\n",
            "Training Epoch: 200 [32256/50000]\tLoss: 0.1261\tLR: 0.000800\n",
            "Training Epoch: 200 [32384/50000]\tLoss: 0.1278\tLR: 0.000800\n",
            "Training Epoch: 200 [32512/50000]\tLoss: 0.2166\tLR: 0.000800\n",
            "Training Epoch: 200 [32640/50000]\tLoss: 0.1158\tLR: 0.000800\n",
            "Training Epoch: 200 [32768/50000]\tLoss: 0.1482\tLR: 0.000800\n",
            "Training Epoch: 200 [32896/50000]\tLoss: 0.1181\tLR: 0.000800\n",
            "Training Epoch: 200 [33024/50000]\tLoss: 0.1939\tLR: 0.000800\n",
            "Training Epoch: 200 [33152/50000]\tLoss: 0.1448\tLR: 0.000800\n",
            "Training Epoch: 200 [33280/50000]\tLoss: 0.1475\tLR: 0.000800\n",
            "Training Epoch: 200 [33408/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 200 [33536/50000]\tLoss: 0.1934\tLR: 0.000800\n",
            "Training Epoch: 200 [33664/50000]\tLoss: 0.1093\tLR: 0.000800\n",
            "Training Epoch: 200 [33792/50000]\tLoss: 0.1609\tLR: 0.000800\n",
            "Training Epoch: 200 [33920/50000]\tLoss: 0.1185\tLR: 0.000800\n",
            "Training Epoch: 200 [34048/50000]\tLoss: 0.1525\tLR: 0.000800\n",
            "Training Epoch: 200 [34176/50000]\tLoss: 0.1285\tLR: 0.000800\n",
            "Training Epoch: 200 [34304/50000]\tLoss: 0.0765\tLR: 0.000800\n",
            "Training Epoch: 200 [34432/50000]\tLoss: 0.0930\tLR: 0.000800\n",
            "Training Epoch: 200 [34560/50000]\tLoss: 0.1362\tLR: 0.000800\n",
            "Training Epoch: 200 [34688/50000]\tLoss: 0.1187\tLR: 0.000800\n",
            "Training Epoch: 200 [34816/50000]\tLoss: 0.1255\tLR: 0.000800\n",
            "Training Epoch: 200 [34944/50000]\tLoss: 0.1168\tLR: 0.000800\n",
            "Training Epoch: 200 [35072/50000]\tLoss: 0.1365\tLR: 0.000800\n",
            "Training Epoch: 200 [35200/50000]\tLoss: 0.1370\tLR: 0.000800\n",
            "Training Epoch: 200 [35328/50000]\tLoss: 0.1405\tLR: 0.000800\n",
            "Training Epoch: 200 [35456/50000]\tLoss: 0.1161\tLR: 0.000800\n",
            "Training Epoch: 200 [35584/50000]\tLoss: 0.1324\tLR: 0.000800\n",
            "Training Epoch: 200 [35712/50000]\tLoss: 0.1652\tLR: 0.000800\n",
            "Training Epoch: 200 [35840/50000]\tLoss: 0.1190\tLR: 0.000800\n",
            "Training Epoch: 200 [35968/50000]\tLoss: 0.1302\tLR: 0.000800\n",
            "Training Epoch: 200 [36096/50000]\tLoss: 0.0973\tLR: 0.000800\n",
            "Training Epoch: 200 [36224/50000]\tLoss: 0.1162\tLR: 0.000800\n",
            "Training Epoch: 200 [36352/50000]\tLoss: 0.1200\tLR: 0.000800\n",
            "Training Epoch: 200 [36480/50000]\tLoss: 0.0887\tLR: 0.000800\n",
            "Training Epoch: 200 [36608/50000]\tLoss: 0.1666\tLR: 0.000800\n",
            "Training Epoch: 200 [36736/50000]\tLoss: 0.1584\tLR: 0.000800\n",
            "Training Epoch: 200 [36864/50000]\tLoss: 0.1690\tLR: 0.000800\n",
            "Training Epoch: 200 [36992/50000]\tLoss: 0.1537\tLR: 0.000800\n",
            "Training Epoch: 200 [37120/50000]\tLoss: 0.1103\tLR: 0.000800\n",
            "Training Epoch: 200 [37248/50000]\tLoss: 0.1083\tLR: 0.000800\n",
            "Training Epoch: 200 [37376/50000]\tLoss: 0.1196\tLR: 0.000800\n",
            "Training Epoch: 200 [37504/50000]\tLoss: 0.1059\tLR: 0.000800\n",
            "Training Epoch: 200 [37632/50000]\tLoss: 0.1063\tLR: 0.000800\n",
            "Training Epoch: 200 [37760/50000]\tLoss: 0.1578\tLR: 0.000800\n",
            "Training Epoch: 200 [37888/50000]\tLoss: 0.1153\tLR: 0.000800\n",
            "Training Epoch: 200 [38016/50000]\tLoss: 0.2166\tLR: 0.000800\n",
            "Training Epoch: 200 [38144/50000]\tLoss: 0.1347\tLR: 0.000800\n",
            "Training Epoch: 200 [38272/50000]\tLoss: 0.1075\tLR: 0.000800\n",
            "Training Epoch: 200 [38400/50000]\tLoss: 0.1130\tLR: 0.000800\n",
            "Training Epoch: 200 [38528/50000]\tLoss: 0.1720\tLR: 0.000800\n",
            "Training Epoch: 200 [38656/50000]\tLoss: 0.0669\tLR: 0.000800\n",
            "Training Epoch: 200 [38784/50000]\tLoss: 0.2064\tLR: 0.000800\n",
            "Training Epoch: 200 [38912/50000]\tLoss: 0.1500\tLR: 0.000800\n",
            "Training Epoch: 200 [39040/50000]\tLoss: 0.1171\tLR: 0.000800\n",
            "Training Epoch: 200 [39168/50000]\tLoss: 0.1392\tLR: 0.000800\n",
            "Training Epoch: 200 [39296/50000]\tLoss: 0.1985\tLR: 0.000800\n",
            "Training Epoch: 200 [39424/50000]\tLoss: 0.1136\tLR: 0.000800\n",
            "Training Epoch: 200 [39552/50000]\tLoss: 0.1107\tLR: 0.000800\n",
            "Training Epoch: 200 [39680/50000]\tLoss: 0.1077\tLR: 0.000800\n",
            "Training Epoch: 200 [39808/50000]\tLoss: 0.0940\tLR: 0.000800\n",
            "Training Epoch: 200 [39936/50000]\tLoss: 0.1435\tLR: 0.000800\n",
            "Training Epoch: 200 [40064/50000]\tLoss: 0.0663\tLR: 0.000800\n",
            "Training Epoch: 200 [40192/50000]\tLoss: 0.1423\tLR: 0.000800\n",
            "Training Epoch: 200 [40320/50000]\tLoss: 0.1765\tLR: 0.000800\n",
            "Training Epoch: 200 [40448/50000]\tLoss: 0.1547\tLR: 0.000800\n",
            "Training Epoch: 200 [40576/50000]\tLoss: 0.2108\tLR: 0.000800\n",
            "Training Epoch: 200 [40704/50000]\tLoss: 0.1551\tLR: 0.000800\n",
            "Training Epoch: 200 [40832/50000]\tLoss: 0.1432\tLR: 0.000800\n",
            "Training Epoch: 200 [40960/50000]\tLoss: 0.1397\tLR: 0.000800\n",
            "Training Epoch: 200 [41088/50000]\tLoss: 0.0864\tLR: 0.000800\n",
            "Training Epoch: 200 [41216/50000]\tLoss: 0.1909\tLR: 0.000800\n",
            "Training Epoch: 200 [41344/50000]\tLoss: 0.1483\tLR: 0.000800\n",
            "Training Epoch: 200 [41472/50000]\tLoss: 0.1264\tLR: 0.000800\n",
            "Training Epoch: 200 [41600/50000]\tLoss: 0.1410\tLR: 0.000800\n",
            "Training Epoch: 200 [41728/50000]\tLoss: 0.1423\tLR: 0.000800\n",
            "Training Epoch: 200 [41856/50000]\tLoss: 0.1244\tLR: 0.000800\n",
            "Training Epoch: 200 [41984/50000]\tLoss: 0.1940\tLR: 0.000800\n",
            "Training Epoch: 200 [42112/50000]\tLoss: 0.1644\tLR: 0.000800\n",
            "Training Epoch: 200 [42240/50000]\tLoss: 0.1545\tLR: 0.000800\n",
            "Training Epoch: 200 [42368/50000]\tLoss: 0.1643\tLR: 0.000800\n",
            "Training Epoch: 200 [42496/50000]\tLoss: 0.1285\tLR: 0.000800\n",
            "Training Epoch: 200 [42624/50000]\tLoss: 0.1197\tLR: 0.000800\n",
            "Training Epoch: 200 [42752/50000]\tLoss: 0.1074\tLR: 0.000800\n",
            "Training Epoch: 200 [42880/50000]\tLoss: 0.1231\tLR: 0.000800\n",
            "Training Epoch: 200 [43008/50000]\tLoss: 0.1083\tLR: 0.000800\n",
            "Training Epoch: 200 [43136/50000]\tLoss: 0.1440\tLR: 0.000800\n",
            "Training Epoch: 200 [43264/50000]\tLoss: 0.1330\tLR: 0.000800\n",
            "Training Epoch: 200 [43392/50000]\tLoss: 0.1194\tLR: 0.000800\n",
            "Training Epoch: 200 [43520/50000]\tLoss: 0.1206\tLR: 0.000800\n",
            "Training Epoch: 200 [43648/50000]\tLoss: 0.1194\tLR: 0.000800\n",
            "Training Epoch: 200 [43776/50000]\tLoss: 0.1132\tLR: 0.000800\n",
            "Training Epoch: 200 [43904/50000]\tLoss: 0.1663\tLR: 0.000800\n",
            "Training Epoch: 200 [44032/50000]\tLoss: 0.1175\tLR: 0.000800\n",
            "Training Epoch: 200 [44160/50000]\tLoss: 0.1260\tLR: 0.000800\n",
            "Training Epoch: 200 [44288/50000]\tLoss: 0.1209\tLR: 0.000800\n",
            "Training Epoch: 200 [44416/50000]\tLoss: 0.1659\tLR: 0.000800\n",
            "Training Epoch: 200 [44544/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 200 [44672/50000]\tLoss: 0.1234\tLR: 0.000800\n",
            "Training Epoch: 200 [44800/50000]\tLoss: 0.1473\tLR: 0.000800\n",
            "Training Epoch: 200 [44928/50000]\tLoss: 0.1928\tLR: 0.000800\n",
            "Training Epoch: 200 [45056/50000]\tLoss: 0.1630\tLR: 0.000800\n",
            "Training Epoch: 200 [45184/50000]\tLoss: 0.1639\tLR: 0.000800\n",
            "Training Epoch: 200 [45312/50000]\tLoss: 0.1331\tLR: 0.000800\n",
            "Training Epoch: 200 [45440/50000]\tLoss: 0.1341\tLR: 0.000800\n",
            "Training Epoch: 200 [45568/50000]\tLoss: 0.1243\tLR: 0.000800\n",
            "Training Epoch: 200 [45696/50000]\tLoss: 0.1938\tLR: 0.000800\n",
            "Training Epoch: 200 [45824/50000]\tLoss: 0.1607\tLR: 0.000800\n",
            "Training Epoch: 200 [45952/50000]\tLoss: 0.1547\tLR: 0.000800\n",
            "Training Epoch: 200 [46080/50000]\tLoss: 0.1060\tLR: 0.000800\n",
            "Training Epoch: 200 [46208/50000]\tLoss: 0.1549\tLR: 0.000800\n",
            "Training Epoch: 200 [46336/50000]\tLoss: 0.1296\tLR: 0.000800\n",
            "Training Epoch: 200 [46464/50000]\tLoss: 0.1612\tLR: 0.000800\n",
            "Training Epoch: 200 [46592/50000]\tLoss: 0.0965\tLR: 0.000800\n",
            "Training Epoch: 200 [46720/50000]\tLoss: 0.0886\tLR: 0.000800\n",
            "Training Epoch: 200 [46848/50000]\tLoss: 0.1233\tLR: 0.000800\n",
            "Training Epoch: 200 [46976/50000]\tLoss: 0.1473\tLR: 0.000800\n",
            "Training Epoch: 200 [47104/50000]\tLoss: 0.1416\tLR: 0.000800\n",
            "Training Epoch: 200 [47232/50000]\tLoss: 0.1762\tLR: 0.000800\n",
            "Training Epoch: 200 [47360/50000]\tLoss: 0.1200\tLR: 0.000800\n",
            "Training Epoch: 200 [47488/50000]\tLoss: 0.1988\tLR: 0.000800\n",
            "Training Epoch: 200 [47616/50000]\tLoss: 0.1283\tLR: 0.000800\n",
            "Training Epoch: 200 [47744/50000]\tLoss: 0.0943\tLR: 0.000800\n",
            "Training Epoch: 200 [47872/50000]\tLoss: 0.0877\tLR: 0.000800\n",
            "Training Epoch: 200 [48000/50000]\tLoss: 0.1603\tLR: 0.000800\n",
            "Training Epoch: 200 [48128/50000]\tLoss: 0.1127\tLR: 0.000800\n",
            "Training Epoch: 200 [48256/50000]\tLoss: 0.1589\tLR: 0.000800\n",
            "Training Epoch: 200 [48384/50000]\tLoss: 0.1195\tLR: 0.000800\n",
            "Training Epoch: 200 [48512/50000]\tLoss: 0.1640\tLR: 0.000800\n",
            "Training Epoch: 200 [48640/50000]\tLoss: 0.0975\tLR: 0.000800\n",
            "Training Epoch: 200 [48768/50000]\tLoss: 0.1458\tLR: 0.000800\n",
            "Training Epoch: 200 [48896/50000]\tLoss: 0.1983\tLR: 0.000800\n",
            "Training Epoch: 200 [49024/50000]\tLoss: 0.1269\tLR: 0.000800\n",
            "Training Epoch: 200 [49152/50000]\tLoss: 0.1504\tLR: 0.000800\n",
            "Training Epoch: 200 [49280/50000]\tLoss: 0.1910\tLR: 0.000800\n",
            "Training Epoch: 200 [49408/50000]\tLoss: 0.1631\tLR: 0.000800\n",
            "Training Epoch: 200 [49536/50000]\tLoss: 0.0936\tLR: 0.000800\n",
            "Training Epoch: 200 [49664/50000]\tLoss: 0.1502\tLR: 0.000800\n",
            "Training Epoch: 200 [49792/50000]\tLoss: 0.0743\tLR: 0.000800\n",
            "Training Epoch: 200 [49920/50000]\tLoss: 0.1372\tLR: 0.000800\n",
            "Training Epoch: 200 [50000/50000]\tLoss: 0.1145\tLR: 0.000800\n",
            "epoch 200 training time consumed: 28.52s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  55819 KiB | 461512 KiB |  90004 GiB |  90004 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  88291 GiB |  88291 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1713 GiB |   1713 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  55819 KiB | 461512 KiB |  90004 GiB |  90004 GiB |\n",
            "|       from large pool |  35072 KiB | 445184 KiB |  88291 GiB |  88291 GiB |\n",
            "|       from small pool |  20747 KiB |  28349 KiB |   1713 GiB |   1713 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  55788 KiB | 460460 KiB |  89737 GiB |  89737 GiB |\n",
            "|       from large pool |  35072 KiB | 444160 KiB |  88025 GiB |  88025 GiB |\n",
            "|       from small pool |  20716 KiB |  28317 KiB |   1711 GiB |   1711 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 550912 KiB | 550912 KiB | 550912 KiB |      0 B   |\n",
            "|       from large pool | 520192 KiB | 520192 KiB | 520192 KiB |      0 B   |\n",
            "|       from small pool |  30720 KiB |  30720 KiB |  30720 KiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  30197 KiB |  89171 KiB |  55063 GiB |  55063 GiB |\n",
            "|       from large pool |  26368 KiB |  86016 KiB |  53295 GiB |  53295 GiB |\n",
            "|       from small pool |   3829 KiB |   6743 KiB |   1767 GiB |   1767 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     381    |     474    |   34875 K  |   34874 K  |\n",
            "|       from large pool |       8    |      61    |   11699 K  |   11699 K  |\n",
            "|       from small pool |     373    |     464    |   23175 K  |   23174 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     381    |     474    |   34875 K  |   34874 K  |\n",
            "|       from large pool |       8    |      61    |   11699 K  |   11699 K  |\n",
            "|       from small pool |     373    |     464    |   23175 K  |   23174 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      40    |      40    |      40    |       0    |\n",
            "|       from large pool |      25    |      25    |      25    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      15    |      24    |   11464 K  |   11464 K  |\n",
            "|       from large pool |       5    |      14    |    5577 K  |    5577 K  |\n",
            "|       from small pool |      10    |      16    |    5886 K  |    5886 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 200, Average loss: 0.0114, Accuracy: 0.6651, Time consumed:3.07s\n",
            "\n",
            "saving weights file to checkpoint/mobilenet/Wednesday_01_May_2024_19h_23m_52s/mobilenet-200-regular.pth\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/Knowledge-Distillation/train.py -net mobilenet -gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoBGre50Kfe"
      },
      "source": [
        "## Testing with CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Maq9faBGQnpD",
        "outputId": "d0a6410f-8b08-46e5-9d39-c09d963cfe9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n",
            "100% 169001437/169001437 [00:03<00:00, 45784404.78it/s]\n",
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "MobileNet(\n",
            "  (stem): Sequential(\n",
            "    (0): BasicConv2d(\n",
            "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1): Sequential(\n",
            "    (0): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (3): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (4): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=1024, out_features=100, bias=True)\n",
            "  (avg): AdaptiveAvgPool2d(output_size=1)\n",
            ")\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "iteration: 1 \t total 625 iterations\n",
            "iteration: 2 \t total 625 iterations\n",
            "iteration: 3 \t total 625 iterations\n",
            "iteration: 4 \t total 625 iterations\n",
            "iteration: 5 \t total 625 iterations\n",
            "iteration: 6 \t total 625 iterations\n",
            "iteration: 7 \t total 625 iterations\n",
            "iteration: 8 \t total 625 iterations\n",
            "iteration: 9 \t total 625 iterations\n",
            "iteration: 10 \t total 625 iterations\n",
            "iteration: 11 \t total 625 iterations\n",
            "iteration: 12 \t total 625 iterations\n",
            "iteration: 13 \t total 625 iterations\n",
            "iteration: 14 \t total 625 iterations\n",
            "iteration: 15 \t total 625 iterations\n",
            "iteration: 16 \t total 625 iterations\n",
            "iteration: 17 \t total 625 iterations\n",
            "iteration: 18 \t total 625 iterations\n",
            "iteration: 19 \t total 625 iterations\n",
            "iteration: 20 \t total 625 iterations\n",
            "iteration: 21 \t total 625 iterations\n",
            "iteration: 22 \t total 625 iterations\n",
            "iteration: 23 \t total 625 iterations\n",
            "iteration: 24 \t total 625 iterations\n",
            "iteration: 25 \t total 625 iterations\n",
            "iteration: 26 \t total 625 iterations\n",
            "iteration: 27 \t total 625 iterations\n",
            "iteration: 28 \t total 625 iterations\n",
            "iteration: 29 \t total 625 iterations\n",
            "iteration: 30 \t total 625 iterations\n",
            "iteration: 31 \t total 625 iterations\n",
            "iteration: 32 \t total 625 iterations\n",
            "iteration: 33 \t total 625 iterations\n",
            "iteration: 34 \t total 625 iterations\n",
            "iteration: 35 \t total 625 iterations\n",
            "iteration: 36 \t total 625 iterations\n",
            "iteration: 37 \t total 625 iterations\n",
            "iteration: 38 \t total 625 iterations\n",
            "iteration: 39 \t total 625 iterations\n",
            "iteration: 40 \t total 625 iterations\n",
            "iteration: 41 \t total 625 iterations\n",
            "iteration: 42 \t total 625 iterations\n",
            "iteration: 43 \t total 625 iterations\n",
            "iteration: 44 \t total 625 iterations\n",
            "iteration: 45 \t total 625 iterations\n",
            "iteration: 46 \t total 625 iterations\n",
            "iteration: 47 \t total 625 iterations\n",
            "iteration: 48 \t total 625 iterations\n",
            "iteration: 49 \t total 625 iterations\n",
            "iteration: 50 \t total 625 iterations\n",
            "iteration: 51 \t total 625 iterations\n",
            "iteration: 52 \t total 625 iterations\n",
            "iteration: 53 \t total 625 iterations\n",
            "iteration: 54 \t total 625 iterations\n",
            "iteration: 55 \t total 625 iterations\n",
            "iteration: 56 \t total 625 iterations\n",
            "iteration: 57 \t total 625 iterations\n",
            "iteration: 58 \t total 625 iterations\n",
            "iteration: 59 \t total 625 iterations\n",
            "iteration: 60 \t total 625 iterations\n",
            "iteration: 61 \t total 625 iterations\n",
            "iteration: 62 \t total 625 iterations\n",
            "iteration: 63 \t total 625 iterations\n",
            "iteration: 64 \t total 625 iterations\n",
            "iteration: 65 \t total 625 iterations\n",
            "iteration: 66 \t total 625 iterations\n",
            "iteration: 67 \t total 625 iterations\n",
            "iteration: 68 \t total 625 iterations\n",
            "iteration: 69 \t total 625 iterations\n",
            "iteration: 70 \t total 625 iterations\n",
            "iteration: 71 \t total 625 iterations\n",
            "iteration: 72 \t total 625 iterations\n",
            "iteration: 73 \t total 625 iterations\n",
            "iteration: 74 \t total 625 iterations\n",
            "iteration: 75 \t total 625 iterations\n",
            "iteration: 76 \t total 625 iterations\n",
            "iteration: 77 \t total 625 iterations\n",
            "iteration: 78 \t total 625 iterations\n",
            "iteration: 79 \t total 625 iterations\n",
            "iteration: 80 \t total 625 iterations\n",
            "iteration: 81 \t total 625 iterations\n",
            "iteration: 82 \t total 625 iterations\n",
            "iteration: 83 \t total 625 iterations\n",
            "iteration: 84 \t total 625 iterations\n",
            "iteration: 85 \t total 625 iterations\n",
            "iteration: 86 \t total 625 iterations\n",
            "iteration: 87 \t total 625 iterations\n",
            "iteration: 88 \t total 625 iterations\n",
            "iteration: 89 \t total 625 iterations\n",
            "iteration: 90 \t total 625 iterations\n",
            "iteration: 91 \t total 625 iterations\n",
            "iteration: 92 \t total 625 iterations\n",
            "iteration: 93 \t total 625 iterations\n",
            "iteration: 94 \t total 625 iterations\n",
            "iteration: 95 \t total 625 iterations\n",
            "iteration: 96 \t total 625 iterations\n",
            "iteration: 97 \t total 625 iterations\n",
            "iteration: 98 \t total 625 iterations\n",
            "iteration: 99 \t total 625 iterations\n",
            "iteration: 100 \t total 625 iterations\n",
            "iteration: 101 \t total 625 iterations\n",
            "iteration: 102 \t total 625 iterations\n",
            "iteration: 103 \t total 625 iterations\n",
            "iteration: 104 \t total 625 iterations\n",
            "iteration: 105 \t total 625 iterations\n",
            "iteration: 106 \t total 625 iterations\n",
            "iteration: 107 \t total 625 iterations\n",
            "iteration: 108 \t total 625 iterations\n",
            "iteration: 109 \t total 625 iterations\n",
            "iteration: 110 \t total 625 iterations\n",
            "iteration: 111 \t total 625 iterations\n",
            "iteration: 112 \t total 625 iterations\n",
            "iteration: 113 \t total 625 iterations\n",
            "iteration: 114 \t total 625 iterations\n",
            "iteration: 115 \t total 625 iterations\n",
            "iteration: 116 \t total 625 iterations\n",
            "iteration: 117 \t total 625 iterations\n",
            "iteration: 118 \t total 625 iterations\n",
            "iteration: 119 \t total 625 iterations\n",
            "iteration: 120 \t total 625 iterations\n",
            "iteration: 121 \t total 625 iterations\n",
            "iteration: 122 \t total 625 iterations\n",
            "iteration: 123 \t total 625 iterations\n",
            "iteration: 124 \t total 625 iterations\n",
            "iteration: 125 \t total 625 iterations\n",
            "iteration: 126 \t total 625 iterations\n",
            "iteration: 127 \t total 625 iterations\n",
            "iteration: 128 \t total 625 iterations\n",
            "iteration: 129 \t total 625 iterations\n",
            "iteration: 130 \t total 625 iterations\n",
            "iteration: 131 \t total 625 iterations\n",
            "iteration: 132 \t total 625 iterations\n",
            "iteration: 133 \t total 625 iterations\n",
            "iteration: 134 \t total 625 iterations\n",
            "iteration: 135 \t total 625 iterations\n",
            "iteration: 136 \t total 625 iterations\n",
            "iteration: 137 \t total 625 iterations\n",
            "iteration: 138 \t total 625 iterations\n",
            "iteration: 139 \t total 625 iterations\n",
            "iteration: 140 \t total 625 iterations\n",
            "iteration: 141 \t total 625 iterations\n",
            "iteration: 142 \t total 625 iterations\n",
            "iteration: 143 \t total 625 iterations\n",
            "iteration: 144 \t total 625 iterations\n",
            "iteration: 145 \t total 625 iterations\n",
            "iteration: 146 \t total 625 iterations\n",
            "iteration: 147 \t total 625 iterations\n",
            "iteration: 148 \t total 625 iterations\n",
            "iteration: 149 \t total 625 iterations\n",
            "iteration: 150 \t total 625 iterations\n",
            "iteration: 151 \t total 625 iterations\n",
            "iteration: 152 \t total 625 iterations\n",
            "iteration: 153 \t total 625 iterations\n",
            "iteration: 154 \t total 625 iterations\n",
            "iteration: 155 \t total 625 iterations\n",
            "iteration: 156 \t total 625 iterations\n",
            "iteration: 157 \t total 625 iterations\n",
            "iteration: 158 \t total 625 iterations\n",
            "iteration: 159 \t total 625 iterations\n",
            "iteration: 160 \t total 625 iterations\n",
            "iteration: 161 \t total 625 iterations\n",
            "iteration: 162 \t total 625 iterations\n",
            "iteration: 163 \t total 625 iterations\n",
            "iteration: 164 \t total 625 iterations\n",
            "iteration: 165 \t total 625 iterations\n",
            "iteration: 166 \t total 625 iterations\n",
            "iteration: 167 \t total 625 iterations\n",
            "iteration: 168 \t total 625 iterations\n",
            "iteration: 169 \t total 625 iterations\n",
            "iteration: 170 \t total 625 iterations\n",
            "iteration: 171 \t total 625 iterations\n",
            "iteration: 172 \t total 625 iterations\n",
            "iteration: 173 \t total 625 iterations\n",
            "iteration: 174 \t total 625 iterations\n",
            "iteration: 175 \t total 625 iterations\n",
            "iteration: 176 \t total 625 iterations\n",
            "iteration: 177 \t total 625 iterations\n",
            "iteration: 178 \t total 625 iterations\n",
            "iteration: 179 \t total 625 iterations\n",
            "iteration: 180 \t total 625 iterations\n",
            "iteration: 181 \t total 625 iterations\n",
            "iteration: 182 \t total 625 iterations\n",
            "iteration: 183 \t total 625 iterations\n",
            "iteration: 184 \t total 625 iterations\n",
            "iteration: 185 \t total 625 iterations\n",
            "iteration: 186 \t total 625 iterations\n",
            "iteration: 187 \t total 625 iterations\n",
            "iteration: 188 \t total 625 iterations\n",
            "iteration: 189 \t total 625 iterations\n",
            "iteration: 190 \t total 625 iterations\n",
            "iteration: 191 \t total 625 iterations\n",
            "iteration: 192 \t total 625 iterations\n",
            "iteration: 193 \t total 625 iterations\n",
            "iteration: 194 \t total 625 iterations\n",
            "iteration: 195 \t total 625 iterations\n",
            "iteration: 196 \t total 625 iterations\n",
            "iteration: 197 \t total 625 iterations\n",
            "iteration: 198 \t total 625 iterations\n",
            "iteration: 199 \t total 625 iterations\n",
            "iteration: 200 \t total 625 iterations\n",
            "iteration: 201 \t total 625 iterations\n",
            "iteration: 202 \t total 625 iterations\n",
            "iteration: 203 \t total 625 iterations\n",
            "iteration: 204 \t total 625 iterations\n",
            "iteration: 205 \t total 625 iterations\n",
            "iteration: 206 \t total 625 iterations\n",
            "iteration: 207 \t total 625 iterations\n",
            "iteration: 208 \t total 625 iterations\n",
            "iteration: 209 \t total 625 iterations\n",
            "iteration: 210 \t total 625 iterations\n",
            "iteration: 211 \t total 625 iterations\n",
            "iteration: 212 \t total 625 iterations\n",
            "iteration: 213 \t total 625 iterations\n",
            "iteration: 214 \t total 625 iterations\n",
            "iteration: 215 \t total 625 iterations\n",
            "iteration: 216 \t total 625 iterations\n",
            "iteration: 217 \t total 625 iterations\n",
            "iteration: 218 \t total 625 iterations\n",
            "iteration: 219 \t total 625 iterations\n",
            "iteration: 220 \t total 625 iterations\n",
            "iteration: 221 \t total 625 iterations\n",
            "iteration: 222 \t total 625 iterations\n",
            "iteration: 223 \t total 625 iterations\n",
            "iteration: 224 \t total 625 iterations\n",
            "iteration: 225 \t total 625 iterations\n",
            "iteration: 226 \t total 625 iterations\n",
            "iteration: 227 \t total 625 iterations\n",
            "iteration: 228 \t total 625 iterations\n",
            "iteration: 229 \t total 625 iterations\n",
            "iteration: 230 \t total 625 iterations\n",
            "iteration: 231 \t total 625 iterations\n",
            "iteration: 232 \t total 625 iterations\n",
            "iteration: 233 \t total 625 iterations\n",
            "iteration: 234 \t total 625 iterations\n",
            "iteration: 235 \t total 625 iterations\n",
            "iteration: 236 \t total 625 iterations\n",
            "iteration: 237 \t total 625 iterations\n",
            "iteration: 238 \t total 625 iterations\n",
            "iteration: 239 \t total 625 iterations\n",
            "iteration: 240 \t total 625 iterations\n",
            "iteration: 241 \t total 625 iterations\n",
            "iteration: 242 \t total 625 iterations\n",
            "iteration: 243 \t total 625 iterations\n",
            "iteration: 244 \t total 625 iterations\n",
            "iteration: 245 \t total 625 iterations\n",
            "iteration: 246 \t total 625 iterations\n",
            "iteration: 247 \t total 625 iterations\n",
            "iteration: 248 \t total 625 iterations\n",
            "iteration: 249 \t total 625 iterations\n",
            "iteration: 250 \t total 625 iterations\n",
            "iteration: 251 \t total 625 iterations\n",
            "iteration: 252 \t total 625 iterations\n",
            "iteration: 253 \t total 625 iterations\n",
            "iteration: 254 \t total 625 iterations\n",
            "iteration: 255 \t total 625 iterations\n",
            "iteration: 256 \t total 625 iterations\n",
            "iteration: 257 \t total 625 iterations\n",
            "iteration: 258 \t total 625 iterations\n",
            "iteration: 259 \t total 625 iterations\n",
            "iteration: 260 \t total 625 iterations\n",
            "iteration: 261 \t total 625 iterations\n",
            "iteration: 262 \t total 625 iterations\n",
            "iteration: 263 \t total 625 iterations\n",
            "iteration: 264 \t total 625 iterations\n",
            "iteration: 265 \t total 625 iterations\n",
            "iteration: 266 \t total 625 iterations\n",
            "iteration: 267 \t total 625 iterations\n",
            "iteration: 268 \t total 625 iterations\n",
            "iteration: 269 \t total 625 iterations\n",
            "iteration: 270 \t total 625 iterations\n",
            "iteration: 271 \t total 625 iterations\n",
            "iteration: 272 \t total 625 iterations\n",
            "iteration: 273 \t total 625 iterations\n",
            "iteration: 274 \t total 625 iterations\n",
            "iteration: 275 \t total 625 iterations\n",
            "iteration: 276 \t total 625 iterations\n",
            "iteration: 277 \t total 625 iterations\n",
            "iteration: 278 \t total 625 iterations\n",
            "iteration: 279 \t total 625 iterations\n",
            "iteration: 280 \t total 625 iterations\n",
            "iteration: 281 \t total 625 iterations\n",
            "iteration: 282 \t total 625 iterations\n",
            "iteration: 283 \t total 625 iterations\n",
            "iteration: 284 \t total 625 iterations\n",
            "iteration: 285 \t total 625 iterations\n",
            "iteration: 286 \t total 625 iterations\n",
            "iteration: 287 \t total 625 iterations\n",
            "iteration: 288 \t total 625 iterations\n",
            "iteration: 289 \t total 625 iterations\n",
            "iteration: 290 \t total 625 iterations\n",
            "iteration: 291 \t total 625 iterations\n",
            "iteration: 292 \t total 625 iterations\n",
            "iteration: 293 \t total 625 iterations\n",
            "iteration: 294 \t total 625 iterations\n",
            "iteration: 295 \t total 625 iterations\n",
            "iteration: 296 \t total 625 iterations\n",
            "iteration: 297 \t total 625 iterations\n",
            "iteration: 298 \t total 625 iterations\n",
            "iteration: 299 \t total 625 iterations\n",
            "iteration: 300 \t total 625 iterations\n",
            "iteration: 301 \t total 625 iterations\n",
            "iteration: 302 \t total 625 iterations\n",
            "iteration: 303 \t total 625 iterations\n",
            "iteration: 304 \t total 625 iterations\n",
            "iteration: 305 \t total 625 iterations\n",
            "iteration: 306 \t total 625 iterations\n",
            "iteration: 307 \t total 625 iterations\n",
            "iteration: 308 \t total 625 iterations\n",
            "iteration: 309 \t total 625 iterations\n",
            "iteration: 310 \t total 625 iterations\n",
            "iteration: 311 \t total 625 iterations\n",
            "iteration: 312 \t total 625 iterations\n",
            "iteration: 313 \t total 625 iterations\n",
            "iteration: 314 \t total 625 iterations\n",
            "iteration: 315 \t total 625 iterations\n",
            "iteration: 316 \t total 625 iterations\n",
            "iteration: 317 \t total 625 iterations\n",
            "iteration: 318 \t total 625 iterations\n",
            "iteration: 319 \t total 625 iterations\n",
            "iteration: 320 \t total 625 iterations\n",
            "iteration: 321 \t total 625 iterations\n",
            "iteration: 322 \t total 625 iterations\n",
            "iteration: 323 \t total 625 iterations\n",
            "iteration: 324 \t total 625 iterations\n",
            "iteration: 325 \t total 625 iterations\n",
            "iteration: 326 \t total 625 iterations\n",
            "iteration: 327 \t total 625 iterations\n",
            "iteration: 328 \t total 625 iterations\n",
            "iteration: 329 \t total 625 iterations\n",
            "iteration: 330 \t total 625 iterations\n",
            "iteration: 331 \t total 625 iterations\n",
            "iteration: 332 \t total 625 iterations\n",
            "iteration: 333 \t total 625 iterations\n",
            "iteration: 334 \t total 625 iterations\n",
            "iteration: 335 \t total 625 iterations\n",
            "iteration: 336 \t total 625 iterations\n",
            "iteration: 337 \t total 625 iterations\n",
            "iteration: 338 \t total 625 iterations\n",
            "iteration: 339 \t total 625 iterations\n",
            "iteration: 340 \t total 625 iterations\n",
            "iteration: 341 \t total 625 iterations\n",
            "iteration: 342 \t total 625 iterations\n",
            "iteration: 343 \t total 625 iterations\n",
            "iteration: 344 \t total 625 iterations\n",
            "iteration: 345 \t total 625 iterations\n",
            "iteration: 346 \t total 625 iterations\n",
            "iteration: 347 \t total 625 iterations\n",
            "iteration: 348 \t total 625 iterations\n",
            "iteration: 349 \t total 625 iterations\n",
            "iteration: 350 \t total 625 iterations\n",
            "iteration: 351 \t total 625 iterations\n",
            "iteration: 352 \t total 625 iterations\n",
            "iteration: 353 \t total 625 iterations\n",
            "iteration: 354 \t total 625 iterations\n",
            "iteration: 355 \t total 625 iterations\n",
            "iteration: 356 \t total 625 iterations\n",
            "iteration: 357 \t total 625 iterations\n",
            "iteration: 358 \t total 625 iterations\n",
            "iteration: 359 \t total 625 iterations\n",
            "iteration: 360 \t total 625 iterations\n",
            "iteration: 361 \t total 625 iterations\n",
            "iteration: 362 \t total 625 iterations\n",
            "iteration: 363 \t total 625 iterations\n",
            "iteration: 364 \t total 625 iterations\n",
            "iteration: 365 \t total 625 iterations\n",
            "iteration: 366 \t total 625 iterations\n",
            "iteration: 367 \t total 625 iterations\n",
            "iteration: 368 \t total 625 iterations\n",
            "iteration: 369 \t total 625 iterations\n",
            "iteration: 370 \t total 625 iterations\n",
            "iteration: 371 \t total 625 iterations\n",
            "iteration: 372 \t total 625 iterations\n",
            "iteration: 373 \t total 625 iterations\n",
            "iteration: 374 \t total 625 iterations\n",
            "iteration: 375 \t total 625 iterations\n",
            "iteration: 376 \t total 625 iterations\n",
            "iteration: 377 \t total 625 iterations\n",
            "iteration: 378 \t total 625 iterations\n",
            "iteration: 379 \t total 625 iterations\n",
            "iteration: 380 \t total 625 iterations\n",
            "iteration: 381 \t total 625 iterations\n",
            "iteration: 382 \t total 625 iterations\n",
            "iteration: 383 \t total 625 iterations\n",
            "iteration: 384 \t total 625 iterations\n",
            "iteration: 385 \t total 625 iterations\n",
            "iteration: 386 \t total 625 iterations\n",
            "iteration: 387 \t total 625 iterations\n",
            "iteration: 388 \t total 625 iterations\n",
            "iteration: 389 \t total 625 iterations\n",
            "iteration: 390 \t total 625 iterations\n",
            "iteration: 391 \t total 625 iterations\n",
            "iteration: 392 \t total 625 iterations\n",
            "iteration: 393 \t total 625 iterations\n",
            "iteration: 394 \t total 625 iterations\n",
            "iteration: 395 \t total 625 iterations\n",
            "iteration: 396 \t total 625 iterations\n",
            "iteration: 397 \t total 625 iterations\n",
            "iteration: 398 \t total 625 iterations\n",
            "iteration: 399 \t total 625 iterations\n",
            "iteration: 400 \t total 625 iterations\n",
            "iteration: 401 \t total 625 iterations\n",
            "iteration: 402 \t total 625 iterations\n",
            "iteration: 403 \t total 625 iterations\n",
            "iteration: 404 \t total 625 iterations\n",
            "iteration: 405 \t total 625 iterations\n",
            "iteration: 406 \t total 625 iterations\n",
            "iteration: 407 \t total 625 iterations\n",
            "iteration: 408 \t total 625 iterations\n",
            "iteration: 409 \t total 625 iterations\n",
            "iteration: 410 \t total 625 iterations\n",
            "iteration: 411 \t total 625 iterations\n",
            "iteration: 412 \t total 625 iterations\n",
            "iteration: 413 \t total 625 iterations\n",
            "iteration: 414 \t total 625 iterations\n",
            "iteration: 415 \t total 625 iterations\n",
            "iteration: 416 \t total 625 iterations\n",
            "iteration: 417 \t total 625 iterations\n",
            "iteration: 418 \t total 625 iterations\n",
            "iteration: 419 \t total 625 iterations\n",
            "iteration: 420 \t total 625 iterations\n",
            "iteration: 421 \t total 625 iterations\n",
            "iteration: 422 \t total 625 iterations\n",
            "iteration: 423 \t total 625 iterations\n",
            "iteration: 424 \t total 625 iterations\n",
            "iteration: 425 \t total 625 iterations\n",
            "iteration: 426 \t total 625 iterations\n",
            "iteration: 427 \t total 625 iterations\n",
            "iteration: 428 \t total 625 iterations\n",
            "iteration: 429 \t total 625 iterations\n",
            "iteration: 430 \t total 625 iterations\n",
            "iteration: 431 \t total 625 iterations\n",
            "iteration: 432 \t total 625 iterations\n",
            "iteration: 433 \t total 625 iterations\n",
            "iteration: 434 \t total 625 iterations\n",
            "iteration: 435 \t total 625 iterations\n",
            "iteration: 436 \t total 625 iterations\n",
            "iteration: 437 \t total 625 iterations\n",
            "iteration: 438 \t total 625 iterations\n",
            "iteration: 439 \t total 625 iterations\n",
            "iteration: 440 \t total 625 iterations\n",
            "iteration: 441 \t total 625 iterations\n",
            "iteration: 442 \t total 625 iterations\n",
            "iteration: 443 \t total 625 iterations\n",
            "iteration: 444 \t total 625 iterations\n",
            "iteration: 445 \t total 625 iterations\n",
            "iteration: 446 \t total 625 iterations\n",
            "iteration: 447 \t total 625 iterations\n",
            "iteration: 448 \t total 625 iterations\n",
            "iteration: 449 \t total 625 iterations\n",
            "iteration: 450 \t total 625 iterations\n",
            "iteration: 451 \t total 625 iterations\n",
            "iteration: 452 \t total 625 iterations\n",
            "iteration: 453 \t total 625 iterations\n",
            "iteration: 454 \t total 625 iterations\n",
            "iteration: 455 \t total 625 iterations\n",
            "iteration: 456 \t total 625 iterations\n",
            "iteration: 457 \t total 625 iterations\n",
            "iteration: 458 \t total 625 iterations\n",
            "iteration: 459 \t total 625 iterations\n",
            "iteration: 460 \t total 625 iterations\n",
            "iteration: 461 \t total 625 iterations\n",
            "iteration: 462 \t total 625 iterations\n",
            "iteration: 463 \t total 625 iterations\n",
            "iteration: 464 \t total 625 iterations\n",
            "iteration: 465 \t total 625 iterations\n",
            "iteration: 466 \t total 625 iterations\n",
            "iteration: 467 \t total 625 iterations\n",
            "iteration: 468 \t total 625 iterations\n",
            "iteration: 469 \t total 625 iterations\n",
            "iteration: 470 \t total 625 iterations\n",
            "iteration: 471 \t total 625 iterations\n",
            "iteration: 472 \t total 625 iterations\n",
            "iteration: 473 \t total 625 iterations\n",
            "iteration: 474 \t total 625 iterations\n",
            "iteration: 475 \t total 625 iterations\n",
            "iteration: 476 \t total 625 iterations\n",
            "iteration: 477 \t total 625 iterations\n",
            "iteration: 478 \t total 625 iterations\n",
            "iteration: 479 \t total 625 iterations\n",
            "iteration: 480 \t total 625 iterations\n",
            "iteration: 481 \t total 625 iterations\n",
            "iteration: 482 \t total 625 iterations\n",
            "iteration: 483 \t total 625 iterations\n",
            "iteration: 484 \t total 625 iterations\n",
            "iteration: 485 \t total 625 iterations\n",
            "iteration: 486 \t total 625 iterations\n",
            "iteration: 487 \t total 625 iterations\n",
            "iteration: 488 \t total 625 iterations\n",
            "iteration: 489 \t total 625 iterations\n",
            "iteration: 490 \t total 625 iterations\n",
            "iteration: 491 \t total 625 iterations\n",
            "iteration: 492 \t total 625 iterations\n",
            "iteration: 493 \t total 625 iterations\n",
            "iteration: 494 \t total 625 iterations\n",
            "iteration: 495 \t total 625 iterations\n",
            "iteration: 496 \t total 625 iterations\n",
            "iteration: 497 \t total 625 iterations\n",
            "iteration: 498 \t total 625 iterations\n",
            "iteration: 499 \t total 625 iterations\n",
            "iteration: 500 \t total 625 iterations\n",
            "iteration: 501 \t total 625 iterations\n",
            "iteration: 502 \t total 625 iterations\n",
            "iteration: 503 \t total 625 iterations\n",
            "iteration: 504 \t total 625 iterations\n",
            "iteration: 505 \t total 625 iterations\n",
            "iteration: 506 \t total 625 iterations\n",
            "iteration: 507 \t total 625 iterations\n",
            "iteration: 508 \t total 625 iterations\n",
            "iteration: 509 \t total 625 iterations\n",
            "iteration: 510 \t total 625 iterations\n",
            "iteration: 511 \t total 625 iterations\n",
            "iteration: 512 \t total 625 iterations\n",
            "iteration: 513 \t total 625 iterations\n",
            "iteration: 514 \t total 625 iterations\n",
            "iteration: 515 \t total 625 iterations\n",
            "iteration: 516 \t total 625 iterations\n",
            "iteration: 517 \t total 625 iterations\n",
            "iteration: 518 \t total 625 iterations\n",
            "iteration: 519 \t total 625 iterations\n",
            "iteration: 520 \t total 625 iterations\n",
            "iteration: 521 \t total 625 iterations\n",
            "iteration: 522 \t total 625 iterations\n",
            "iteration: 523 \t total 625 iterations\n",
            "iteration: 524 \t total 625 iterations\n",
            "iteration: 525 \t total 625 iterations\n",
            "iteration: 526 \t total 625 iterations\n",
            "iteration: 527 \t total 625 iterations\n",
            "iteration: 528 \t total 625 iterations\n",
            "iteration: 529 \t total 625 iterations\n",
            "iteration: 530 \t total 625 iterations\n",
            "iteration: 531 \t total 625 iterations\n",
            "iteration: 532 \t total 625 iterations\n",
            "iteration: 533 \t total 625 iterations\n",
            "iteration: 534 \t total 625 iterations\n",
            "iteration: 535 \t total 625 iterations\n",
            "iteration: 536 \t total 625 iterations\n",
            "iteration: 537 \t total 625 iterations\n",
            "iteration: 538 \t total 625 iterations\n",
            "iteration: 539 \t total 625 iterations\n",
            "iteration: 540 \t total 625 iterations\n",
            "iteration: 541 \t total 625 iterations\n",
            "iteration: 542 \t total 625 iterations\n",
            "iteration: 543 \t total 625 iterations\n",
            "iteration: 544 \t total 625 iterations\n",
            "iteration: 545 \t total 625 iterations\n",
            "iteration: 546 \t total 625 iterations\n",
            "iteration: 547 \t total 625 iterations\n",
            "iteration: 548 \t total 625 iterations\n",
            "iteration: 549 \t total 625 iterations\n",
            "iteration: 550 \t total 625 iterations\n",
            "iteration: 551 \t total 625 iterations\n",
            "iteration: 552 \t total 625 iterations\n",
            "iteration: 553 \t total 625 iterations\n",
            "iteration: 554 \t total 625 iterations\n",
            "iteration: 555 \t total 625 iterations\n",
            "iteration: 556 \t total 625 iterations\n",
            "iteration: 557 \t total 625 iterations\n",
            "iteration: 558 \t total 625 iterations\n",
            "iteration: 559 \t total 625 iterations\n",
            "iteration: 560 \t total 625 iterations\n",
            "iteration: 561 \t total 625 iterations\n",
            "iteration: 562 \t total 625 iterations\n",
            "iteration: 563 \t total 625 iterations\n",
            "iteration: 564 \t total 625 iterations\n",
            "iteration: 565 \t total 625 iterations\n",
            "iteration: 566 \t total 625 iterations\n",
            "iteration: 567 \t total 625 iterations\n",
            "iteration: 568 \t total 625 iterations\n",
            "iteration: 569 \t total 625 iterations\n",
            "iteration: 570 \t total 625 iterations\n",
            "iteration: 571 \t total 625 iterations\n",
            "iteration: 572 \t total 625 iterations\n",
            "iteration: 573 \t total 625 iterations\n",
            "iteration: 574 \t total 625 iterations\n",
            "iteration: 575 \t total 625 iterations\n",
            "iteration: 576 \t total 625 iterations\n",
            "iteration: 577 \t total 625 iterations\n",
            "iteration: 578 \t total 625 iterations\n",
            "iteration: 579 \t total 625 iterations\n",
            "iteration: 580 \t total 625 iterations\n",
            "iteration: 581 \t total 625 iterations\n",
            "iteration: 582 \t total 625 iterations\n",
            "iteration: 583 \t total 625 iterations\n",
            "iteration: 584 \t total 625 iterations\n",
            "iteration: 585 \t total 625 iterations\n",
            "iteration: 586 \t total 625 iterations\n",
            "iteration: 587 \t total 625 iterations\n",
            "iteration: 588 \t total 625 iterations\n",
            "iteration: 589 \t total 625 iterations\n",
            "iteration: 590 \t total 625 iterations\n",
            "iteration: 591 \t total 625 iterations\n",
            "iteration: 592 \t total 625 iterations\n",
            "iteration: 593 \t total 625 iterations\n",
            "iteration: 594 \t total 625 iterations\n",
            "iteration: 595 \t total 625 iterations\n",
            "iteration: 596 \t total 625 iterations\n",
            "iteration: 597 \t total 625 iterations\n",
            "iteration: 598 \t total 625 iterations\n",
            "iteration: 599 \t total 625 iterations\n",
            "iteration: 600 \t total 625 iterations\n",
            "iteration: 601 \t total 625 iterations\n",
            "iteration: 602 \t total 625 iterations\n",
            "iteration: 603 \t total 625 iterations\n",
            "iteration: 604 \t total 625 iterations\n",
            "iteration: 605 \t total 625 iterations\n",
            "iteration: 606 \t total 625 iterations\n",
            "iteration: 607 \t total 625 iterations\n",
            "iteration: 608 \t total 625 iterations\n",
            "iteration: 609 \t total 625 iterations\n",
            "iteration: 610 \t total 625 iterations\n",
            "iteration: 611 \t total 625 iterations\n",
            "iteration: 612 \t total 625 iterations\n",
            "iteration: 613 \t total 625 iterations\n",
            "iteration: 614 \t total 625 iterations\n",
            "iteration: 615 \t total 625 iterations\n",
            "iteration: 616 \t total 625 iterations\n",
            "iteration: 617 \t total 625 iterations\n",
            "iteration: 618 \t total 625 iterations\n",
            "iteration: 619 \t total 625 iterations\n",
            "iteration: 620 \t total 625 iterations\n",
            "iteration: 621 \t total 625 iterations\n",
            "iteration: 622 \t total 625 iterations\n",
            "iteration: 623 \t total 625 iterations\n",
            "iteration: 624 \t total 625 iterations\n",
            "iteration: 625 \t total 625 iterations\n",
            "\n",
            "Parameter numbers: 3315428\n",
            "MobileNet(\n",
            "  3.32 M, 100.000% Params, 47.91 MMac, 99.140% MACs, \n",
            "  (stem): Sequential(\n",
            "    3.52 k, 0.106% Params, 3.74 MMac, 7.730% MACs, \n",
            "    (0): BasicConv2d(\n",
            "      928, 0.028% Params, 983.04 KMac, 2.034% MACs, \n",
            "      (conv): Conv2d(864, 0.026% Params, 884.74 KMac, 1.831% MACs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, 0.002% Params, 65.54 KMac, 0.136% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      2.59 k, 0.078% Params, 2.75 MMac, 5.696% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        352, 0.011% Params, 393.22 KMac, 0.814% MACs, \n",
            "        (0): Conv2d(288, 0.009% Params, 294.91 KMac, 0.610% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "        (1): BatchNorm2d(64, 0.002% Params, 65.54 KMac, 0.136% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        2.24 k, 0.068% Params, 2.36 MMac, 4.882% MACs, \n",
            "        (0): Conv2d(2.11 k, 0.064% Params, 2.16 MMac, 4.476% MACs, 32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(128, 0.004% Params, 131.07 KMac, 0.271% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 65.54 KMac, 0.136% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1): Sequential(\n",
            "    27.46 k, 0.828% Params, 7.14 MMac, 14.783% MACs, \n",
            "    (0): DepthSeperabelConv2d(\n",
            "      9.28 k, 0.280% Params, 2.42 MMac, 5.018% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        704, 0.021% Params, 196.61 KMac, 0.407% MACs, \n",
            "        (0): Conv2d(576, 0.017% Params, 147.46 KMac, 0.305% MACs, 64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "        (1): BatchNorm2d(128, 0.004% Params, 32.77 KMac, 0.068% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 16.38 KMac, 0.034% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        8.58 k, 0.259% Params, 2.23 MMac, 4.611% MACs, \n",
            "        (0): Conv2d(8.32 k, 0.251% Params, 2.13 MMac, 4.408% MACs, 64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(256, 0.008% Params, 65.54 KMac, 0.136% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      18.18 k, 0.548% Params, 4.72 MMac, 9.765% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        1.41 k, 0.042% Params, 393.22 KMac, 0.814% MACs, \n",
            "        (0): Conv2d(1.15 k, 0.035% Params, 294.91 KMac, 0.610% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(256, 0.008% Params, 65.54 KMac, 0.136% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        16.77 k, 0.506% Params, 4.33 MMac, 8.951% MACs, \n",
            "        (0): Conv2d(16.51 k, 0.498% Params, 4.23 MMac, 8.748% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(256, 0.008% Params, 65.54 KMac, 0.136% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    104.06 k, 3.139% Params, 6.72 MMac, 13.901% MACs, \n",
            "    (0): DepthSeperabelConv2d(\n",
            "      34.94 k, 1.054% Params, 2.26 MMac, 4.679% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        1.41 k, 0.042% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(1.15 k, 0.035% Params, 73.73 KMac, 0.153% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(256, 0.008% Params, 16.38 KMac, 0.034% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        33.54 k, 1.012% Params, 2.16 MMac, 4.476% MACs, \n",
            "        (0): Conv2d(33.02 k, 0.996% Params, 2.11 MMac, 4.374% MACs, 128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, 0.015% Params, 32.77 KMac, 0.068% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 16.38 KMac, 0.034% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      69.12 k, 2.085% Params, 4.46 MMac, 9.222% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        2.82 k, 0.085% Params, 196.61 KMac, 0.407% MACs, \n",
            "        (0): Conv2d(2.3 k, 0.069% Params, 147.46 KMac, 0.305% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(512, 0.015% Params, 32.77 KMac, 0.068% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 16.38 KMac, 0.034% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        66.3 k, 2.000% Params, 4.26 MMac, 8.815% MACs, \n",
            "        (0): Conv2d(65.79 k, 1.984% Params, 4.21 MMac, 8.714% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, 0.015% Params, 32.77 KMac, 0.068% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 16.38 KMac, 0.034% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    1.48 M, 44.700% Params, 23.81 MMac, 49.265% MACs, \n",
            "    (0): DepthSeperabelConv2d(\n",
            "      135.42 k, 4.085% Params, 2.18 MMac, 4.509% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        2.82 k, 0.085% Params, 49.15 KMac, 0.102% MACs, \n",
            "        (0): Conv2d(2.3 k, 0.069% Params, 36.86 KMac, 0.076% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(512, 0.015% Params, 8.19 KMac, 0.017% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 4.1 KMac, 0.008% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        132.61 k, 4.000% Params, 2.13 MMac, 4.408% MACs, \n",
            "        (0): Conv2d(131.58 k, 3.969% Params, 2.11 MMac, 4.357% MACs, 256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (3): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (4): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    1.6 M, 48.136% Params, 6.4 MMac, 13.240% MACs, \n",
            "    (0): DepthSeperabelConv2d(\n",
            "      532.99 k, 16.076% Params, 2.14 MMac, 4.425% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 24.58 KMac, 0.051% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 18.43 KMac, 0.038% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 4.1 KMac, 0.008% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 2.05 KMac, 0.004% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        527.36 k, 15.906% Params, 2.11 MMac, 4.374% MACs, \n",
            "        (0): Conv2d(525.31 k, 15.844% Params, 2.1 MMac, 4.348% MACs, 512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(2.05 k, 0.062% Params, 8.19 KMac, 0.017% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 4.1 KMac, 0.008% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      1.06 M, 32.060% Params, 4.26 MMac, 8.815% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        11.26 k, 0.340% Params, 49.15 KMac, 0.102% MACs, \n",
            "        (0): Conv2d(9.22 k, 0.278% Params, 36.86 KMac, 0.076% MACs, 1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "        (1): BatchNorm2d(2.05 k, 0.062% Params, 8.19 KMac, 0.017% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 4.1 KMac, 0.008% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        1.05 M, 31.720% Params, 4.21 MMac, 8.714% MACs, \n",
            "        (0): Conv2d(1.05 M, 31.658% Params, 4.2 MMac, 8.688% MACs, 1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(2.05 k, 0.062% Params, 8.19 KMac, 0.017% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 4.1 KMac, 0.008% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(102.5 k, 3.092% Params, 102.5 KMac, 0.212% MACs, in_features=1024, out_features=100, bias=True)\n",
            "  (avg): AdaptiveAvgPool2d(0, 0.000% Params, 4.1 KMac, 0.008% MACs, output_size=1)\n",
            ")\n",
            "Top 1 err:  tensor(0.3306)\n",
            "Top 5 err:  tensor(0.1015)\n",
            "FLOPs: 48.32 MMac\n",
            "Params: 3.32 M\n",
            "Inference time: 36.0226 seconds\n",
            "Time per inference step: 57.6361 milliseconds\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/Knowledge-Distillation/test.py -net mobilenet -weights /content/drive/MyDrive/Knowledge-Distillation/runs/mobilenet/mobilenet-124-best.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgIfp6xH0N4P"
      },
      "source": [
        "## Testing with GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReUjhn4R4rXd",
        "outputId": "e4c116de-0706-460c-b8f6-951ab781cc5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "MobileNet(\n",
            "  (stem): Sequential(\n",
            "    (0): BasicConv2d(\n",
            "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1): Sequential(\n",
            "    (0): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (3): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (4): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      (depthwise): Sequential(\n",
            "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=1024, out_features=100, bias=True)\n",
            "  (avg): AdaptiveAvgPool2d(output_size=1)\n",
            ")\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "iteration: 1 \t total 625 iterations\n",
            "iteration: 2 \t total 625 iterations\n",
            "iteration: 3 \t total 625 iterations\n",
            "iteration: 4 \t total 625 iterations\n",
            "iteration: 5 \t total 625 iterations\n",
            "iteration: 6 \t total 625 iterations\n",
            "iteration: 7 \t total 625 iterations\n",
            "iteration: 8 \t total 625 iterations\n",
            "iteration: 9 \t total 625 iterations\n",
            "iteration: 10 \t total 625 iterations\n",
            "iteration: 11 \t total 625 iterations\n",
            "iteration: 12 \t total 625 iterations\n",
            "iteration: 13 \t total 625 iterations\n",
            "iteration: 14 \t total 625 iterations\n",
            "iteration: 15 \t total 625 iterations\n",
            "iteration: 16 \t total 625 iterations\n",
            "iteration: 17 \t total 625 iterations\n",
            "iteration: 18 \t total 625 iterations\n",
            "iteration: 19 \t total 625 iterations\n",
            "iteration: 20 \t total 625 iterations\n",
            "iteration: 21 \t total 625 iterations\n",
            "iteration: 22 \t total 625 iterations\n",
            "iteration: 23 \t total 625 iterations\n",
            "iteration: 24 \t total 625 iterations\n",
            "iteration: 25 \t total 625 iterations\n",
            "iteration: 26 \t total 625 iterations\n",
            "iteration: 27 \t total 625 iterations\n",
            "iteration: 28 \t total 625 iterations\n",
            "iteration: 29 \t total 625 iterations\n",
            "iteration: 30 \t total 625 iterations\n",
            "iteration: 31 \t total 625 iterations\n",
            "iteration: 32 \t total 625 iterations\n",
            "iteration: 33 \t total 625 iterations\n",
            "iteration: 34 \t total 625 iterations\n",
            "iteration: 35 \t total 625 iterations\n",
            "iteration: 36 \t total 625 iterations\n",
            "iteration: 37 \t total 625 iterations\n",
            "iteration: 38 \t total 625 iterations\n",
            "iteration: 39 \t total 625 iterations\n",
            "iteration: 40 \t total 625 iterations\n",
            "iteration: 41 \t total 625 iterations\n",
            "iteration: 42 \t total 625 iterations\n",
            "iteration: 43 \t total 625 iterations\n",
            "iteration: 44 \t total 625 iterations\n",
            "iteration: 45 \t total 625 iterations\n",
            "iteration: 46 \t total 625 iterations\n",
            "iteration: 47 \t total 625 iterations\n",
            "iteration: 48 \t total 625 iterations\n",
            "iteration: 49 \t total 625 iterations\n",
            "iteration: 50 \t total 625 iterations\n",
            "iteration: 51 \t total 625 iterations\n",
            "iteration: 52 \t total 625 iterations\n",
            "iteration: 53 \t total 625 iterations\n",
            "iteration: 54 \t total 625 iterations\n",
            "iteration: 55 \t total 625 iterations\n",
            "iteration: 56 \t total 625 iterations\n",
            "iteration: 57 \t total 625 iterations\n",
            "iteration: 58 \t total 625 iterations\n",
            "iteration: 59 \t total 625 iterations\n",
            "iteration: 60 \t total 625 iterations\n",
            "iteration: 61 \t total 625 iterations\n",
            "iteration: 62 \t total 625 iterations\n",
            "iteration: 63 \t total 625 iterations\n",
            "iteration: 64 \t total 625 iterations\n",
            "iteration: 65 \t total 625 iterations\n",
            "iteration: 66 \t total 625 iterations\n",
            "iteration: 67 \t total 625 iterations\n",
            "iteration: 68 \t total 625 iterations\n",
            "iteration: 69 \t total 625 iterations\n",
            "iteration: 70 \t total 625 iterations\n",
            "iteration: 71 \t total 625 iterations\n",
            "iteration: 72 \t total 625 iterations\n",
            "iteration: 73 \t total 625 iterations\n",
            "iteration: 74 \t total 625 iterations\n",
            "iteration: 75 \t total 625 iterations\n",
            "iteration: 76 \t total 625 iterations\n",
            "iteration: 77 \t total 625 iterations\n",
            "iteration: 78 \t total 625 iterations\n",
            "iteration: 79 \t total 625 iterations\n",
            "iteration: 80 \t total 625 iterations\n",
            "iteration: 81 \t total 625 iterations\n",
            "iteration: 82 \t total 625 iterations\n",
            "iteration: 83 \t total 625 iterations\n",
            "iteration: 84 \t total 625 iterations\n",
            "iteration: 85 \t total 625 iterations\n",
            "iteration: 86 \t total 625 iterations\n",
            "iteration: 87 \t total 625 iterations\n",
            "iteration: 88 \t total 625 iterations\n",
            "iteration: 89 \t total 625 iterations\n",
            "iteration: 90 \t total 625 iterations\n",
            "iteration: 91 \t total 625 iterations\n",
            "iteration: 92 \t total 625 iterations\n",
            "iteration: 93 \t total 625 iterations\n",
            "iteration: 94 \t total 625 iterations\n",
            "iteration: 95 \t total 625 iterations\n",
            "iteration: 96 \t total 625 iterations\n",
            "iteration: 97 \t total 625 iterations\n",
            "iteration: 98 \t total 625 iterations\n",
            "iteration: 99 \t total 625 iterations\n",
            "iteration: 100 \t total 625 iterations\n",
            "iteration: 101 \t total 625 iterations\n",
            "iteration: 102 \t total 625 iterations\n",
            "iteration: 103 \t total 625 iterations\n",
            "iteration: 104 \t total 625 iterations\n",
            "iteration: 105 \t total 625 iterations\n",
            "iteration: 106 \t total 625 iterations\n",
            "iteration: 107 \t total 625 iterations\n",
            "iteration: 108 \t total 625 iterations\n",
            "iteration: 109 \t total 625 iterations\n",
            "iteration: 110 \t total 625 iterations\n",
            "iteration: 111 \t total 625 iterations\n",
            "iteration: 112 \t total 625 iterations\n",
            "iteration: 113 \t total 625 iterations\n",
            "iteration: 114 \t total 625 iterations\n",
            "iteration: 115 \t total 625 iterations\n",
            "iteration: 116 \t total 625 iterations\n",
            "iteration: 117 \t total 625 iterations\n",
            "iteration: 118 \t total 625 iterations\n",
            "iteration: 119 \t total 625 iterations\n",
            "iteration: 120 \t total 625 iterations\n",
            "iteration: 121 \t total 625 iterations\n",
            "iteration: 122 \t total 625 iterations\n",
            "iteration: 123 \t total 625 iterations\n",
            "iteration: 124 \t total 625 iterations\n",
            "iteration: 125 \t total 625 iterations\n",
            "iteration: 126 \t total 625 iterations\n",
            "iteration: 127 \t total 625 iterations\n",
            "iteration: 128 \t total 625 iterations\n",
            "iteration: 129 \t total 625 iterations\n",
            "iteration: 130 \t total 625 iterations\n",
            "iteration: 131 \t total 625 iterations\n",
            "iteration: 132 \t total 625 iterations\n",
            "iteration: 133 \t total 625 iterations\n",
            "iteration: 134 \t total 625 iterations\n",
            "iteration: 135 \t total 625 iterations\n",
            "iteration: 136 \t total 625 iterations\n",
            "iteration: 137 \t total 625 iterations\n",
            "iteration: 138 \t total 625 iterations\n",
            "iteration: 139 \t total 625 iterations\n",
            "iteration: 140 \t total 625 iterations\n",
            "iteration: 141 \t total 625 iterations\n",
            "iteration: 142 \t total 625 iterations\n",
            "iteration: 143 \t total 625 iterations\n",
            "iteration: 144 \t total 625 iterations\n",
            "iteration: 145 \t total 625 iterations\n",
            "iteration: 146 \t total 625 iterations\n",
            "iteration: 147 \t total 625 iterations\n",
            "iteration: 148 \t total 625 iterations\n",
            "iteration: 149 \t total 625 iterations\n",
            "iteration: 150 \t total 625 iterations\n",
            "iteration: 151 \t total 625 iterations\n",
            "iteration: 152 \t total 625 iterations\n",
            "iteration: 153 \t total 625 iterations\n",
            "iteration: 154 \t total 625 iterations\n",
            "iteration: 155 \t total 625 iterations\n",
            "iteration: 156 \t total 625 iterations\n",
            "iteration: 157 \t total 625 iterations\n",
            "iteration: 158 \t total 625 iterations\n",
            "iteration: 159 \t total 625 iterations\n",
            "iteration: 160 \t total 625 iterations\n",
            "iteration: 161 \t total 625 iterations\n",
            "iteration: 162 \t total 625 iterations\n",
            "iteration: 163 \t total 625 iterations\n",
            "iteration: 164 \t total 625 iterations\n",
            "iteration: 165 \t total 625 iterations\n",
            "iteration: 166 \t total 625 iterations\n",
            "iteration: 167 \t total 625 iterations\n",
            "iteration: 168 \t total 625 iterations\n",
            "iteration: 169 \t total 625 iterations\n",
            "iteration: 170 \t total 625 iterations\n",
            "iteration: 171 \t total 625 iterations\n",
            "iteration: 172 \t total 625 iterations\n",
            "iteration: 173 \t total 625 iterations\n",
            "iteration: 174 \t total 625 iterations\n",
            "iteration: 175 \t total 625 iterations\n",
            "iteration: 176 \t total 625 iterations\n",
            "iteration: 177 \t total 625 iterations\n",
            "iteration: 178 \t total 625 iterations\n",
            "iteration: 179 \t total 625 iterations\n",
            "iteration: 180 \t total 625 iterations\n",
            "iteration: 181 \t total 625 iterations\n",
            "iteration: 182 \t total 625 iterations\n",
            "iteration: 183 \t total 625 iterations\n",
            "iteration: 184 \t total 625 iterations\n",
            "iteration: 185 \t total 625 iterations\n",
            "iteration: 186 \t total 625 iterations\n",
            "iteration: 187 \t total 625 iterations\n",
            "iteration: 188 \t total 625 iterations\n",
            "iteration: 189 \t total 625 iterations\n",
            "iteration: 190 \t total 625 iterations\n",
            "iteration: 191 \t total 625 iterations\n",
            "iteration: 192 \t total 625 iterations\n",
            "iteration: 193 \t total 625 iterations\n",
            "iteration: 194 \t total 625 iterations\n",
            "iteration: 195 \t total 625 iterations\n",
            "iteration: 196 \t total 625 iterations\n",
            "iteration: 197 \t total 625 iterations\n",
            "iteration: 198 \t total 625 iterations\n",
            "iteration: 199 \t total 625 iterations\n",
            "iteration: 200 \t total 625 iterations\n",
            "iteration: 201 \t total 625 iterations\n",
            "iteration: 202 \t total 625 iterations\n",
            "iteration: 203 \t total 625 iterations\n",
            "iteration: 204 \t total 625 iterations\n",
            "iteration: 205 \t total 625 iterations\n",
            "iteration: 206 \t total 625 iterations\n",
            "iteration: 207 \t total 625 iterations\n",
            "iteration: 208 \t total 625 iterations\n",
            "iteration: 209 \t total 625 iterations\n",
            "iteration: 210 \t total 625 iterations\n",
            "iteration: 211 \t total 625 iterations\n",
            "iteration: 212 \t total 625 iterations\n",
            "iteration: 213 \t total 625 iterations\n",
            "iteration: 214 \t total 625 iterations\n",
            "iteration: 215 \t total 625 iterations\n",
            "iteration: 216 \t total 625 iterations\n",
            "iteration: 217 \t total 625 iterations\n",
            "iteration: 218 \t total 625 iterations\n",
            "iteration: 219 \t total 625 iterations\n",
            "iteration: 220 \t total 625 iterations\n",
            "iteration: 221 \t total 625 iterations\n",
            "iteration: 222 \t total 625 iterations\n",
            "iteration: 223 \t total 625 iterations\n",
            "iteration: 224 \t total 625 iterations\n",
            "iteration: 225 \t total 625 iterations\n",
            "iteration: 226 \t total 625 iterations\n",
            "iteration: 227 \t total 625 iterations\n",
            "iteration: 228 \t total 625 iterations\n",
            "iteration: 229 \t total 625 iterations\n",
            "iteration: 230 \t total 625 iterations\n",
            "iteration: 231 \t total 625 iterations\n",
            "iteration: 232 \t total 625 iterations\n",
            "iteration: 233 \t total 625 iterations\n",
            "iteration: 234 \t total 625 iterations\n",
            "iteration: 235 \t total 625 iterations\n",
            "iteration: 236 \t total 625 iterations\n",
            "iteration: 237 \t total 625 iterations\n",
            "iteration: 238 \t total 625 iterations\n",
            "iteration: 239 \t total 625 iterations\n",
            "iteration: 240 \t total 625 iterations\n",
            "iteration: 241 \t total 625 iterations\n",
            "iteration: 242 \t total 625 iterations\n",
            "iteration: 243 \t total 625 iterations\n",
            "iteration: 244 \t total 625 iterations\n",
            "iteration: 245 \t total 625 iterations\n",
            "iteration: 246 \t total 625 iterations\n",
            "iteration: 247 \t total 625 iterations\n",
            "iteration: 248 \t total 625 iterations\n",
            "iteration: 249 \t total 625 iterations\n",
            "iteration: 250 \t total 625 iterations\n",
            "iteration: 251 \t total 625 iterations\n",
            "iteration: 252 \t total 625 iterations\n",
            "iteration: 253 \t total 625 iterations\n",
            "iteration: 254 \t total 625 iterations\n",
            "iteration: 255 \t total 625 iterations\n",
            "iteration: 256 \t total 625 iterations\n",
            "iteration: 257 \t total 625 iterations\n",
            "iteration: 258 \t total 625 iterations\n",
            "iteration: 259 \t total 625 iterations\n",
            "iteration: 260 \t total 625 iterations\n",
            "iteration: 261 \t total 625 iterations\n",
            "iteration: 262 \t total 625 iterations\n",
            "iteration: 263 \t total 625 iterations\n",
            "iteration: 264 \t total 625 iterations\n",
            "iteration: 265 \t total 625 iterations\n",
            "iteration: 266 \t total 625 iterations\n",
            "iteration: 267 \t total 625 iterations\n",
            "iteration: 268 \t total 625 iterations\n",
            "iteration: 269 \t total 625 iterations\n",
            "iteration: 270 \t total 625 iterations\n",
            "iteration: 271 \t total 625 iterations\n",
            "iteration: 272 \t total 625 iterations\n",
            "iteration: 273 \t total 625 iterations\n",
            "iteration: 274 \t total 625 iterations\n",
            "iteration: 275 \t total 625 iterations\n",
            "iteration: 276 \t total 625 iterations\n",
            "iteration: 277 \t total 625 iterations\n",
            "iteration: 278 \t total 625 iterations\n",
            "iteration: 279 \t total 625 iterations\n",
            "iteration: 280 \t total 625 iterations\n",
            "iteration: 281 \t total 625 iterations\n",
            "iteration: 282 \t total 625 iterations\n",
            "iteration: 283 \t total 625 iterations\n",
            "iteration: 284 \t total 625 iterations\n",
            "iteration: 285 \t total 625 iterations\n",
            "iteration: 286 \t total 625 iterations\n",
            "iteration: 287 \t total 625 iterations\n",
            "iteration: 288 \t total 625 iterations\n",
            "iteration: 289 \t total 625 iterations\n",
            "iteration: 290 \t total 625 iterations\n",
            "iteration: 291 \t total 625 iterations\n",
            "iteration: 292 \t total 625 iterations\n",
            "iteration: 293 \t total 625 iterations\n",
            "iteration: 294 \t total 625 iterations\n",
            "iteration: 295 \t total 625 iterations\n",
            "iteration: 296 \t total 625 iterations\n",
            "iteration: 297 \t total 625 iterations\n",
            "iteration: 298 \t total 625 iterations\n",
            "iteration: 299 \t total 625 iterations\n",
            "iteration: 300 \t total 625 iterations\n",
            "iteration: 301 \t total 625 iterations\n",
            "iteration: 302 \t total 625 iterations\n",
            "iteration: 303 \t total 625 iterations\n",
            "iteration: 304 \t total 625 iterations\n",
            "iteration: 305 \t total 625 iterations\n",
            "iteration: 306 \t total 625 iterations\n",
            "iteration: 307 \t total 625 iterations\n",
            "iteration: 308 \t total 625 iterations\n",
            "iteration: 309 \t total 625 iterations\n",
            "iteration: 310 \t total 625 iterations\n",
            "iteration: 311 \t total 625 iterations\n",
            "iteration: 312 \t total 625 iterations\n",
            "iteration: 313 \t total 625 iterations\n",
            "iteration: 314 \t total 625 iterations\n",
            "iteration: 315 \t total 625 iterations\n",
            "iteration: 316 \t total 625 iterations\n",
            "iteration: 317 \t total 625 iterations\n",
            "iteration: 318 \t total 625 iterations\n",
            "iteration: 319 \t total 625 iterations\n",
            "iteration: 320 \t total 625 iterations\n",
            "iteration: 321 \t total 625 iterations\n",
            "iteration: 322 \t total 625 iterations\n",
            "iteration: 323 \t total 625 iterations\n",
            "iteration: 324 \t total 625 iterations\n",
            "iteration: 325 \t total 625 iterations\n",
            "iteration: 326 \t total 625 iterations\n",
            "iteration: 327 \t total 625 iterations\n",
            "iteration: 328 \t total 625 iterations\n",
            "iteration: 329 \t total 625 iterations\n",
            "iteration: 330 \t total 625 iterations\n",
            "iteration: 331 \t total 625 iterations\n",
            "iteration: 332 \t total 625 iterations\n",
            "iteration: 333 \t total 625 iterations\n",
            "iteration: 334 \t total 625 iterations\n",
            "iteration: 335 \t total 625 iterations\n",
            "iteration: 336 \t total 625 iterations\n",
            "iteration: 337 \t total 625 iterations\n",
            "iteration: 338 \t total 625 iterations\n",
            "iteration: 339 \t total 625 iterations\n",
            "iteration: 340 \t total 625 iterations\n",
            "iteration: 341 \t total 625 iterations\n",
            "iteration: 342 \t total 625 iterations\n",
            "iteration: 343 \t total 625 iterations\n",
            "iteration: 344 \t total 625 iterations\n",
            "iteration: 345 \t total 625 iterations\n",
            "iteration: 346 \t total 625 iterations\n",
            "iteration: 347 \t total 625 iterations\n",
            "iteration: 348 \t total 625 iterations\n",
            "iteration: 349 \t total 625 iterations\n",
            "iteration: 350 \t total 625 iterations\n",
            "iteration: 351 \t total 625 iterations\n",
            "iteration: 352 \t total 625 iterations\n",
            "iteration: 353 \t total 625 iterations\n",
            "iteration: 354 \t total 625 iterations\n",
            "iteration: 355 \t total 625 iterations\n",
            "iteration: 356 \t total 625 iterations\n",
            "iteration: 357 \t total 625 iterations\n",
            "iteration: 358 \t total 625 iterations\n",
            "iteration: 359 \t total 625 iterations\n",
            "iteration: 360 \t total 625 iterations\n",
            "iteration: 361 \t total 625 iterations\n",
            "iteration: 362 \t total 625 iterations\n",
            "iteration: 363 \t total 625 iterations\n",
            "iteration: 364 \t total 625 iterations\n",
            "iteration: 365 \t total 625 iterations\n",
            "iteration: 366 \t total 625 iterations\n",
            "iteration: 367 \t total 625 iterations\n",
            "iteration: 368 \t total 625 iterations\n",
            "iteration: 369 \t total 625 iterations\n",
            "iteration: 370 \t total 625 iterations\n",
            "iteration: 371 \t total 625 iterations\n",
            "iteration: 372 \t total 625 iterations\n",
            "iteration: 373 \t total 625 iterations\n",
            "iteration: 374 \t total 625 iterations\n",
            "iteration: 375 \t total 625 iterations\n",
            "iteration: 376 \t total 625 iterations\n",
            "iteration: 377 \t total 625 iterations\n",
            "iteration: 378 \t total 625 iterations\n",
            "iteration: 379 \t total 625 iterations\n",
            "iteration: 380 \t total 625 iterations\n",
            "iteration: 381 \t total 625 iterations\n",
            "iteration: 382 \t total 625 iterations\n",
            "iteration: 383 \t total 625 iterations\n",
            "iteration: 384 \t total 625 iterations\n",
            "iteration: 385 \t total 625 iterations\n",
            "iteration: 386 \t total 625 iterations\n",
            "iteration: 387 \t total 625 iterations\n",
            "iteration: 388 \t total 625 iterations\n",
            "iteration: 389 \t total 625 iterations\n",
            "iteration: 390 \t total 625 iterations\n",
            "iteration: 391 \t total 625 iterations\n",
            "iteration: 392 \t total 625 iterations\n",
            "iteration: 393 \t total 625 iterations\n",
            "iteration: 394 \t total 625 iterations\n",
            "iteration: 395 \t total 625 iterations\n",
            "iteration: 396 \t total 625 iterations\n",
            "iteration: 397 \t total 625 iterations\n",
            "iteration: 398 \t total 625 iterations\n",
            "iteration: 399 \t total 625 iterations\n",
            "iteration: 400 \t total 625 iterations\n",
            "iteration: 401 \t total 625 iterations\n",
            "iteration: 402 \t total 625 iterations\n",
            "iteration: 403 \t total 625 iterations\n",
            "iteration: 404 \t total 625 iterations\n",
            "iteration: 405 \t total 625 iterations\n",
            "iteration: 406 \t total 625 iterations\n",
            "iteration: 407 \t total 625 iterations\n",
            "iteration: 408 \t total 625 iterations\n",
            "iteration: 409 \t total 625 iterations\n",
            "iteration: 410 \t total 625 iterations\n",
            "iteration: 411 \t total 625 iterations\n",
            "iteration: 412 \t total 625 iterations\n",
            "iteration: 413 \t total 625 iterations\n",
            "iteration: 414 \t total 625 iterations\n",
            "iteration: 415 \t total 625 iterations\n",
            "iteration: 416 \t total 625 iterations\n",
            "iteration: 417 \t total 625 iterations\n",
            "iteration: 418 \t total 625 iterations\n",
            "iteration: 419 \t total 625 iterations\n",
            "iteration: 420 \t total 625 iterations\n",
            "iteration: 421 \t total 625 iterations\n",
            "iteration: 422 \t total 625 iterations\n",
            "iteration: 423 \t total 625 iterations\n",
            "iteration: 424 \t total 625 iterations\n",
            "iteration: 425 \t total 625 iterations\n",
            "iteration: 426 \t total 625 iterations\n",
            "iteration: 427 \t total 625 iterations\n",
            "iteration: 428 \t total 625 iterations\n",
            "iteration: 429 \t total 625 iterations\n",
            "iteration: 430 \t total 625 iterations\n",
            "iteration: 431 \t total 625 iterations\n",
            "iteration: 432 \t total 625 iterations\n",
            "iteration: 433 \t total 625 iterations\n",
            "iteration: 434 \t total 625 iterations\n",
            "iteration: 435 \t total 625 iterations\n",
            "iteration: 436 \t total 625 iterations\n",
            "iteration: 437 \t total 625 iterations\n",
            "iteration: 438 \t total 625 iterations\n",
            "iteration: 439 \t total 625 iterations\n",
            "iteration: 440 \t total 625 iterations\n",
            "iteration: 441 \t total 625 iterations\n",
            "iteration: 442 \t total 625 iterations\n",
            "iteration: 443 \t total 625 iterations\n",
            "iteration: 444 \t total 625 iterations\n",
            "iteration: 445 \t total 625 iterations\n",
            "iteration: 446 \t total 625 iterations\n",
            "iteration: 447 \t total 625 iterations\n",
            "iteration: 448 \t total 625 iterations\n",
            "iteration: 449 \t total 625 iterations\n",
            "iteration: 450 \t total 625 iterations\n",
            "iteration: 451 \t total 625 iterations\n",
            "iteration: 452 \t total 625 iterations\n",
            "iteration: 453 \t total 625 iterations\n",
            "iteration: 454 \t total 625 iterations\n",
            "iteration: 455 \t total 625 iterations\n",
            "iteration: 456 \t total 625 iterations\n",
            "iteration: 457 \t total 625 iterations\n",
            "iteration: 458 \t total 625 iterations\n",
            "iteration: 459 \t total 625 iterations\n",
            "iteration: 460 \t total 625 iterations\n",
            "iteration: 461 \t total 625 iterations\n",
            "iteration: 462 \t total 625 iterations\n",
            "iteration: 463 \t total 625 iterations\n",
            "iteration: 464 \t total 625 iterations\n",
            "iteration: 465 \t total 625 iterations\n",
            "iteration: 466 \t total 625 iterations\n",
            "iteration: 467 \t total 625 iterations\n",
            "iteration: 468 \t total 625 iterations\n",
            "iteration: 469 \t total 625 iterations\n",
            "iteration: 470 \t total 625 iterations\n",
            "iteration: 471 \t total 625 iterations\n",
            "iteration: 472 \t total 625 iterations\n",
            "iteration: 473 \t total 625 iterations\n",
            "iteration: 474 \t total 625 iterations\n",
            "iteration: 475 \t total 625 iterations\n",
            "iteration: 476 \t total 625 iterations\n",
            "iteration: 477 \t total 625 iterations\n",
            "iteration: 478 \t total 625 iterations\n",
            "iteration: 479 \t total 625 iterations\n",
            "iteration: 480 \t total 625 iterations\n",
            "iteration: 481 \t total 625 iterations\n",
            "iteration: 482 \t total 625 iterations\n",
            "iteration: 483 \t total 625 iterations\n",
            "iteration: 484 \t total 625 iterations\n",
            "iteration: 485 \t total 625 iterations\n",
            "iteration: 486 \t total 625 iterations\n",
            "iteration: 487 \t total 625 iterations\n",
            "iteration: 488 \t total 625 iterations\n",
            "iteration: 489 \t total 625 iterations\n",
            "iteration: 490 \t total 625 iterations\n",
            "iteration: 491 \t total 625 iterations\n",
            "iteration: 492 \t total 625 iterations\n",
            "iteration: 493 \t total 625 iterations\n",
            "iteration: 494 \t total 625 iterations\n",
            "iteration: 495 \t total 625 iterations\n",
            "iteration: 496 \t total 625 iterations\n",
            "iteration: 497 \t total 625 iterations\n",
            "iteration: 498 \t total 625 iterations\n",
            "iteration: 499 \t total 625 iterations\n",
            "iteration: 500 \t total 625 iterations\n",
            "iteration: 501 \t total 625 iterations\n",
            "iteration: 502 \t total 625 iterations\n",
            "iteration: 503 \t total 625 iterations\n",
            "iteration: 504 \t total 625 iterations\n",
            "iteration: 505 \t total 625 iterations\n",
            "iteration: 506 \t total 625 iterations\n",
            "iteration: 507 \t total 625 iterations\n",
            "iteration: 508 \t total 625 iterations\n",
            "iteration: 509 \t total 625 iterations\n",
            "iteration: 510 \t total 625 iterations\n",
            "iteration: 511 \t total 625 iterations\n",
            "iteration: 512 \t total 625 iterations\n",
            "iteration: 513 \t total 625 iterations\n",
            "iteration: 514 \t total 625 iterations\n",
            "iteration: 515 \t total 625 iterations\n",
            "iteration: 516 \t total 625 iterations\n",
            "iteration: 517 \t total 625 iterations\n",
            "iteration: 518 \t total 625 iterations\n",
            "iteration: 519 \t total 625 iterations\n",
            "iteration: 520 \t total 625 iterations\n",
            "iteration: 521 \t total 625 iterations\n",
            "iteration: 522 \t total 625 iterations\n",
            "iteration: 523 \t total 625 iterations\n",
            "iteration: 524 \t total 625 iterations\n",
            "iteration: 525 \t total 625 iterations\n",
            "iteration: 526 \t total 625 iterations\n",
            "iteration: 527 \t total 625 iterations\n",
            "iteration: 528 \t total 625 iterations\n",
            "iteration: 529 \t total 625 iterations\n",
            "iteration: 530 \t total 625 iterations\n",
            "iteration: 531 \t total 625 iterations\n",
            "iteration: 532 \t total 625 iterations\n",
            "iteration: 533 \t total 625 iterations\n",
            "iteration: 534 \t total 625 iterations\n",
            "iteration: 535 \t total 625 iterations\n",
            "iteration: 536 \t total 625 iterations\n",
            "iteration: 537 \t total 625 iterations\n",
            "iteration: 538 \t total 625 iterations\n",
            "iteration: 539 \t total 625 iterations\n",
            "iteration: 540 \t total 625 iterations\n",
            "iteration: 541 \t total 625 iterations\n",
            "iteration: 542 \t total 625 iterations\n",
            "iteration: 543 \t total 625 iterations\n",
            "iteration: 544 \t total 625 iterations\n",
            "iteration: 545 \t total 625 iterations\n",
            "iteration: 546 \t total 625 iterations\n",
            "iteration: 547 \t total 625 iterations\n",
            "iteration: 548 \t total 625 iterations\n",
            "iteration: 549 \t total 625 iterations\n",
            "iteration: 550 \t total 625 iterations\n",
            "iteration: 551 \t total 625 iterations\n",
            "iteration: 552 \t total 625 iterations\n",
            "iteration: 553 \t total 625 iterations\n",
            "iteration: 554 \t total 625 iterations\n",
            "iteration: 555 \t total 625 iterations\n",
            "iteration: 556 \t total 625 iterations\n",
            "iteration: 557 \t total 625 iterations\n",
            "iteration: 558 \t total 625 iterations\n",
            "iteration: 559 \t total 625 iterations\n",
            "iteration: 560 \t total 625 iterations\n",
            "iteration: 561 \t total 625 iterations\n",
            "iteration: 562 \t total 625 iterations\n",
            "iteration: 563 \t total 625 iterations\n",
            "iteration: 564 \t total 625 iterations\n",
            "iteration: 565 \t total 625 iterations\n",
            "iteration: 566 \t total 625 iterations\n",
            "iteration: 567 \t total 625 iterations\n",
            "iteration: 568 \t total 625 iterations\n",
            "iteration: 569 \t total 625 iterations\n",
            "iteration: 570 \t total 625 iterations\n",
            "iteration: 571 \t total 625 iterations\n",
            "iteration: 572 \t total 625 iterations\n",
            "iteration: 573 \t total 625 iterations\n",
            "iteration: 574 \t total 625 iterations\n",
            "iteration: 575 \t total 625 iterations\n",
            "iteration: 576 \t total 625 iterations\n",
            "iteration: 577 \t total 625 iterations\n",
            "iteration: 578 \t total 625 iterations\n",
            "iteration: 579 \t total 625 iterations\n",
            "iteration: 580 \t total 625 iterations\n",
            "iteration: 581 \t total 625 iterations\n",
            "iteration: 582 \t total 625 iterations\n",
            "iteration: 583 \t total 625 iterations\n",
            "iteration: 584 \t total 625 iterations\n",
            "iteration: 585 \t total 625 iterations\n",
            "iteration: 586 \t total 625 iterations\n",
            "iteration: 587 \t total 625 iterations\n",
            "iteration: 588 \t total 625 iterations\n",
            "iteration: 589 \t total 625 iterations\n",
            "iteration: 590 \t total 625 iterations\n",
            "iteration: 591 \t total 625 iterations\n",
            "iteration: 592 \t total 625 iterations\n",
            "iteration: 593 \t total 625 iterations\n",
            "iteration: 594 \t total 625 iterations\n",
            "iteration: 595 \t total 625 iterations\n",
            "iteration: 596 \t total 625 iterations\n",
            "iteration: 597 \t total 625 iterations\n",
            "iteration: 598 \t total 625 iterations\n",
            "iteration: 599 \t total 625 iterations\n",
            "iteration: 600 \t total 625 iterations\n",
            "iteration: 601 \t total 625 iterations\n",
            "iteration: 602 \t total 625 iterations\n",
            "iteration: 603 \t total 625 iterations\n",
            "iteration: 604 \t total 625 iterations\n",
            "iteration: 605 \t total 625 iterations\n",
            "iteration: 606 \t total 625 iterations\n",
            "iteration: 607 \t total 625 iterations\n",
            "iteration: 608 \t total 625 iterations\n",
            "iteration: 609 \t total 625 iterations\n",
            "iteration: 610 \t total 625 iterations\n",
            "iteration: 611 \t total 625 iterations\n",
            "iteration: 612 \t total 625 iterations\n",
            "iteration: 613 \t total 625 iterations\n",
            "iteration: 614 \t total 625 iterations\n",
            "iteration: 615 \t total 625 iterations\n",
            "iteration: 616 \t total 625 iterations\n",
            "iteration: 617 \t total 625 iterations\n",
            "iteration: 618 \t total 625 iterations\n",
            "iteration: 619 \t total 625 iterations\n",
            "iteration: 620 \t total 625 iterations\n",
            "iteration: 621 \t total 625 iterations\n",
            "iteration: 622 \t total 625 iterations\n",
            "iteration: 623 \t total 625 iterations\n",
            "iteration: 624 \t total 625 iterations\n",
            "iteration: 625 \t total 625 iterations\n",
            "\n",
            "Parameter numbers: 3315428\n",
            "MobileNet(\n",
            "  3.32 M, 100.000% Params, 47.91 MMac, 99.140% MACs, \n",
            "  (stem): Sequential(\n",
            "    3.52 k, 0.106% Params, 3.74 MMac, 7.730% MACs, \n",
            "    (0): BasicConv2d(\n",
            "      928, 0.028% Params, 983.04 KMac, 2.034% MACs, \n",
            "      (conv): Conv2d(864, 0.026% Params, 884.74 KMac, 1.831% MACs, 3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, 0.002% Params, 65.54 KMac, 0.136% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      2.59 k, 0.078% Params, 2.75 MMac, 5.696% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        352, 0.011% Params, 393.22 KMac, 0.814% MACs, \n",
            "        (0): Conv2d(288, 0.009% Params, 294.91 KMac, 0.610% MACs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "        (1): BatchNorm2d(64, 0.002% Params, 65.54 KMac, 0.136% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        2.24 k, 0.068% Params, 2.36 MMac, 4.882% MACs, \n",
            "        (0): Conv2d(2.11 k, 0.064% Params, 2.16 MMac, 4.476% MACs, 32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(128, 0.004% Params, 131.07 KMac, 0.271% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 65.54 KMac, 0.136% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv1): Sequential(\n",
            "    27.46 k, 0.828% Params, 7.14 MMac, 14.783% MACs, \n",
            "    (0): DepthSeperabelConv2d(\n",
            "      9.28 k, 0.280% Params, 2.42 MMac, 5.018% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        704, 0.021% Params, 196.61 KMac, 0.407% MACs, \n",
            "        (0): Conv2d(576, 0.017% Params, 147.46 KMac, 0.305% MACs, 64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "        (1): BatchNorm2d(128, 0.004% Params, 32.77 KMac, 0.068% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 16.38 KMac, 0.034% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        8.58 k, 0.259% Params, 2.23 MMac, 4.611% MACs, \n",
            "        (0): Conv2d(8.32 k, 0.251% Params, 2.13 MMac, 4.408% MACs, 64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(256, 0.008% Params, 65.54 KMac, 0.136% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      18.18 k, 0.548% Params, 4.72 MMac, 9.765% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        1.41 k, 0.042% Params, 393.22 KMac, 0.814% MACs, \n",
            "        (0): Conv2d(1.15 k, 0.035% Params, 294.91 KMac, 0.610% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(256, 0.008% Params, 65.54 KMac, 0.136% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        16.77 k, 0.506% Params, 4.33 MMac, 8.951% MACs, \n",
            "        (0): Conv2d(16.51 k, 0.498% Params, 4.23 MMac, 8.748% MACs, 128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(256, 0.008% Params, 65.54 KMac, 0.136% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 32.77 KMac, 0.068% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    104.06 k, 3.139% Params, 6.72 MMac, 13.901% MACs, \n",
            "    (0): DepthSeperabelConv2d(\n",
            "      34.94 k, 1.054% Params, 2.26 MMac, 4.679% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        1.41 k, 0.042% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(1.15 k, 0.035% Params, 73.73 KMac, 0.153% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(256, 0.008% Params, 16.38 KMac, 0.034% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        33.54 k, 1.012% Params, 2.16 MMac, 4.476% MACs, \n",
            "        (0): Conv2d(33.02 k, 0.996% Params, 2.11 MMac, 4.374% MACs, 128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, 0.015% Params, 32.77 KMac, 0.068% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 16.38 KMac, 0.034% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      69.12 k, 2.085% Params, 4.46 MMac, 9.222% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        2.82 k, 0.085% Params, 196.61 KMac, 0.407% MACs, \n",
            "        (0): Conv2d(2.3 k, 0.069% Params, 147.46 KMac, 0.305% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(512, 0.015% Params, 32.77 KMac, 0.068% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 16.38 KMac, 0.034% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        66.3 k, 2.000% Params, 4.26 MMac, 8.815% MACs, \n",
            "        (0): Conv2d(65.79 k, 1.984% Params, 4.21 MMac, 8.714% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(512, 0.015% Params, 32.77 KMac, 0.068% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 16.38 KMac, 0.034% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    1.48 M, 44.700% Params, 23.81 MMac, 49.265% MACs, \n",
            "    (0): DepthSeperabelConv2d(\n",
            "      135.42 k, 4.085% Params, 2.18 MMac, 4.509% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        2.82 k, 0.085% Params, 49.15 KMac, 0.102% MACs, \n",
            "        (0): Conv2d(2.3 k, 0.069% Params, 36.86 KMac, 0.076% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(512, 0.015% Params, 8.19 KMac, 0.017% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 4.1 KMac, 0.008% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        132.61 k, 4.000% Params, 2.13 MMac, 4.408% MACs, \n",
            "        (0): Conv2d(131.58 k, 3.969% Params, 2.11 MMac, 4.357% MACs, 256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (3): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (4): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): DepthSeperabelConv2d(\n",
            "      269.31 k, 8.123% Params, 4.33 MMac, 8.951% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 98.3 KMac, 0.203% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 73.73 KMac, 0.153% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        263.68 k, 7.953% Params, 4.23 MMac, 8.748% MACs, \n",
            "        (0): Conv2d(262.66 k, 7.922% Params, 4.2 MMac, 8.697% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 16.38 KMac, 0.034% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 8.19 KMac, 0.017% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    1.6 M, 48.136% Params, 6.4 MMac, 13.240% MACs, \n",
            "    (0): DepthSeperabelConv2d(\n",
            "      532.99 k, 16.076% Params, 2.14 MMac, 4.425% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        5.63 k, 0.170% Params, 24.58 KMac, 0.051% MACs, \n",
            "        (0): Conv2d(4.61 k, 0.139% Params, 18.43 KMac, 0.038% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
            "        (1): BatchNorm2d(1.02 k, 0.031% Params, 4.1 KMac, 0.008% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 2.05 KMac, 0.004% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        527.36 k, 15.906% Params, 2.11 MMac, 4.374% MACs, \n",
            "        (0): Conv2d(525.31 k, 15.844% Params, 2.1 MMac, 4.348% MACs, 512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(2.05 k, 0.062% Params, 8.19 KMac, 0.017% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 4.1 KMac, 0.008% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DepthSeperabelConv2d(\n",
            "      1.06 M, 32.060% Params, 4.26 MMac, 8.815% MACs, \n",
            "      (depthwise): Sequential(\n",
            "        11.26 k, 0.340% Params, 49.15 KMac, 0.102% MACs, \n",
            "        (0): Conv2d(9.22 k, 0.278% Params, 36.86 KMac, 0.076% MACs, 1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "        (1): BatchNorm2d(2.05 k, 0.062% Params, 8.19 KMac, 0.017% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 4.1 KMac, 0.008% MACs, inplace=True)\n",
            "      )\n",
            "      (pointwise): Sequential(\n",
            "        1.05 M, 31.720% Params, 4.21 MMac, 8.714% MACs, \n",
            "        (0): Conv2d(1.05 M, 31.658% Params, 4.2 MMac, 8.688% MACs, 1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): BatchNorm2d(2.05 k, 0.062% Params, 8.19 KMac, 0.017% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(0, 0.000% Params, 4.1 KMac, 0.008% MACs, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(102.5 k, 3.092% Params, 102.5 KMac, 0.212% MACs, in_features=1024, out_features=100, bias=True)\n",
            "  (avg): AdaptiveAvgPool2d(0, 0.000% Params, 4.1 KMac, 0.008% MACs, output_size=1)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             864\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "       BasicConv2d-4           [-1, 32, 32, 32]               0\n",
            "            Conv2d-5           [-1, 32, 32, 32]             288\n",
            "       BatchNorm2d-6           [-1, 32, 32, 32]              64\n",
            "              ReLU-7           [-1, 32, 32, 32]               0\n",
            "            Conv2d-8           [-1, 64, 32, 32]           2,112\n",
            "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
            "             ReLU-10           [-1, 64, 32, 32]               0\n",
            "DepthSeperabelConv2d-11           [-1, 64, 32, 32]               0\n",
            "           Conv2d-12           [-1, 64, 16, 16]             576\n",
            "      BatchNorm2d-13           [-1, 64, 16, 16]             128\n",
            "             ReLU-14           [-1, 64, 16, 16]               0\n",
            "           Conv2d-15          [-1, 128, 16, 16]           8,320\n",
            "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
            "             ReLU-17          [-1, 128, 16, 16]               0\n",
            "DepthSeperabelConv2d-18          [-1, 128, 16, 16]               0\n",
            "           Conv2d-19          [-1, 128, 16, 16]           1,152\n",
            "      BatchNorm2d-20          [-1, 128, 16, 16]             256\n",
            "             ReLU-21          [-1, 128, 16, 16]               0\n",
            "           Conv2d-22          [-1, 128, 16, 16]          16,512\n",
            "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
            "             ReLU-24          [-1, 128, 16, 16]               0\n",
            "DepthSeperabelConv2d-25          [-1, 128, 16, 16]               0\n",
            "           Conv2d-26            [-1, 128, 8, 8]           1,152\n",
            "      BatchNorm2d-27            [-1, 128, 8, 8]             256\n",
            "             ReLU-28            [-1, 128, 8, 8]               0\n",
            "           Conv2d-29            [-1, 256, 8, 8]          33,024\n",
            "      BatchNorm2d-30            [-1, 256, 8, 8]             512\n",
            "             ReLU-31            [-1, 256, 8, 8]               0\n",
            "DepthSeperabelConv2d-32            [-1, 256, 8, 8]               0\n",
            "           Conv2d-33            [-1, 256, 8, 8]           2,304\n",
            "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
            "             ReLU-35            [-1, 256, 8, 8]               0\n",
            "           Conv2d-36            [-1, 256, 8, 8]          65,792\n",
            "      BatchNorm2d-37            [-1, 256, 8, 8]             512\n",
            "             ReLU-38            [-1, 256, 8, 8]               0\n",
            "DepthSeperabelConv2d-39            [-1, 256, 8, 8]               0\n",
            "           Conv2d-40            [-1, 256, 4, 4]           2,304\n",
            "      BatchNorm2d-41            [-1, 256, 4, 4]             512\n",
            "             ReLU-42            [-1, 256, 4, 4]               0\n",
            "           Conv2d-43            [-1, 512, 4, 4]         131,584\n",
            "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-45            [-1, 512, 4, 4]               0\n",
            "DepthSeperabelConv2d-46            [-1, 512, 4, 4]               0\n",
            "           Conv2d-47            [-1, 512, 4, 4]           4,608\n",
            "      BatchNorm2d-48            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-49            [-1, 512, 4, 4]               0\n",
            "           Conv2d-50            [-1, 512, 4, 4]         262,656\n",
            "      BatchNorm2d-51            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-52            [-1, 512, 4, 4]               0\n",
            "DepthSeperabelConv2d-53            [-1, 512, 4, 4]               0\n",
            "           Conv2d-54            [-1, 512, 4, 4]           4,608\n",
            "      BatchNorm2d-55            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-56            [-1, 512, 4, 4]               0\n",
            "           Conv2d-57            [-1, 512, 4, 4]         262,656\n",
            "      BatchNorm2d-58            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-59            [-1, 512, 4, 4]               0\n",
            "DepthSeperabelConv2d-60            [-1, 512, 4, 4]               0\n",
            "           Conv2d-61            [-1, 512, 4, 4]           4,608\n",
            "      BatchNorm2d-62            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-63            [-1, 512, 4, 4]               0\n",
            "           Conv2d-64            [-1, 512, 4, 4]         262,656\n",
            "      BatchNorm2d-65            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-66            [-1, 512, 4, 4]               0\n",
            "DepthSeperabelConv2d-67            [-1, 512, 4, 4]               0\n",
            "           Conv2d-68            [-1, 512, 4, 4]           4,608\n",
            "      BatchNorm2d-69            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-70            [-1, 512, 4, 4]               0\n",
            "           Conv2d-71            [-1, 512, 4, 4]         262,656\n",
            "      BatchNorm2d-72            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-73            [-1, 512, 4, 4]               0\n",
            "DepthSeperabelConv2d-74            [-1, 512, 4, 4]               0\n",
            "           Conv2d-75            [-1, 512, 4, 4]           4,608\n",
            "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-77            [-1, 512, 4, 4]               0\n",
            "           Conv2d-78            [-1, 512, 4, 4]         262,656\n",
            "      BatchNorm2d-79            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-80            [-1, 512, 4, 4]               0\n",
            "DepthSeperabelConv2d-81            [-1, 512, 4, 4]               0\n",
            "           Conv2d-82            [-1, 512, 2, 2]           4,608\n",
            "      BatchNorm2d-83            [-1, 512, 2, 2]           1,024\n",
            "             ReLU-84            [-1, 512, 2, 2]               0\n",
            "           Conv2d-85           [-1, 1024, 2, 2]         525,312\n",
            "      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-87           [-1, 1024, 2, 2]               0\n",
            "DepthSeperabelConv2d-88           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-89           [-1, 1024, 2, 2]           9,216\n",
            "      BatchNorm2d-90           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-91           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-92           [-1, 1024, 2, 2]       1,049,600\n",
            "      BatchNorm2d-93           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-94           [-1, 1024, 2, 2]               0\n",
            "DepthSeperabelConv2d-95           [-1, 1024, 2, 2]               0\n",
            "AdaptiveAvgPool2d-96           [-1, 1024, 1, 1]               0\n",
            "           Linear-97                  [-1, 100]         102,500\n",
            "================================================================\n",
            "Total params: 3,315,428\n",
            "Trainable params: 3,315,428\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 11.37\n",
            "Params size (MB): 12.65\n",
            "Estimated Total Size (MB): 24.03\n",
            "----------------------------------------------------------------\n",
            "Top 1 err:  tensor(0.3306, device='cuda:0')\n",
            "Top 5 err:  tensor(0.1015, device='cuda:0')\n",
            "FLOPs: 48.32 MMac\n",
            "Params: 3.32 M\n",
            "Inference time: 5.6745 seconds\n",
            "Time per inference step: 9.0793 milliseconds\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/Knowledge-Distillation/test.py -net mobilenet -gpu -weights /content/drive/MyDrive/Knowledge-Distillation/runs/mobilenet/mobilenet-124-best.pth"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
